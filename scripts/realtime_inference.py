#!/usr/bin/env python3
"""
å¯¦æ™‚æ”åƒé ­æ‰‹èªè­˜åˆ¥è…³æœ¬

âš ï¸  æ¨¡å‹ç‰ˆæœ¬ï¼šv3.0 - Deep Improvementsï¼ˆæ·±åº¦æ”¹é€²ç‰ˆï¼‰

ğŸ¯ v3.0 é—œéµæ”¹é€²ï¼š
1. å¤šå°ºåº¦æ± åŒ–ï¼šæ•æ‰ä¸åŒæ™‚é–“å°ºåº¦çš„ç‰¹å¾µ
2. MaxNorm ç´„æŸï¼šé™åˆ¶æ¬Šé‡å¤§å°ï¼Œé˜²æ­¢éåº¦è‡ªä¿¡
3. Focal Lossï¼šå°ˆæ³¨æ–¼é›£åˆ†é¡æ¨£æœ¬
4. Temperature Scalingï¼šè¼¸å‡ºä¿¡å¿ƒåº¦æ›´æº–ç¢ºï¼ˆT=1.5ï¼‰
5. Mixup æ•¸æ“šå¢å¼·ï¼šè¨“ç·´æ™‚æ¨£æœ¬æ··åˆï¼ˆæ¨è«–æ™‚ç„¡å½±éŸ¿ï¼‰
6. Label Smoothingï¼šé˜²æ­¢æ¨¡å‹éåº¦æ“¬åˆæ¨™ç±¤

ğŸ“Š é æœŸè¼¸å‡ºï¼š
- ä¿¡å¿ƒåº¦ç¯„åœï¼š70-85%ï¼ˆä¸å†æ˜¯ 98%+ï¼‰
- Top-5 åˆ†ä½ˆæ›´å‡å‹»ï¼šåæ˜ çœŸå¯¦ä¸ç¢ºå®šæ€§
- èª¤åˆ¤æ™‚ä¸æœƒçµ¦å‡ºæ¥µç«¯ä¿¡å¿ƒåº¦

ğŸ”§ æ¨è«–ç„¡éœ€ä¿®æ”¹ï¼š
- æ‰€æœ‰æ­£å‰‡åŒ–æŠ€è¡“åœ¨æ¨è«–æ™‚è‡ªå‹•è™•ç†
- BatchNorm/LayerNorm è‡ªå‹•åˆ‡æ›åˆ°æ¨è«–æ¨¡å¼
- Temperature Scaling è‡ªå‹•æ‡‰ç”¨æ–¼è¼¸å‡º logits
- MaxNorm ç´„æŸä¸å½±éŸ¿æ¨è«–é€Ÿåº¦

åŠŸèƒ½ï¼š
1. ä½¿ç”¨å‰ç½®é¡é ­éŒ„å½±ï¼ˆ80 å¹€ï¼Œç´„ 2.67 ç§’ @ 30fpsï¼‰
2. ç„¡éœ€ä¿å­˜æ–‡ä»¶ï¼šç›´æ¥å¾è¨˜æ†¶é«”æå–ç‰¹å¾µ
3. ä¸¦è¡Œç‰¹å¾µæå–ï¼šRGB (MobileNetV3) + Skeleton (MediaPipe)
4. å¯¦æ™‚æ¨è«–ï¼šCPU æ¨¡å¼ï¼ˆæ¯” MPS å¿« 23 å€ï¼‰
5. æŒçºŒé¡¯ç¤ºæœ€æ–°çµæœï¼ˆæŒ‰ C æ¸…é™¤ï¼‰
"""

import os
import sys

# âš ï¸ é—œéµï¼åœ¨å°å…¥ä»»ä½•å…¶ä»–æ¨¡çµ„å‰å…ˆè¨­ç½®ç’°å¢ƒè®Šæ•¸
# ç¦ç”¨ MediaPipe GPU/OpenGLï¼ˆå®¹å™¨ç’°å¢ƒç„¡ GPU æ”¯æ´ï¼‰
os.environ['CUDA_VISIBLE_DEVICES'] = ''  # ç¦ç”¨ CUDA
os.environ['MEDIAPIPE_GPU_DISABLED'] = '1'  # ç¦ç”¨ MediaPipe GPU
os.environ['MEDIAPIPE_DISABLE_GPU'] = '1'  # æ›¿ä»£è®Šæ•¸
os.environ['GLOG_minloglevel'] = '2'  # æ¸›å°‘ Google Log æ—¥èªŒ
os.environ['KERAS_BACKEND'] = 'tensorflow'  # è¨­ç½® Keras backend
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # æ¸›å°‘ TensorFlow æ—¥èªŒ

# ç¾åœ¨æ‰å°å…¥å…¶ä»–æ¨¡çµ„
import cv2
import numpy as np
import torch
from pathlib import Path
from datetime import datetime
import tempfile
import threading
import queue
import logging

import tensorflow as tf
import keras

# å°å…¥æ¨¡çµ„
sys.path.append(str(Path(__file__).parent))
sys.path.append(str(Path(__file__).parent / "feature_extraction"))

from rgb_feature_extraction import RGBFeatureExtractor
from skeleton_feature_extraction import EnhancedSkeletonExtractor


# ==================== è‡ªå®šç¾©å±¤å®šç¾©ï¼ˆè¼‰å…¥æ¨¡å‹éœ€è¦ï¼‰====================
@keras.saving.register_keras_serializable()
class FocalLoss(keras.losses.Loss):
    """Focal Loss"""
    def __init__(self, num_classes=15, gamma=2.0, alpha=None, label_smoothing=0.1, name='focal_loss', **kwargs):
        super().__init__(name=name, **kwargs)
        self.num_classes = num_classes
        self.gamma = gamma
        self.alpha = alpha
        self.label_smoothing = label_smoothing
    def call(self, y_true, y_pred):
        from keras import ops
        if len(ops.shape(y_true)) == 1:
            y_true_one_hot = ops.one_hot(ops.cast(y_true, 'int32'), self.num_classes)
            if self.label_smoothing > 0:
                y_true_one_hot = y_true_one_hot * (1.0 - self.label_smoothing) + self.label_smoothing / self.num_classes
        else:
            y_true_one_hot = y_true
        epsilon = 1e-7
        y_pred = ops.clip(y_pred, epsilon, 1.0 - epsilon)
        cross_entropy = -y_true_one_hot * ops.log(y_pred)
        p_t = ops.sum(y_true_one_hot * y_pred, axis=-1, keepdims=True)
        focal_weight = ops.power(1.0 - p_t, self.gamma)
        focal_cross_entropy = focal_weight * cross_entropy
        if self.alpha is not None:
            alpha_weight = y_true_one_hot * self.alpha
            focal_cross_entropy = alpha_weight * focal_cross_entropy
        return ops.mean(ops.sum(focal_cross_entropy, axis=-1))
    def get_config(self):
        config = super().get_config()
        config.update({'num_classes': self.num_classes, 'gamma': self.gamma, 'alpha': self.alpha, 'label_smoothing': self.label_smoothing})
        return config

@keras.saving.register_keras_serializable()
class MixupAccuracy(keras.metrics.Metric):
    """Mixup Accuracy"""
    def __init__(self, name='mixup_accuracy', **kwargs):
        super().__init__(name=name, **kwargs)
        self.total = self.add_weight(name='total', initializer='zeros')
        self.count = self.add_weight(name='count', initializer='zeros')
    def update_state(self, y_true, y_pred, sample_weight=None):
        from keras import ops
        if len(ops.shape(y_true)) == 1:
            y_true_labels = ops.cast(y_true, 'int32')
        else:
            y_true_labels = ops.argmax(y_true, axis=-1)
        y_pred_labels = ops.argmax(y_pred, axis=-1)
        matches = ops.cast(ops.equal(y_true_labels, y_pred_labels), 'float32')
        self.total.assign_add(ops.sum(matches))
        self.count.assign_add(ops.cast(ops.size(matches), 'float32'))
    def result(self):
        return self.total / self.count
    def reset_state(self):
        self.total.assign(0.0)
        self.count.assign(0.0)

@keras.saving.register_keras_serializable()
class MixupTop3Accuracy(keras.metrics.Metric):
    """Mixup Top-3 Accuracy"""
    def __init__(self, name='mixup_top3_accuracy', **kwargs):
        super().__init__(name=name, **kwargs)
        self.total = self.add_weight(name='total', initializer='zeros')
        self.count = self.add_weight(name='count', initializer='zeros')
    def update_state(self, y_true, y_pred, sample_weight=None):
        from keras import ops
        if len(ops.shape(y_true)) == 1:
            y_true_labels = ops.cast(y_true, 'int32')
        else:
            y_true_labels = ops.argmax(y_true, axis=-1)
        top3_pred = ops.top_k(y_pred, k=3)[1]
        y_true_expanded = ops.expand_dims(y_true_labels, axis=-1)
        matches = ops.any(ops.equal(top3_pred, y_true_expanded), axis=-1)
        matches = ops.cast(matches, 'float32')
        self.total.assign_add(ops.sum(matches))
        self.count.assign_add(ops.cast(ops.size(matches), 'float32'))
    def result(self):
        return self.total / self.count
    def reset_state(self):
        self.total.assign(0.0)
        self.count.assign(0.0)


class RealtimeSignLanguageRecognition:
    """å¯¦æ™‚æ‰‹èªè­˜åˆ¥å™¨"""
    
    # éŒ„å½±åƒæ•¸ï¼ˆèˆ‡ VideoProcessor ä¸€è‡´ï¼‰
    TARGET_FRAMES = 80
    TARGET_FPS = 30
    TARGET_WIDTH = 224
    TARGET_HEIGHT = 224

    def _setup_logging(self, log_file=None, enable_logging=True):
        """è¨­ç½®æ—¥èªŒç³»çµ±"""
        if not enable_logging:
            # å¦‚æœç¦ç”¨æ—¥èªŒï¼Œåªè¨­ç½®åŸºæœ¬çš„æ§åˆ¶å°è¼¸å‡º
            logging.basicConfig(
                level=logging.INFO,
                format='%(asctime)s [%(levelname)s] %(message)s',
                datefmt='%Y-%m-%d %H:%M:%S',
                handlers=[logging.StreamHandler(sys.stdout)]
            )
            self.logger = logging.getLogger(__name__)
            self.log_file = None
            return
            
        # å‰µå»º logs ç›®éŒ„
        logs_dir = Path("logs")
        logs_dir.mkdir(exist_ok=True)

        # å¦‚æœæ²’æœ‰æŒ‡å®š log_fileï¼Œä½¿ç”¨é»˜èªè·¯å¾‘
        if log_file is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            log_file = logs_dir / f"realtime_inference_{timestamp}.log"

        # é…ç½®æ ¹æ—¥èªŒå™¨
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s [%(levelname)s] %(message)s',
            datefmt='%Y-%m-%d %H:%M:%S',
            handlers=[
                logging.FileHandler(log_file, encoding='utf-8'),
                logging.StreamHandler(sys.stdout)  # åŒæ™‚è¼¸å‡ºåˆ°æ§åˆ¶å°
            ]
        )

        self.logger = logging.getLogger(__name__)
        self.log_file = log_file

        self.logger.info(f"ğŸ“ æ—¥èªŒæ–‡ä»¶: {log_file}")
        self.logger.info(f"ğŸš€ ç¨‹åºå•Ÿå‹•æ™‚é–“: {datetime.now()}")
        self.logger.info(f"ğŸ”§ ç³»çµ±ä¿¡æ¯: Python {sys.version.split()[0]} on {sys.platform}")

    def __init__(self, model_path, label_map_path, device='mps', camera_id=0, log_file=None, enable_logging=False):
        """
        åˆå§‹åŒ–å¯¦æ™‚è­˜åˆ¥å™¨

        Args:
            model_path: æ¨¡å‹è·¯å¾‘
            label_map_path: æ¨™ç±¤æ˜ å°„è·¯å¾‘
            device: è¨­å‚™é¡å‹ ('mps', 'gpu', 'cpu')
            camera_id: æ”åƒé ­ IDï¼ˆ0=å‰ç½®é¡é ­ï¼Œ1=å¾Œç½®é¡é ­ï¼‰
            log_file: æ—¥èªŒæ–‡ä»¶è·¯å¾‘ï¼ˆå¦‚æœç‚º Noneï¼Œå‰‡ä½¿ç”¨é»˜èªè·¯å¾‘ï¼‰
            enable_logging: æ˜¯å¦å•Ÿç”¨æ—¥èªŒè¨˜éŒ„ï¼ˆé è¨­ç‚º Falseï¼‰
        """
        self.device = device
        self.camera_id = camera_id

        # è¨­ç½®æ—¥èªŒ
        self._setup_logging(log_file, enable_logging)

        self.logger.info("=" * 70)
        self.logger.info("ğŸ¥ å¯¦æ™‚æ‰‹èªè­˜åˆ¥ç³»çµ±")
        self.logger.info("=" * 70)
        
        # è¼‰å…¥æ¨¡å‹
        self._load_model(model_path, label_map_path)
        
        # åˆå§‹åŒ–è™•ç†å™¨
        self._init_processors()
        
        # éŒ„å½±ç‹€æ…‹
        self.is_recording = False
        self.frame_count = 0
        
        # ä½¿ç”¨é åˆ†é…çš„å¾ªç’°ç·©è¡å€ï¼ˆé¿å…å‹•æ…‹ list å¢é•·ï¼‰
        self.frame_buffer = np.zeros(
            (self.TARGET_FRAMES, self.TARGET_HEIGHT, self.TARGET_WIDTH, 3), 
            dtype=np.uint8
        )
        
        # çµæœéšŠåˆ—ï¼ˆç”¨æ–¼ç·šç¨‹é–“é€šä¿¡ï¼‰
        self.result_queue = queue.Queue()
        
        # æœ€æ–°çš„è­˜åˆ¥çµæœï¼ˆæŒä¹…ä¿å­˜ï¼‰
        self.latest_result = None
        
        # æ¨¡å‹é ç†±
        self._warmup_model()
        
        self.logger.info(f"âœ… ç³»çµ±åˆå§‹åŒ–å®Œæˆï¼")
        self.logger.info(f"ğŸ“¹ æ”åƒé ­ ID: {camera_id}")
        self.logger.info(f"ğŸ¬ éŒ„å½±è¦æ ¼: {self.TARGET_FRAMES} å¹€ @ {self.TARGET_FPS} fps, {self.TARGET_WIDTH}x{self.TARGET_HEIGHT}")
        self.logger.info("=" * 70)
    
    def _load_model(self, model_path, label_map_path):
        """è¼‰å…¥æ¨¡å‹å’Œæ¨™ç±¤"""
        import json

        self.logger.info(f"ğŸ“¥ è¼‰å…¥æ¨¡å‹: {model_path}")

        # è¨­ç½® TensorFlow è¨­å‚™
        if self.device == 'mps':
            physical_devices = tf.config.list_physical_devices('GPU')
            if physical_devices:
                self.logger.info(f"ğŸš€ ä½¿ç”¨ M1 MPS åŠ é€Ÿ")
            else:
                self.logger.warning("âš ï¸  MPS ä¸å¯ç”¨ï¼Œä½¿ç”¨ CPU")
                self.device = 'cpu'

        # å•Ÿç”¨ unsafe deserializationï¼ˆè™•ç† Lambda å±¤ï¼‰
        keras.config.enable_unsafe_deserialization()
        
        # è¼‰å…¥æ¨¡å‹
        custom_objects = {
            'FocalLoss': FocalLoss,
            'MixupAccuracy': MixupAccuracy,
            'MixupTop3Accuracy': MixupTop3Accuracy
        }
        self.model = keras.models.load_model(model_path, custom_objects=custom_objects)
        keras.mixed_precision.set_global_policy('float32')

        # è¼‰å…¥æ¨™ç±¤
        with open(label_map_path, 'r', encoding='utf-8') as f:
            self.label_map = json.load(f)
        self.idx_to_word = {v: k for k, v in self.label_map.items()}

        self.logger.info(f"âœ… æ¨¡å‹è¼‰å…¥æˆåŠŸï¼ˆ{len(self.label_map)} å€‹å–®è©ï¼‰")
    
    def _warmup_model(self):
        """
        é ç†±æ¨¡å‹ï¼ˆé¦–æ¬¡æ¨è«–ç·¨è­¯å„ªåŒ–ï¼‰
        
        æ³¨æ„ï¼šBiGRU æ¨¡å‹åœ¨ TensorFlow Metal (MPS) ä¸Šæ¨è«–æ¥µæ…¢ï¼ˆ~27sï¼‰
        CPU åè€Œå¿« 23 å€ï¼ˆ~1.1sï¼‰ï¼Œå› æ­¤å¼·åˆ¶ä½¿ç”¨ CPU æ¨è«–
        """
        self.logger.info("ğŸ”¥ é ç†±æ¨¡å‹...")
        self.logger.info("âš ï¸  æ³¨æ„ï¼šBiGRU åœ¨ MPS ä¸Šå¾ˆæ…¢ï¼Œä½¿ç”¨ CPU æ¨è«–ï¼ˆå¿« 23 å€ï¼‰")
        
        dummy_input = np.zeros((1, 300, 1119), dtype=np.float32)
        
        # å¼·åˆ¶ä½¿ç”¨ CPU æ¨è«–ï¼ˆæ¯” MPS å¿«ï¼‰
        with tf.device('/CPU:0'):
            _ = self.model.predict(dummy_input, verbose=0)
        
        self.logger.info("âœ… æ¨¡å‹é ç†±å®Œæˆ")
    
    def _init_processors(self):
        """åˆå§‹åŒ–è™•ç†å™¨"""
        self.logger.info("\nğŸ”§ åˆå§‹åŒ–è™•ç†å™¨...")

        # RGB ç‰¹å¾µæå–å™¨ï¼ˆPyTorch - MPSï¼‰
        if self.device == 'mps':
            torch_device = torch.device('mps')
            device_type = 'gpu'
        elif self.device == 'gpu':
            torch_device = torch.device('cuda')
            device_type = 'gpu'
        else:
            torch_device = torch.device('cpu')
            device_type = 'cpu'

        self.rgb_extractor = RGBFeatureExtractor(torch_device, device_type)

        # éª¨æ¶ç‰¹å¾µæå–å™¨ï¼ˆMediaPipe Holistic - 159ç¶­ï¼Œè‡ªå‹•ä½¿ç”¨ Metal åŠ é€Ÿï¼‰
        from skeleton_feature_extraction import EnhancedSkeletonExtractor
        self.skeleton_extractor = EnhancedSkeletonExtractor(num_threads=4)

        self.logger.info("âœ… è™•ç†å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def start_camera(self):
        """å•Ÿå‹•æ”åƒé ­"""
        self.cap = cv2.VideoCapture(self.camera_id)
        
        if not self.cap.isOpened():
            raise RuntimeError(f"âŒ ç„¡æ³•æ‰“é–‹æ”åƒé ­ {self.camera_id}")
        
        # è¨­ç½®æ”åƒé ­åƒæ•¸
        self.cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
        self.cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)
        self.cap.set(cv2.CAP_PROP_FPS, self.TARGET_FPS)
        
        actual_width = int(self.cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        actual_height = int(self.cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        actual_fps = int(self.cap.get(cv2.CAP_PROP_FPS))
        
        self.logger.info(f"ğŸ“¹ æ”åƒé ­å·²å•Ÿå‹•: {actual_width}x{actual_height} @ {actual_fps} fps")
        
        return True
    
    def stop_camera(self):
        """é—œé–‰æ”åƒé ­"""
        if hasattr(self, 'cap') and self.cap is not None:
            self.cap.release()
    
    def start_recording(self):
        """é–‹å§‹éŒ„å½±"""
        self.is_recording = True
        self.frame_count = 0
        # é‡ç½®ç·©è¡å€ï¼ˆä¸éœ€è¦ï¼Œå› ç‚ºæœƒè¢«è¦†è“‹ï¼‰
        self.logger.info(f"\nğŸ”´ é–‹å§‹éŒ„å½±ï¼ˆç›®æ¨™ {self.TARGET_FRAMES} å¹€ï¼‰...")
    
    def stop_recording(self):
        """åœæ­¢éŒ„å½±"""
        self.is_recording = False
        self.logger.info(f"â¹ï¸  åœæ­¢éŒ„å½±ï¼ˆå·²éŒ„ {self.frame_count} å¹€ï¼‰")
    
    def add_frame(self, frame):
        """æ·»åŠ å¹€åˆ°éŒ„å½±ç·©è¡å€ï¼ˆå„ªåŒ–ç‰ˆï¼šéŒ„å½±æ™‚åŒæ­¥ resizeï¼‰"""
        if self.is_recording and self.frame_count < self.TARGET_FRAMES:
            # ç›´æ¥ resize åˆ°ç›®æ¨™å°ºå¯¸ä¸¦å¯«å…¥ç·©è¡å€
            frame_resized = cv2.resize(frame, (self.TARGET_WIDTH, self.TARGET_HEIGHT))
            
            # è½‰æ›ç‚º RGBï¼ˆMediaPipe å’Œæ¨¡å‹éƒ½éœ€è¦ RGBï¼‰
            frame_rgb = cv2.cvtColor(frame_resized, cv2.COLOR_BGR2RGB)
            
            # å¯«å…¥é åˆ†é…çš„ç·©è¡å€
            self.frame_buffer[self.frame_count] = frame_rgb
            self.frame_count += 1
            
            # é”åˆ°ç›®æ¨™å¹€æ•¸å¾Œè‡ªå‹•åœæ­¢ä¸¦é–‹å§‹è™•ç†
            if self.frame_count >= self.TARGET_FRAMES:
                self.stop_recording()
                # åœ¨æ–°ç·šç¨‹ä¸­è™•ç†è¦–é »ï¼ˆé¿å…é˜»å¡ä¸»ç·šç¨‹ï¼‰
                threading.Thread(target=self._process_video_optimized, daemon=True).start()
    
    def _process_video_optimized(self):
        """
        å„ªåŒ–ç‰ˆï¼šè™•ç†éŒ„è£½çš„è¦–é »ï¼ˆåœ¨å¾Œå°ç·šç¨‹é‹è¡Œï¼‰
        âœ… ç›´æ¥å¾è¨˜æ†¶é«”ä¸­çš„ frames æå–ç‰¹å¾µ
        âœ… ç„¡éœ€ä¿å­˜/è®€å–è‡¨æ™‚æ–‡ä»¶
        âœ… æ¸›å°‘ I/O é–‹éŠ· ~1000ms
        """
        import time
        start_time = time.time()
        self.logger.info(f"\nâš™ï¸  é–‹å§‹è™•ç†è¦–é »ï¼ˆå„ªåŒ–æ¨¡å¼ï¼‰...")

        try:
            # 1. æº–å‚™ framesï¼ˆå·²ç¶“æ˜¯ 224x224 RGBï¼‰
            # å–å‡ºå¯¦éš›éŒ„è£½çš„å¹€æ•¸
            frames = [self.frame_buffer[i] for i in range(self.frame_count)]
            self.logger.info(f"ğŸ“Š è™•ç† {len(frames)} å¹€ ({self.TARGET_WIDTH}x{self.TARGET_HEIGHT} RGB)")
            
            # 2. ä¸¦è¡Œç‰¹å¾µæå–ï¼ˆç›´æ¥å¾ framesï¼‰
            self.logger.info("ğŸ”„ ä¸¦è¡Œç‰¹å¾µæå–ï¼ˆç„¡ I/Oï¼‰...")
            
            rgb_features = None
            skeleton_features = None
            errors = []
            
            def extract_rgb():
                nonlocal rgb_features, errors
                try:
                    t0 = time.time()
                    self.logger.info("  ğŸ“¸ æå– RGB ç‰¹å¾µ...")
                    rgb_features = self.rgb_extractor.extract_features_from_frames(frames)
                    t1 = time.time()
                    self.logger.info(f"  âœ… RGB ç‰¹å¾µ: {rgb_features.shape} ({(t1-t0)*1000:.0f}ms)")
                except Exception as e:
                    errors.append(f"RGB æå–å¤±æ•—: {e}")
                    import traceback
                    self.logger.error(traceback.format_exc())
            
            def extract_skeleton():
                nonlocal skeleton_features, errors
                try:
                    t0 = time.time()
                    self.logger.info("  ğŸ¦´ æå–éª¨æ¶ç‰¹å¾µ...")
                    # å‚³å…¥åŸå§‹å¹€å°ºå¯¸ç”¨æ–¼æ­£è¦åŒ–
                    skeleton_features = self.skeleton_extractor.extract_features_from_frames(
                        frames, 
                        frame_width=self.TARGET_WIDTH, 
                        frame_height=self.TARGET_HEIGHT
                    )
                    t1 = time.time()
                    self.logger.info(f"  âœ… éª¨æ¶ç‰¹å¾µ: {skeleton_features.shape} ({(t1-t0)*1000:.0f}ms)")
                except Exception as e:
                    errors.append(f"éª¨æ¶æå–å¤±æ•—: {e}")
                    import traceback
                    self.logger.error(traceback.format_exc())
            
            # ä¸¦è¡ŒåŸ·è¡Œ
            rgb_thread = threading.Thread(target=extract_rgb)
            skeleton_thread = threading.Thread(target=extract_skeleton)
            
            rgb_thread.start()
            skeleton_thread.start()
            
            rgb_thread.join()
            skeleton_thread.join()
            
            # æª¢æŸ¥éŒ¯èª¤
            if errors:
                error_msg = "; ".join(errors)
                raise RuntimeError(error_msg)
            
            if rgb_features is None or skeleton_features is None:
                raise ValueError("ç‰¹å¾µæå–å¤±æ•—")
            
            # 3. èåˆç‰¹å¾µ
            self.logger.info("ğŸ”€ èåˆç‰¹å¾µ...")
            min_len = min(len(rgb_features), len(skeleton_features))
            rgb_features = rgb_features[:min_len]
            skeleton_features = skeleton_features[:min_len]
            
            concat_features = np.concatenate([rgb_features, skeleton_features], axis=1)
            
            # Padding åˆ° 300ï¼ˆæ¨¡å‹è¼¸å…¥è¦æ±‚ï¼‰
            max_length = 300
            if len(concat_features) < max_length:
                padding = np.zeros((max_length - len(concat_features), 1119), dtype=np.float32)
                concat_features = np.concatenate([concat_features, padding], axis=0)
            else:
                concat_features = concat_features[:max_length]
            
            self.logger.info(f"âœ… ç‰¹å¾µèåˆå®Œæˆ: {concat_features.shape}")
            
            # 4. æ¨è«–ï¼ˆå¼·åˆ¶ä½¿ç”¨ CPUï¼Œæ¯” MPS å¿« 23 å€ï¼‰
            t0 = time.time()
            self.logger.info("ğŸ¤– åŸ·è¡Œæ¨è«–ï¼ˆCPU æ¨¡å¼ï¼‰...")
            features_batch = np.expand_dims(concat_features, axis=0)
            
            # å¼·åˆ¶ä½¿ç”¨ CPUï¼ˆBiGRU åœ¨ CPU ä¸Šæ¯” MPS å¿« 23 å€ï¼‰
            with tf.device('/CPU:0'):
                predictions = self.model.predict(features_batch, verbose=0)
            
            t1 = time.time()
            self.logger.info(f"âœ… æ¨è«–å®Œæˆ ({(t1-t0)*1000:.0f}ms)")
            
            # 5. ç²å– Top-5 çµæœ
            top_indices = np.argsort(predictions[0])[::-1][:5]
            results = [(self.idx_to_word[idx], float(predictions[0][idx])) for idx in top_indices]
            
            # 6. å°‡çµæœæ”¾å…¥éšŠåˆ—
            self.result_queue.put({
                'success': True,
                'results': results,
                'timestamp': datetime.now()
            })
            
            # 7. åœ¨çµ‚ç«¯æ‰“å°çµæœ
            total_time = time.time() - start_time
            result_text = "\n" + "=" * 70 + "\nğŸ¯ è¾¨è­˜çµæœ:\n" + "=" * 70
            for i, (word, confidence) in enumerate(results, 1):
                bar = "â–ˆ" * int(confidence * 50)
                result_text += f"\n{i}. {word:15s} {confidence*100:6.2f}% {bar}"
            result_text += f"\n" + "=" * 70
            result_text += f"\nâ±ï¸  ç¸½è€—æ™‚: {total_time*1000:.0f}ms"
            result_text += f"\nâœ… æ¨è«–å®Œæˆï¼æŒ‰ SPACE ç¹¼çºŒéŒ„å½±..."
            result_text += "\n" + "=" * 70
            
            self.logger.info(result_text)
        
        except Exception as e:
            self.logger.error(f"âŒ è™•ç†å¤±æ•—: {e}")
            import traceback
            self.logger.error(f"è©³ç´°éŒ¯èª¤ä¿¡æ¯:\n{traceback.format_exc()}")
            self.result_queue.put({
                'success': False,
                'error': str(e),
                'timestamp': datetime.now()
            })
    
    
    def draw_ui(self, frame):
        """ç¹ªè£½ UIï¼ˆé¡¯ç¤ºç‹€æ…‹å’Œçµæœï¼‰"""
        # å‰µå»ºåŠé€æ˜çš„ç‹€æ…‹æ¬„
        overlay = frame.copy()
        height, width = frame.shape[:2]
        
        # é ‚éƒ¨ç‹€æ…‹æ¬„
        cv2.rectangle(overlay, (0, 0), (width, 80), (0, 0, 0), -1)
        frame = cv2.addWeighted(overlay, 0.6, frame, 0.4, 0)
        
        # é¡¯ç¤ºç‹€æ…‹
        if self.is_recording:
            status_text = f"Recording... ({self.frame_count}/{self.TARGET_FRAMES})"
            color = (0, 0, 255)  # ç´…è‰²
            
            # é€²åº¦æ¢
            progress = self.frame_count / self.TARGET_FRAMES
            bar_width = int(width * 0.8)
            bar_x = int(width * 0.1)
            cv2.rectangle(frame, (bar_x, 50), (bar_x + bar_width, 70), (100, 100, 100), -1)
            cv2.rectangle(frame, (bar_x, 50), (bar_x + int(bar_width * progress), 70), color, -1)
        else:
            status_text = "Ready - Press SPACE to start"
            color = (0, 255, 0)  # ç¶ è‰²
        
        cv2.putText(frame, status_text, (10, 30), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.8, color, 2)
        
        # å¾éšŠåˆ—ä¸­ç²å–æœ€æ–°çµæœï¼ˆå¦‚æœæœ‰ï¼‰
        try:
            while not self.result_queue.empty():
                self.latest_result = self.result_queue.get_nowait()
        except queue.Empty:
            pass
        
        # é¡¯ç¤ºæœ€æ–°çš„è­˜åˆ¥çµæœï¼ˆæŒçºŒé¡¯ç¤ºï¼‰
        if self.latest_result is not None:
            if self.latest_result['success']:
                # é¡¯ç¤º Top-3 çµæœ
                y_offset = height - 150
                cv2.rectangle(overlay, (0, y_offset), (width, height), (0, 0, 0), -1)
                frame = cv2.addWeighted(overlay, 0.7, frame, 0.3, 0)
                
                cv2.putText(frame, "Results:", (10, y_offset + 30),
                           cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
                
                for i, (word, conf) in enumerate(self.latest_result['results'][:3]):
                    text = f"{i+1}. {word}: {conf*100:.1f}%"
                    y = y_offset + 60 + i * 30
                    cv2.putText(frame, text, (10, y),
                                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 255), 2)
            else:
                # é¡¯ç¤ºéŒ¯èª¤
                cv2.putText(frame, f"Error: {self.latest_result['error']}", (10, height - 30),
                           cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 2)
        
        # èªªæ˜æ–‡å­—
        help_text = [
            "SPACE: Record",
            "C: Clear",
            "Q: Quit"
        ]
        
        for i, text in enumerate(help_text):
            cv2.putText(frame, text, (width - 250, 30 + i * 25),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (200, 200, 200), 1)
        
        return frame
    
    def run(self):
        """ä¸»å¾ªç’°"""
        try:
            # å•Ÿå‹•æ”åƒé ­
            self.start_camera()

            self.logger.info("\n" + "=" * 70)
            self.logger.info("ğŸ“¹ æ”åƒé ­å·²å•Ÿå‹•ï¼")
            self.logger.info("=" * 70)
            self.logger.info("ä½¿ç”¨èªªæ˜ï¼š")
            self.logger.info("  - æŒ‰ SPACE é–‹å§‹éŒ„å½±ï¼ˆè‡ªå‹•éŒ„è£½ 80 å¹€ï¼‰")
            self.logger.info("  - éŒ„å½±å®Œæˆå¾Œè‡ªå‹•è™•ç†ä¸¦é¡¯ç¤ºçµæœ")
            self.logger.info("  - æŒ‰ C æ¸…é™¤è¾¨è­˜çµæœ")
            self.logger.info("  - æŒ‰ Q é€€å‡º")
            self.logger.info("=" * 70)

            while True:
                # è®€å–å¹€
                ret, frame = self.cap.read()
                if not ret:
                    self.logger.error("âŒ è®€å–å¹€å¤±æ•—")
                    break
                
                # æ°´å¹³ç¿»è½‰ï¼ˆå‰ç½®é¡é ­é¡åƒæ•ˆæœï¼‰
                frame = cv2.flip(frame, 1)
                
                # æ·»åŠ å¹€åˆ°éŒ„å½±ç·©è¡å€ï¼ˆå¦‚æœæ­£åœ¨éŒ„å½±ï¼‰
                if self.is_recording:
                    self.add_frame(frame)
                
                # ç¹ªè£½ UI
                display_frame = self.draw_ui(frame)
                
                # é¡¯ç¤º
                cv2.imshow('Sign Language Recognition - Realtime', display_frame)
                
                # æŒ‰éµè™•ç†
                key = cv2.waitKey(1) & 0xFF
                
                if key == ord('q') or key == ord('Q'):
                    # é€€å‡º
                    break
                elif key == ord(' '):
                    # ç©ºæ ¼éµï¼šé–‹å§‹/åœæ­¢éŒ„å½±
                    if not self.is_recording:
                        self.start_recording()
                    else:
                        self.stop_recording()
                elif key == ord('c') or key == ord('C'):
                    # C éµï¼šæ¸…é™¤çµæœ
                    self.latest_result = None
                    self.logger.info("\nğŸ—‘ï¸  å·²æ¸…é™¤è¾¨è­˜çµæœ")

        finally:
            # æ¸…ç†
            self.stop_camera()
            cv2.destroyAllWindows()
            self.logger.info("\nğŸ‘‹ ç¨‹åºå·²é€€å‡º")


def main():
    """ä¸»å‡½æ•¸"""
    import argparse
    
    parser = argparse.ArgumentParser(description='å¯¦æ™‚æ”åƒé ­æ‰‹èªè­˜åˆ¥')
    parser.add_argument('--model', type=str, 
                       default='model_output/best_model_mps.keras',
                       help='æ¨¡å‹è·¯å¾‘')
    parser.add_argument('--labels', type=str,
                       default='model_output/label_map.json',
                       help='æ¨™ç±¤æ˜ å°„è·¯å¾‘')
    parser.add_argument('--device', type=str, default='mps',
                       choices=['mps', 'gpu', 'cpu'],
                       help='è¨­å‚™é¡å‹')
    parser.add_argument('--camera', type=int, default=0,
                       help='æ”åƒé ­ IDï¼ˆ0=å‰ç½®ï¼Œ1=å¾Œç½®ï¼‰')
    parser.add_argument('--log-file', type=str, default=None,
                       help='æ—¥èªŒæ–‡ä»¶è·¯å¾‘ï¼ˆé»˜èªä½¿ç”¨ logs/realtime_inference_YYYYMMDD_HHMMSS.logï¼‰')
    parser.add_argument('--enable-log', action='store_true',
                       help='å•Ÿç”¨æ—¥èªŒè¨˜éŒ„ï¼ˆé è¨­ç¦ç”¨ï¼‰')
    
    args = parser.parse_args()
    
    # æª¢æŸ¥æ–‡ä»¶
    model_path = Path(args.model)
    label_path = Path(args.labels)
    
    if not model_path.exists():
        print(f"âŒ æ¨¡å‹ä¸å­˜åœ¨: {model_path}")
        print("è«‹å…ˆåŸ·è¡Œ: python3 convert_model_for_mps.py")
        return

    if not label_path.exists():
        print(f"âŒ æ¨™ç±¤æ˜ å°„ä¸å­˜åœ¨: {label_path}")
        return
    
    # å‰µå»ºè­˜åˆ¥å™¨ä¸¦é‹è¡Œ
    recognizer = RealtimeSignLanguageRecognition(
        model_path=model_path,
        label_map_path=label_path,
        device=args.device,
        camera_id=args.camera,
        log_file=args.log_file,
        enable_logging=args.enable_log  # åªæœ‰åœ¨æŒ‡å®š --enable-log æ™‚æ‰å•Ÿç”¨
    )
    
    recognizer.run()


if __name__ == "__main__":
    main()

