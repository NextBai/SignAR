"""
æ‰‹èªå½±ç‰‡å‰è™•ç†è…³æœ¬

æ¨¡çµ„æ¶æ§‹ï¼š
1. VideoProcessorï¼ˆæ ¸å¿ƒè™•ç†å™¨ï¼‰- è¾¨è­˜æ™‚ä¹Ÿæœƒç”¨åˆ°
   - æ™ºèƒ½äººé«”æª¢æ¸¬å’Œè£åˆ‡ï¼ˆMediaPipe Poseï¼‰
   - å½±ç‰‡æ¨™æº–åŒ–ï¼ˆå¹€æ•¸ã€FPSã€è§£æåº¦ï¼‰
   
2. DataAugmentorï¼ˆæ•¸æ“šå¢å¼·å™¨ï¼‰- åƒ…è¨“ç·´å‰è™•ç†ä½¿ç”¨
   - æ¥µè¼•å¾®æ—‹è½‰ã€ç¸®æ”¾ã€äº®åº¦èª¿æ•´
   - å‰µå»ºå¤šå€‹å¢å¼·ç‰ˆæœ¬
"""
import os
import cv2
import numpy as np
from tqdm import tqdm
from concurrent.futures import ThreadPoolExecutor
import zipfile
import psutil
import mediapipe as mp
import threading
import asyncio
from functools import partial


# ==================== MediaPipe ç·šç¨‹æ± ç®¡ç† ====================

class MediaPipePosePool:
    """
    MediaPipe Pose ç·šç¨‹æ± ç®¡ç†
    - æ¯å€‹ç·šç¨‹ä½¿ç”¨ç¨ç«‹çš„æª¢æ¸¬å™¨å¯¦ä¾‹ï¼ˆé¿å…æ™‚é–“æˆ³è¨˜è¡çªï¼‰
    - ä½¿ç”¨ ThreadLocal ç¢ºä¿ç·šç¨‹éš”é›¢
    - è‡ªå‹•è³‡æºç®¡ç†
    
    æ ¹æ“š MediaPipe è¨­è¨ˆåŸç†ï¼Œç¢ºä¿æ¯å€‹ç·šç¨‹ç¨ç«‹å¯¦ä¾‹åŒ–å¯ä»¥ï¼š
    1. é¿å…è³‡æºç«¶çˆ­å’Œæ•¸æ“šæ··æ·†
    2. å……åˆ†åˆ©ç”¨ CPU/GPU ä¸¦è¡Œè™•ç†èƒ½åŠ›
    3. é˜²æ­¢æ™‚é–“æˆ³è¨˜ä¸ä¸€è‡´å•é¡Œ
    """
    _thread_local = threading.local()
    _lock = threading.Lock()
    _all_instances = []
    
    @classmethod
    def get_detector(cls, min_detection_confidence=0.5):
        """
        ç²å–ç•¶å‰ç·šç¨‹å°ˆç”¨çš„ Pose æª¢æ¸¬å™¨å¯¦ä¾‹
        
        Args:
            min_detection_confidence: æœ€ä½æª¢æ¸¬ä¿¡å¿ƒåº¦
        
        Returns:
            MediaPipe Pose æª¢æ¸¬å™¨å¯¦ä¾‹ï¼ˆç·šç¨‹å°ˆç”¨ï¼‰
        """
        # æª¢æŸ¥ç•¶å‰ç·šç¨‹æ˜¯å¦å·²æœ‰å¯¦ä¾‹
        if not hasattr(cls._thread_local, 'detector') or cls._thread_local.detector is None:
            with cls._lock:
                # ç‚ºç•¶å‰ç·šç¨‹å‰µå»ºç¨ç«‹çš„å¯¦ä¾‹
                mp_pose = mp.solutions.pose
                detector = mp_pose.Pose(
                    static_image_mode=False,
                    model_complexity=1,
                    min_detection_confidence=min_detection_confidence,
                    min_tracking_confidence=0.5
                )
                cls._thread_local.detector = detector
                cls._all_instances.append(detector)
        
        return cls._thread_local.detector
    
    @classmethod
    def close_thread_detector(cls):
        """æ¸…ç†ç•¶å‰ç·šç¨‹çš„æª¢æ¸¬å™¨"""
        if hasattr(cls._thread_local, 'detector') and cls._thread_local.detector is not None:
            cls._thread_local.detector.close()
            cls._thread_local.detector = None
    
    @classmethod
    def close_all(cls):
        """æ¸…ç†æ‰€æœ‰ç·šç¨‹çš„æª¢æ¸¬å™¨"""
        with cls._lock:
            for detector in cls._all_instances:
                try:
                    detector.close()
                except:
                    pass
            cls._all_instances.clear()


class SelfieSegmentationPool:
    """
    MediaPipe SelfieSegmentation ç·šç¨‹æ± ç®¡ç†
    - æ¯å€‹ç·šç¨‹ä½¿ç”¨ç¨ç«‹çš„åˆ†å‰²å™¨å¯¦ä¾‹ï¼ˆé¿å…æ™‚é–“æˆ³è¨˜è¡çªï¼‰
    - ä½¿ç”¨ ThreadLocal ç¢ºä¿ç·šç¨‹éš”é›¢
    - è‡ªå‹•è³‡æºç®¡ç†
    """
    _thread_local = threading.local()
    _lock = threading.Lock()
    _all_instances = []

    @classmethod
    def get_segmenter(cls, model_selection=1):
        """
        ç²å–ç•¶å‰ç·šç¨‹å°ˆç”¨çš„ SelfieSegmentation å¯¦ä¾‹

        Args:
            model_selection: æ¨¡å‹é¸æ“‡ (0=ä¸€èˆ¬å“è³ª/é€Ÿåº¦, 1=è¼ƒé«˜å“è³ª/è¼ƒæ…¢é€Ÿåº¦)

        Returns:
            MediaPipe SelfieSegmentation å¯¦ä¾‹ï¼ˆç·šç¨‹å°ˆç”¨ï¼‰
        """
        # æª¢æŸ¥ç•¶å‰ç·šç¨‹æ˜¯å¦å·²æœ‰å¯¦ä¾‹
        if not hasattr(cls._thread_local, 'segmenter') or cls._thread_local.segmenter is None:
            with cls._lock:
                # ç‚ºç•¶å‰ç·šç¨‹å‰µå»ºç¨ç«‹çš„å¯¦ä¾‹
                mp_selfie_segmentation = mp.solutions.selfie_segmentation
                segmenter = mp_selfie_segmentation.SelfieSegmentation(
                    model_selection=model_selection
                )
                cls._thread_local.segmenter = segmenter
                cls._all_instances.append(segmenter)
        
        return cls._thread_local.segmenter

    @classmethod
    def close_thread_segmenter(cls):
        """æ¸…ç†ç•¶å‰ç·šç¨‹çš„åˆ†å‰²å™¨"""
        if hasattr(cls._thread_local, 'segmenter') and cls._thread_local.segmenter is not None:
            cls._thread_local.segmenter.close()
            cls._thread_local.segmenter = None

    @classmethod
    def close_all(cls):
        """æ¸…ç†æ‰€æœ‰ç·šç¨‹çš„åˆ†å‰²å™¨"""
        with cls._lock:
            for segmenter in cls._all_instances:
                try:
                    segmenter.close()
                except:
                    pass
            cls._all_instances.clear()


# ==================== æ•¸æ“šå¢å¼·å¿«å–æ©Ÿåˆ¶ ====================

class AugmentationCache:
    """
    å¢å¼·çµæœå¿«å– - é¿å…é‡è¤‡è¨ˆç®—ç›¸åŒçš„å¢å¼·æ“ä½œ
    """
    def __init__(self):
        self._cache = {}
        self._lock = threading.Lock()
    
    def get_key(self, frame_hash, augmentation_type):
        """ç”Ÿæˆå¿«å–éµ"""
        return f"{frame_hash}_{augmentation_type}"
    
    def get(self, frame_hash, augmentation_type):
        """ç²å–å¿«å–çš„å¢å¼·çµæœ"""
        key = self.get_key(frame_hash, augmentation_type)
        with self._lock:
            return self._cache.get(key)
    
    def set(self, frame_hash, augmentation_type, result):
        """ä¿å­˜å¢å¼·çµæœåˆ°å¿«å–"""
        key = self.get_key(frame_hash, augmentation_type)
        with self._lock:
            self._cache[key] = result
    
    def clear(self):
        """æ¸…ç†å¿«å–"""
        with self._lock:
            self._cache.clear()


# ==================== æ•¸æ“šå¢å¼·æ¨¡çµ„ï¼ˆç¨ç«‹ï¼‰ ====================
class DataAugmentor:
    """
    æ•¸æ“šå¢å¼·å™¨ - æ‰‹èªè¾¨è­˜æœ€ä½³åŒ–ç‰ˆæœ¬
    
    å¢å¼·ç­–ç•¥ï¼ˆä¿ç•™èªæ„çš„å¾®èª¿ï¼‰ï¼š
    1. åŸºç¤ç‰ˆæœ¬ï¼šOriginal (åŸå§‹) å’Œ Mirror (æ°´å¹³ç¿»è½‰)
    2. å…‰ç…§å¢å¼·ï¼šäº®åº¦èª¿æ•´ (Â±2%)ã€å°æ¯”åº¦èª¿æ•´ (Â±3%)
    3. è¦–è¦ºå¢å¼·ï¼šé£½å’Œåº¦èª¿æ•´ (Â±3%)ã€è¼•å¾®é«˜æ–¯æ¨¡ç³Š
    4. å¹¾ä½•å¢å¼·ï¼šæ—‹è½‰ (Â±1Â°)
    
    çµ„åˆç­–ç•¥ï¼š
    - Original + Original_Brightness + Original_Contrast + Original_Saturation
    - Mirror + Mirror_Brightness + Mirror_Contrast + Mirror_Saturation
    = å…± 8 å€‹ç‰ˆæœ¬
    """
    
    def __init__(self, cache=None):
        # å¢å¼·åƒæ•¸ - æ¥µç‚ºä¿å®ˆï¼ˆé‡å°æ‰‹èªè¾¨è­˜ï¼‰
        self.augmentation_params = {
            'rotation_range': (-1, 1),              # Â±1Â° æ—‹è½‰
            'brightness_range': (0.98, 1.02),      # Â±2% äº®åº¦
            'contrast_range': (0.97, 1.03),        # Â±3% å°æ¯”åº¦
            'saturation_range': (0.97, 1.03),      # Â±3% é£½å’Œåº¦
            'blur_sigma': 0.3,                      # è¼•å¾®é«˜æ–¯æ¨¡ç³Š
        }
        self.cache = cache if cache is not None else AugmentationCache()
    
    def _adjust_brightness(self, frame, factor):
        """èª¿æ•´äº®åº¦"""
        adjusted = frame.astype(np.float32)
        adjusted = np.clip(adjusted * factor, 0, 255)
        return adjusted.astype(np.uint8)
    
    def _adjust_contrast(self, frame, factor):
        """èª¿æ•´å°æ¯”åº¦ (ç›¸å°æ–¼ä¸­å€¼ 128)"""
        adjusted = frame.astype(np.float32)
        adjusted = 128 + (adjusted - 128) * factor
        adjusted = np.clip(adjusted, 0, 255)
        return adjusted.astype(np.uint8)
    
    def _adjust_saturation(self, frame):
        """èª¿æ•´é£½å’Œåº¦ - è½‰æ›ç‚º HSV èª¿æ•´ S é€šé“"""
        # BGR to HSV
        hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV).astype(np.float32)
        # èª¿æ•´ S é€šé“ (Â±3%)
        saturation_factor = np.random.RandomState(42).choice([0.97, 1.03])
        hsv[:, :, 1] = np.clip(hsv[:, :, 1] * saturation_factor, 0, 255)
        # HSV to BGR
        hsv = hsv.astype(np.uint8)
        bgr = cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR)
        return bgr
    
    def _apply_blur(self, frame, sigma):
        """æ‡‰ç”¨è¼•å¾®é«˜æ–¯æ¨¡ç³Š"""
        if sigma > 0:
            kernel_size = 3
            return cv2.GaussianBlur(frame, (kernel_size, kernel_size), sigma)
        return frame
    
    def apply_augmentation(self, frame, augmentation_type=None):
        """
        ç¢ºå®šæ€§æ•¸æ“šå¢å¼·ï¼ˆå¿«å–ç‰ˆæœ¬ï¼‰
        
        Args:
            frame: è¼¸å…¥å¹€ [H, W, 3]
            augmentation_type: å¢å¼·é¡å‹
                - None: åŸå§‹å¹€
                - 'mirror': æ°´å¹³ç¿»è½‰
                - 'brightness': äº®åº¦èª¿æ•´
                - 'contrast': å°æ¯”åº¦èª¿æ•´
                - 'saturation': é£½å’Œåº¦èª¿æ•´
                - 'rotation': æ—‹è½‰
                - è¤‡åˆ: 'mirror+brightness' ç­‰
        
        Returns:
            å¢å¼·å¾Œçš„å¹€ [H, W, 3]
        """
        # è¨ˆç®—å¹€çš„é›œæ¹Šå€¼ç”¨æ–¼å¿«å–
        frame_hash = hash(frame.tobytes())
        
        # å˜—è©¦å¾å¿«å–ç²å–
        cached_result = self.cache.get(frame_hash, augmentation_type)
        if cached_result is not None:
            return cached_result
        
        augmented = frame.astype(np.uint8)
        
        # è§£æè¤‡åˆå¢å¼·é¡å‹ï¼ˆä¾‹å¦‚ "mirror+brightness"ï¼‰
        aug_list = augmentation_type.split('+') if augmentation_type else []
        
        for aug_type in aug_list:
            aug_type = aug_type.strip()
            
            if aug_type == 'mirror':
                augmented = cv2.flip(augmented, 1)
            
            elif aug_type == 'brightness':
                factor = np.random.RandomState(42).choice([0.98, 1.02])
                augmented = self._adjust_brightness(augmented, factor)
            
            elif aug_type == 'contrast':
                factor = np.random.RandomState(42).choice([0.97, 1.03])
                augmented = self._adjust_contrast(augmented, factor)
            
            elif aug_type == 'saturation':
                augmented = self._adjust_saturation(augmented)
            
            elif aug_type == 'rotation':
                angle = np.random.RandomState(42).choice([-1, 1])
                h, w = augmented.shape[:2]
                center = (w // 2, h // 2)
                rotation_matrix = cv2.getRotationMatrix2D(center, angle, 1.0)
                augmented = cv2.warpAffine(augmented, rotation_matrix, (w, h),
                                         borderMode=cv2.BORDER_REFLECT)
            
            elif aug_type == 'blur':
                augmented = self._apply_blur(augmented, self.augmentation_params['blur_sigma'])
        
        result = augmented.astype(np.uint8)
        
        # ä¿å­˜åˆ°å¿«å–
        self.cache.set(frame_hash, augmentation_type, result)
        
        return result
    
    def create_augmented_versions(self, frames, output_base_path, save_func):
        """
        ç‚ºçµ¦å®šçš„å¹€åºåˆ—å‰µå»ºå¢å¼·ç‰ˆæœ¬ä¸¦ä¿å­˜
        
        å¢å¼·çµ„åˆï¼ˆå…± 8 å€‹ç‰ˆæœ¬ï¼‰ï¼š
        1. Original
        2. Original + Brightness
        3. Original + Contrast
        4. Original + Saturation
        5. Mirror
        6. Mirror + Brightness
        7. Mirror + Contrast
        8. Mirror + Saturation
        
        Args:
            frames: æ¨™æº–åŒ–å¾Œçš„å¹€åˆ—è¡¨
            output_base_path: è¼¸å‡ºåŸºç¤è·¯å¾‘ï¼ˆä¸å«å‰¯æª”åï¼‰
            save_func: ä¿å­˜å½±ç‰‡çš„å‡½æ•¸ï¼Œç°½åç‚º save_func(frames, output_path)
        
        Returns:
            æˆåŠŸä¿å­˜çš„ç‰ˆæœ¬æ•¸é‡
        """
        # å®šç¾©å¢å¼·çµ„åˆ
        augmentation_configs = [
            # Original ç³»åˆ—
            (None, ''),
            ('brightness', '_brightness'),
            ('contrast', '_contrast'),
            ('saturation', '_saturation'),
            # Mirror ç³»åˆ—
            ('mirror', '_mirror'),
            ('mirror+brightness', '_mirror_brightness'),
            ('mirror+contrast', '_mirror_contrast'),
            ('mirror+saturation', '_mirror_saturation'),
        ]
        
        success_count = 0
        output_ext = '.mp4'
        
        for aug_type, suffix in augmentation_configs:
            # æ‡‰ç”¨å¢å¼·
            if aug_type is None:
                augmented_frames = frames
            else:
                augmented_frames = [self.apply_augmentation(frame, aug_type) for frame in frames]
            
            # ä¿å­˜
            output_file = output_base_path + suffix + output_ext
            if save_func(augmented_frames, output_file):
                success_count += 1
        
        return success_count


# ==================== æ ¸å¿ƒå½±ç‰‡è™•ç†å™¨ ====================

class VideoProcessor:
    """
    æ ¸å¿ƒå½±ç‰‡è™•ç†å™¨ - è¾¨è­˜æ™‚ä¹Ÿæœƒç”¨åˆ°

    åŠŸèƒ½ï¼š
    1. æ™ºèƒ½äººé«”æª¢æ¸¬å’Œè£åˆ‡ï¼ˆMediaPipe Poseï¼‰
    2. èƒŒæ™¯ç§»é™¤ï¼ˆMediaPipe SelfieSegmentationï¼‰
    3. å½±ç‰‡æ¨™æº–åŒ–ï¼ˆFPSã€è§£æåº¦ï¼‰
    4. å½±ç‰‡ç·¨ç¢¼å„ªåŒ–ï¼ˆh.264ï¼‰

    æ³¨æ„ï¼š
    - ä¸åŒ…å«æ•¸æ“šå¢å¼·åŠŸèƒ½ï¼æ•¸æ“šå¢å¼·è«‹ä½¿ç”¨ DataAugmentor
    - ä¸å¼·åˆ¶å¹€æ•¸æ¨™æº–åŒ–ï¼ç”±å‘¼å«è€…æ±ºå®šç›®æ¨™å¹€æ•¸
    """
    
    # æ¨™æº–åŒ–åƒæ•¸
    TARGET_FPS = 30         # ç›®æ¨™å¹€ç‡
    TARGET_WIDTH = 224      # ç›®æ¨™å¯¬åº¦
    TARGET_HEIGHT = 224     # ç›®æ¨™é«˜åº¦
    TARGET_BITRATE = 2000   # ç›®æ¨™æ¯”ç‰¹ç‡ (kbps)
    OUTPUT_EXT = '.mp4'     # çµ±ä¸€è¼¸å‡ºæ ¼å¼
    
    # äººé«”è£åˆ‡åƒæ•¸
    CROP_PADDING = 0.40     # é‚Šç•Œæ¡†æ“´å±•æ¯”ä¾‹ï¼ˆ40%ï¼‰- å¤§å¹…å¢åŠ è£åˆ‡å€åŸŸ
    MIN_DETECTION_CONFIDENCE = 0.5  # æœ€ä½æª¢æ¸¬ä¿¡å¿ƒåº¦

    # èƒŒæ™¯ç§»é™¤åƒæ•¸
    DEFAULT_BG_COLOR = (0, 255, 0)   # é è¨­èƒŒæ™¯é¡è‰² (ç¶ å¹• BGR æ ¼å¼)
    SEGMENTATION_MODEL = 1            # SelfieSegmentation æ¨¡å‹é¸æ“‡ (1=è¼ƒé«˜å“è³ª)
    
    # é‚Šç•Œå„ªåŒ–åƒæ•¸
    MASK_THRESHOLD = 0.5              # é®ç½©äºŒå€¼åŒ–é–¾å€¼
    MORPHOLOGY_KERNEL_SIZE = 5        # å½¢æ…‹å­¸æ“ä½œæ ¸å¿ƒå¤§å°
    MORPHOLOGY_ITERATIONS = 2         # å½¢æ…‹å­¸æ“ä½œè¿­ä»£æ¬¡æ•¸
    BLUR_KERNEL_SIZE = 5              # é«˜æ–¯æ¨¡ç³Šæ ¸å¿ƒå¤§å°ï¼ˆå¹³æ»‘é‚Šç•Œï¼‰

    def __init__(self, enable_cropping=True, enable_background_removal=False, target_frames=None):
        """
        åˆå§‹åŒ–å½±ç‰‡è™•ç†å™¨

        Args:
            enable_cropping: æ˜¯å¦å•Ÿç”¨æ™ºèƒ½äººé«”è£åˆ‡ï¼ˆé è¨­é–‹å•Ÿï¼‰
            enable_background_removal: æ˜¯å¦å•Ÿç”¨èƒŒæ™¯ç§»é™¤ï¼ˆé è¨­é—œé–‰ï¼‰
            target_frames: ç›®æ¨™å¹€æ•¸ï¼ˆNone=ä¸å¼·åˆ¶æ¨™æº–åŒ–ï¼Œç”±å‘¼å«è€…æ±ºå®šï¼‰
        """
        # æª¢æŸ¥æ˜¯å¦æœ‰ffmpegï¼ˆç”¨æ–¼å¾Œè™•ç†ç¢ºä¿h264ç·¨ç¢¼ï¼‰
        self.has_ffmpeg = self._check_ffmpeg()

        # æ˜¯å¦å•Ÿç”¨æ™ºèƒ½è£åˆ‡
        self.enable_cropping = enable_cropping

        # æ˜¯å¦å•Ÿç”¨èƒŒæ™¯ç§»é™¤
        self.enable_background_removal = enable_background_removal

        # ç›®æ¨™å¹€æ•¸ï¼ˆè¨“ç·´æ™‚ä½¿ç”¨ï¼Œè¾¨è­˜æ™‚ç”± SlidingWindowInference æ±ºå®šï¼‰
        self.target_frames = target_frames

        # ç¬¬ä¸€å¹€è£åˆ‡åƒæ•¸ï¼ˆç”¨æ–¼å›ºå®šè£åˆ‡å€åŸŸï¼Œæ¯å€‹å½±ç‰‡ç¨ç«‹ï¼‰
        self.fixed_crop_params = None

        # ä½¿ç”¨ç·šç¨‹æ± ç®¡ç†çš„ MediaPipe Poseï¼ˆæ¯å€‹ç·šç¨‹ç¨ç«‹å¯¦ä¾‹ï¼‰
        if self.enable_cropping:
            self.pose_detector = MediaPipePosePool.get_detector(
                min_detection_confidence=self.MIN_DETECTION_CONFIDENCE
            )
        else:
            self.pose_detector = None

        # ä½¿ç”¨ç·šç¨‹æ± ç®¡ç†çš„ SelfieSegmentationï¼ˆæ¯å€‹ç·šç¨‹ç¨ç«‹å¯¦ä¾‹ï¼‰
        if self.enable_background_removal:
            self.segmenter = SelfieSegmentationPool.get_segmenter(
                model_selection=self.SEGMENTATION_MODEL
            )
        else:
            self.segmenter = None

    def _check_ffmpeg(self):
        """æª¢æŸ¥ç³»çµ±æ˜¯å¦æœ‰ffmpeg"""
        import subprocess
        try:
            subprocess.run(['ffmpeg', '-version'], 
                         stdout=subprocess.DEVNULL, 
                         stderr=subprocess.DEVNULL, 
                         check=True)
            return True
        except (subprocess.CalledProcessError, FileNotFoundError):
            return False
    
    def detect_and_crop_person(self, frame, is_first_frame=False):
        """
        æª¢æ¸¬ä¸¦è£åˆ‡ç•«é¢ä¸­ã€Œæœ€å‰é¢ã€çš„äººçš„å®Œæ•´ä¸ŠåŠèº«
        
        ä½¿ç”¨ MediaPipe Pose æª¢æ¸¬äººé«”é—œéµé»ï¼Œè¨ˆç®—åŒ…å«ï¼š
        - é ­éƒ¨ï¼ˆé¼»å­ã€çœ¼ç›ã€è€³æœµï¼‰
        - ä¸ŠåŠèº«ï¼ˆè‚©è†€ã€æ‰‹è‚˜ã€æ‰‹è…•ï¼‰
        - å®Œæ•´æ‰‹è‡‚ï¼ˆç¢ºä¿ä¸è£åˆ‡ï¼‰
        
        Args:
            frame: è¼¸å…¥å¹€ [H, W, 3] BGR
            is_first_frame: æ˜¯å¦ç‚ºå½±ç‰‡çš„ç¬¬ä¸€å¹€ï¼ˆç”¨æ–¼å›ºå®šè£åˆ‡åƒæ•¸ï¼‰
        
        Returns:
            cropped_frame: è£åˆ‡å¾Œçš„å¹€ï¼ˆå¦‚æœæª¢æ¸¬å¤±æ•—è¿”å›åŸå§‹å¹€ï¼‰
            success: æ˜¯å¦æˆåŠŸæª¢æ¸¬ä¸¦è£åˆ‡
        """
        if not self.enable_cropping or self.pose_detector is None:
            return frame, False
        
        h, w = frame.shape[:2]
        
        # å¦‚æœå·²ç¶“æœ‰å›ºå®šè£åˆ‡åƒæ•¸ä¸”ä¸æ˜¯ç¬¬ä¸€å¹€ï¼Œç›´æ¥ä½¿ç”¨
        if self.fixed_crop_params is not None and not is_first_frame:
            x_min, y_min, x_max, y_max = self.fixed_crop_params
            # ç¢ºä¿è£åˆ‡å€åŸŸåœ¨ç•¶å‰å¹€çš„ç¯„åœå…§
            x_min = max(0, min(x_min, w-1))
            x_max = max(x_min+1, min(x_max, w))
            y_min = max(0, min(y_min, h-1))
            y_max = max(y_min+1, min(y_max, h))
            
            cropped = frame[y_min:y_max, x_min:x_max]
            return cropped if cropped.size > 0 else frame, cropped.size > 0
        
        # è½‰æ›ç‚º RGBï¼ˆMediaPipe éœ€è¦ RGBï¼‰
        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        
        # åŸ·è¡Œå§¿æ…‹æª¢æ¸¬
        results = self.pose_detector.process(frame_rgb)
        
        # å¦‚æœæ²’æœ‰æª¢æ¸¬åˆ°äººé«”ï¼Œè¿”å›åŸå§‹å¹€
        if not results.pose_landmarks:
            return frame, False
        
        # æå–é—œéµé»
        landmarks = results.pose_landmarks.landmark
        
        # å®šç¾©éœ€è¦åŒ…å«çš„é—œéµé»ï¼ˆä¸»è¦è¿½ç„¦é ­éƒ¨ + è‚©è†€ï¼Œé¿å…æ‰‹è‡‚å¹²æ“¾ï¼‰
        # MediaPipe Pose é—œéµé»ç´¢å¼•ï¼š
        # 0: é¼»å­, 1-2: çœ¼ç›, 3-4: è€³æœµ
        # 11-12: è‚©è†€, 13-14: æ‰‹è‚˜, 15-16: æ‰‹è…•, 17-22: æ‰‹æŒ‡
        head_and_shoulders_indices = [
            0,   # é¼»å­ï¼ˆé ­éƒ¨ä¸­å¿ƒï¼‰
            1, 2,  # å·¦å³çœ¼ç›
            3, 4,  # å·¦å³è€³æœµ
            11, 12,  # å·¦å³è‚©è†€ï¼ˆæä¾›ä¸ŠåŠèº«ç©©å®šæ€§ï¼‰
        ]
        
        # è¨ˆç®—é‚Šç•Œæ¡†
        x_coords = []
        y_coords = []
        
        for idx in head_and_shoulders_indices:
            if idx < len(landmarks):
                landmark = landmarks[idx]
                # æª¢æŸ¥é—œéµé»å¯è¦‹æ€§ï¼ˆvisibility > 0.5 è¡¨ç¤ºå¯è¦‹ï¼‰
                if landmark.visibility > 0.5:
                    x_coords.append(landmark.x * w)
                    y_coords.append(landmark.y * h)
        
        # å¦‚æœæª¢æ¸¬åˆ°çš„é—œéµé»å¤ªå°‘ï¼Œè¿”å›åŸå§‹å¹€
        if len(x_coords) < 4:
            return frame, False
        
        # è¨ˆç®—é‚Šç•Œæ¡†
        x_min = int(min(x_coords))
        x_max = int(max(x_coords))
        y_min = int(min(y_coords))
        y_max = int(max(y_coords))
        
        # æ·»åŠ  paddingï¼ˆç¢ºä¿ä¸è£åˆ‡åˆ°æ‰‹è‡‚ï¼‰
        bbox_width = x_max - x_min
        bbox_height = y_max - y_min
        
        padding_x = int(bbox_width * self.CROP_PADDING)
        padding_y = int(bbox_height * self.CROP_PADDING)
        
        x_min = max(0, x_min - padding_x)
        x_max = min(w, x_max + padding_x)
        y_min = max(0, y_min - padding_y)
        y_max = min(h, y_max + padding_y)
        
        # å›ºå®šè£åˆ‡å€åŸŸå¤§å°ï¼Œé¿å…è¦–è¦ºä¸Šçš„æ”¾å¤§ç¸®å°
        # ä½¿ç”¨å›ºå®šçš„é‚Šé•·ï¼ˆå–åŸå§‹é‚Šç•Œæ¡†çš„æœ€å¤§å€¼ä½œç‚ºåŸºæº–ï¼Œä¸¦å¢åŠ 60%ä»¥ç²å¾—æ›´å¤§å€åŸŸï¼‰
        base_size = max(x_max - x_min, y_max - y_min)
        fixed_size = int(base_size * 1.6)  # å¢åŠ 60%è®“è£åˆ‡æ¡†æ›´å¤§
        
        # ä»¥æª¢æ¸¬åˆ°çš„ä¸­å¿ƒç‚ºåŸºæº–ï¼Œå‰µå»ºå›ºå®šå¤§å°çš„è£åˆ‡å€åŸŸ
        # ç¨å¾®å¾€ä¸‹èª¿æ•´ä¸­å¿ƒé»ï¼Œé¿å…è£åˆ‡æ¡†åä¸Š
        center_x = (x_min + x_max) // 2
        center_y = (y_min + y_max) // 2
        
        # å¾€ä¸‹èª¿æ•´ä¸­å¿ƒé» 10%ï¼Œè®“è£åˆ‡æ¡†åŒ…å«æ›´å¤šä¸‹æ–¹å€åŸŸ
        center_y = int(center_y * 1.1)
        
        half_size = fixed_size // 2
        x_min = max(0, center_x - half_size)
        x_max = min(w, center_x + half_size)
        y_min = max(0, center_y - half_size)
        y_max = min(h, center_y + half_size)
        
        # å¦‚æœè£åˆ‡å€åŸŸå°æ–¼å›ºå®šå¤§å°ï¼Œå˜—è©¦å±…ä¸­æ“´å±•
        if x_max - x_min < fixed_size:
            expand = (fixed_size - (x_max - x_min)) // 2
            x_min = max(0, x_min - expand)
            x_max = min(w, x_max + expand)
        
        if y_max - y_min < fixed_size:
            expand = (fixed_size - (y_max - y_min)) // 2
            y_min = max(0, y_min - expand)
            y_max = min(h, y_max + expand)
        
        # å¦‚æœæ˜¯ç¬¬ä¸€å¹€ï¼Œå„²å­˜è£åˆ‡åƒæ•¸
        if is_first_frame:
            self.fixed_crop_params = (x_min, y_min, x_max, y_max)
        
        # è£åˆ‡
        cropped = frame[y_min:y_max, x_min:x_max]
        
        # æª¢æŸ¥è£åˆ‡çµæœæ˜¯å¦æœ‰æ•ˆ
        if cropped.size == 0:
            return frame, False
        
        return cropped, True

    def apply_background_removal(self, frame, bg_color=None):
        """
        æ‡‰ç”¨èƒŒæ™¯ç§»é™¤åˆ°å–®ä¸€å¹€ï¼ˆå„ªåŒ–ç‰ˆï¼šé‚Šç•Œå¹³æ»‘è™•ç†ï¼‰

        å„ªåŒ–ç­–ç•¥ï¼š
        1. æé«˜è¼¸å…¥è§£æåº¦ï¼ˆä¿æŒåŸå§‹è§£æåº¦è™•ç†ï¼‰
        2. å½¢æ…‹å­¸æ“ä½œå¹³æ»‘é‚Šç•Œï¼ˆè†¨è„¹+ä¾µè•ï¼‰
        3. é«˜æ–¯æ¨¡ç³Šæ¸›å°‘é‹¸é½’
        4. ç¶ å¹•èƒŒæ™¯ä¾¿æ–¼è§€å¯Ÿèª¿æ•´

        Args:
            frame: è¼¸å…¥å¹€ [H, W, 3] BGR
            bg_color: èƒŒæ™¯é¡è‰² (B, G, R)ï¼Œé è¨­ç‚ºç¶ å¹•

        Returns:
            è™•ç†å¾Œçš„å¹€ [H, W, 3] BGRï¼ˆå‰æ™¯ä¿æŒï¼ŒèƒŒæ™¯ç‚ºç¶ å¹•ï¼‰
        """
        if not self.enable_background_removal or self.segmenter is None:
            return frame

        if bg_color is None:
            bg_color = self.DEFAULT_BG_COLOR

        try:
            h, w = frame.shape[:2]

            # æ­¥é©Ÿ1: è½‰æ›ç‚º RGBï¼ˆMediaPipe éœ€è¦ RGB æ ¼å¼ï¼‰
            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)

            # æ­¥é©Ÿ2: é€²è¡Œåˆ†å‰²ï¼ˆä½¿ç”¨åŸå§‹è§£æåº¦æå‡ç²¾åº¦ï¼‰
            results = self.segmenter.process(frame_rgb)
            mask = results.segmentation_mask

            if mask is not None:
                # ç¢ºä¿é®ç½©å’Œå¹€æœ‰ç›¸åŒçš„å°ºå¯¸
                mask = cv2.resize(mask, (w, h))

                # æ­¥é©Ÿ3: äºŒå€¼åŒ–é®ç½©ï¼ˆæé«˜å°æ¯”åº¦ï¼‰
                mask_binary = (mask > self.MASK_THRESHOLD).astype(np.uint8)

                # æ­¥é©Ÿ4: å½¢æ…‹å­¸æ“ä½œå¹³æ»‘é‚Šç•Œ
                # 4.1 è†¨è„¹æ“ä½œï¼šå¡«è£œäººé«”å…§éƒ¨çš„å°å­”æ´
                kernel = np.ones((self.MORPHOLOGY_KERNEL_SIZE, self.MORPHOLOGY_KERNEL_SIZE), np.uint8)
                mask_dilated = cv2.dilate(mask_binary, kernel, iterations=self.MORPHOLOGY_ITERATIONS)

                # 4.2 ä¾µè•æ“ä½œï¼šç¸®å°é‚Šç•Œï¼Œç§»é™¤å™ªé»
                mask_eroded = cv2.erode(mask_dilated, kernel, iterations=self.MORPHOLOGY_ITERATIONS)

                # æ­¥é©Ÿ5: é«˜æ–¯æ¨¡ç³Šå¹³æ»‘é‚Šç•Œï¼ˆæ¸›å°‘é‹¸é½’ï¼‰
                mask_smooth = cv2.GaussianBlur(mask_eroded.astype(np.float32),
                                              (self.BLUR_KERNEL_SIZE, self.BLUR_KERNEL_SIZE), 0)

                # æ­¥é©Ÿ6: å‰µå»ºç¶ å¹•èƒŒæ™¯
                green_background = np.full((h, w, 3), bg_color, dtype=np.uint8)

                # æ­¥é©Ÿ7: æ ¹æ“šå¹³æ»‘é®ç½©åˆæˆæœ€çµ‚åœ–åƒ
                # ä½¿ç”¨åŠ æ¬Šæ··åˆç²å¾—å¹³æ»‘éæ¸¡
                mask_3channel = mask_smooth[:, :, np.newaxis]
                output_frame = (frame * mask_3channel + green_background * (1 - mask_3channel)).astype(np.uint8)

                return output_frame
            else:
                # å¦‚æœåˆ†å‰²å¤±æ•—ï¼Œè¿”å›åŸå§‹å¹€
                return frame

        except Exception as e:
            print(f"èƒŒæ™¯ç§»é™¤è™•ç†å¤±æ•—: {str(e)}")
            import traceback
            traceback.print_exc()
            return frame

    def normalize_frames(self, frames, target_count):
        """
        æ™ºèƒ½å¹€æ•¸æ¨™æº–åŒ–
        - å¦‚æœåŸå§‹å¹€æ•¸ > ç›®æ¨™å¹€æ•¸ï¼šå‡å‹»æ¡æ¨£
        - å¦‚æœåŸå§‹å¹€æ•¸ < ç›®æ¨™å¹€æ•¸ï¼šç·šæ€§æ’å€¼
        
        Args:
            frames: åŸå§‹å¹€åˆ—è¡¨
            target_count: ç›®æ¨™å¹€æ•¸
            
        Returns:
            æ¨™æº–åŒ–å¾Œçš„å¹€åˆ—è¡¨
        """
        original_count = len(frames)
        
        if original_count == target_count:
            return frames
        
        # ç”Ÿæˆæ¡æ¨£ç´¢å¼•
        indices = np.linspace(0, original_count - 1, target_count)
        
        normalized_frames = []
        for idx in indices:
            # å¦‚æœæ˜¯æ•´æ•¸ç´¢å¼•ï¼Œç›´æ¥å–å¹€
            if idx == int(idx):
                normalized_frames.append(frames[int(idx)])
            else:
                # ç·šæ€§æ’å€¼
                lower_idx = int(np.floor(idx))
                upper_idx = int(np.ceil(idx))
                weight = idx - lower_idx
                
                # æ··åˆå…©å¹€
                lower_frame = frames[lower_idx].astype(np.float32)
                upper_frame = frames[upper_idx].astype(np.float32)
                interpolated = (1 - weight) * lower_frame + weight * upper_frame
                normalized_frames.append(interpolated.astype(np.uint8))
        
        return normalized_frames
    
    def process_and_crop_frames(self, frames):
        """
        è™•ç†å¹€åºåˆ—ï¼šæ™ºèƒ½è£åˆ‡ + èƒŒæ™¯ç§»é™¤ + è§£æåº¦æ¨™æº–åŒ–

        æ³¨æ„ï¼šæ­¤å‡½æ•¸ä¸åŒ…å«æ•¸æ“šå¢å¼·ï¼åƒ…ç”¨æ–¼æ ¸å¿ƒè™•ç†

        Args:
            frames: è¼¸å…¥å¹€åˆ—è¡¨

        Returns:
            è™•ç†å¾Œçš„å¹€åˆ—è¡¨ï¼ˆæ¨™æº–åŒ–åˆ° 224x224ï¼‰
        """
        # é‡ç½®å›ºå®šè£åˆ‡åƒæ•¸ï¼ˆç‚ºæ¯å€‹æ–°å½±ç‰‡é‡ç½®ï¼‰
        self.fixed_crop_params = None

        processed_frames = []

        for i, frame in enumerate(frames):
            # 1. æ™ºèƒ½è£åˆ‡äººé«”ï¼ˆå¦‚æœå•Ÿç”¨ï¼‰
            if self.enable_cropping:
                frame_cropped, success = self.detect_and_crop_person(frame, is_first_frame=(i == 0))
                if success:
                    frame = frame_cropped

            # 2. èƒŒæ™¯ç§»é™¤ï¼ˆå¦‚æœå•Ÿç”¨ï¼‰
            if self.enable_background_removal:
                frame = self.apply_background_removal(frame)

            # 3. è§£æåº¦æ¨™æº–åŒ–
            frame_resized = cv2.resize(frame, (self.TARGET_WIDTH, self.TARGET_HEIGHT))
            processed_frames.append(frame_resized)

        return processed_frames

    def _convert_to_h264(self, input_video, output_video):
        """ä½¿ç”¨ffmpegè½‰æ›ç‚ºh264ç·¨ç¢¼ï¼Œä¸¦è¨­å®šæ¯”ç‰¹ç‡"""
        import subprocess
        
        cmd = [
            'ffmpeg',
            '-i', input_video,
            '-c:v', 'libx264',              # h264ç·¨ç¢¼
            '-b:v', f'{self.TARGET_BITRATE}k',  # æ¯”ç‰¹ç‡
            '-preset', 'medium',            # ç·¨ç¢¼é€Ÿåº¦
            '-movflags', '+faststart',      # å„ªåŒ–ä¸²æµ
            '-y',                           # è¦†è“‹è¼¸å‡º
            output_video
        ]
        
        try:
            subprocess.run(cmd, 
                         stdout=subprocess.DEVNULL, 
                         stderr=subprocess.DEVNULL, 
                         check=True)
            return True
        except subprocess.CalledProcessError:
            return False

    def _save_video_frames(self, frames, output_path):
        """
        å°‡å¹€åºåˆ—ä¿å­˜ç‚ºå½±ç‰‡æ–‡ä»¶

        Args:
            frames: å¹€åˆ—è¡¨
            output_path: è¼¸å‡ºå½±ç‰‡è·¯å¾‘

        Returns:
            ä¿å­˜æ˜¯å¦æˆåŠŸ
        """
        try:
            temp_output = output_path + '.temp.mp4'
            fourcc = cv2.VideoWriter_fourcc(*'mp4v')

            out = cv2.VideoWriter(
                temp_output,
                fourcc,
                self.TARGET_FPS,
                (self.TARGET_WIDTH, self.TARGET_HEIGHT)
            )

            if not out.isOpened():
                print(f"âŒ ç„¡æ³•å‰µå»ºè¼¸å‡ºå½±ç‰‡: {output_path}")
                return False

            for frame in frames:
                out.write(frame)

            out.release()

            # ä½¿ç”¨ffmpegè½‰æ›ç‚ºæ¨™æº–h264ç·¨ç¢¼ï¼ˆå¦‚æœæœ‰ffmpegï¼‰
            if self.has_ffmpeg:
                if self._convert_to_h264(temp_output, output_path):
                    # è½‰æ›æˆåŠŸï¼Œåˆªé™¤è‡¨æ™‚æª”æ¡ˆ
                    os.remove(temp_output)
                else:
                    # è½‰æ›å¤±æ•—ï¼Œä½¿ç”¨è‡¨æ™‚æª”æ¡ˆä½œç‚ºæœ€çµ‚è¼¸å‡º
                    print(f"âš ï¸  ffmpegè½‰æ›å¤±æ•—ï¼Œä½¿ç”¨mp4vç·¨ç¢¼: {os.path.basename(output_path)}")
                    os.rename(temp_output, output_path)
            else:
                # æ²’æœ‰ffmpegï¼Œç›´æ¥ä½¿ç”¨mp4vç·¨ç¢¼
                os.rename(temp_output, output_path)

            # é©—è­‰è¼¸å‡º
            verify_cap = cv2.VideoCapture(output_path)
            verify_frames = int(verify_cap.get(cv2.CAP_PROP_FRAME_COUNT))
            verify_cap.release()

            # æª¢æŸ¥æ˜¯å¦ç¬¦åˆè¦æ ¼ï¼ˆå…è¨±1å¹€çš„èª¤å·®ï¼Œå¦‚æœæœ‰è¨­å®šç›®æ¨™å¹€æ•¸ï¼‰
            if self.target_frames is not None and abs(verify_frames - self.target_frames) > 1:
                print(f"âš ï¸  è­¦å‘Š: {os.path.basename(output_path)} å¹€æ•¸ä¸ç¬¦ (æœŸæœ›{self.target_frames}, å¯¦éš›{verify_frames})")

            return True

        except Exception as e:
            print(f"âŒ ä¿å­˜å½±ç‰‡æ™‚å‡ºéŒ¯: {output_path}, éŒ¯èª¤: {str(e)}")
            return False

    def process_video(self, input_path, output_path, augmentor=None, target_frames=None):
        """
        è™•ç†å–®ä¸€å½±ç‰‡ï¼šæ¨™æº–åŒ– + å¯é¸çš„æ•¸æ“šå¢å¼·
        
        æ ¸å¿ƒè™•ç†æµç¨‹ï¼š
        1. å¹€æ•¸æ¨™æº–åŒ–ï¼ˆå¯é¸ï¼Œå¦‚æœæä¾› target_framesï¼‰
        2. æ™ºèƒ½è£åˆ‡äººé«”ï¼ˆå¯é¸ï¼‰
        3. è§£æåº¦æ¨™æº–åŒ–ï¼ˆ224x224ï¼‰
        4. æ•¸æ“šå¢å¼·ï¼ˆå¯é¸ï¼Œéœ€è¦æä¾› augmentorï¼‰
        
        å¦‚æœæä¾› augmentorï¼š
        - å‰µå»º8å€‹å¢å¼·ç‰ˆæœ¬ï¼ˆåŸå§‹ã€æ—‹è½‰ã€ç¸®æ”¾ã€äº®åº¦ Ã— ç„¡åè½‰/åè½‰ï¼‰
        
        å¦‚æœä¸æä¾› augmentorï¼š
        - åªè¼¸å‡º1å€‹æ¨™æº–åŒ–ç‰ˆæœ¬
        
        Args:
            input_path: è¼¸å…¥å½±ç‰‡è·¯å¾‘
            output_path: è¼¸å‡ºå½±ç‰‡è·¯å¾‘
            augmentor: DataAugmentor å¯¦ä¾‹ï¼ˆå¯é¸ï¼Œè¨“ç·´æ™‚ä½¿ç”¨ï¼‰
            target_frames: ç›®æ¨™å¹€æ•¸ï¼ˆå¯é¸ï¼Œè¦†è“‹ __init__ è¨­å®šï¼‰
        
        Returns:
            è™•ç†æ˜¯å¦æˆåŠŸ
        """
        try:
            # ç¢ºä¿è¼¸å‡ºå‰¯æª”åç‚º.mp4
            output_path_base = os.path.splitext(output_path)[0]

            # ç¢ºä¿è¼¸å‡ºç›®éŒ„å­˜åœ¨
            output_dir = os.path.dirname(output_path)
            if output_dir:  # åªæœ‰ç•¶æœ‰ç›®éŒ„éƒ¨åˆ†æ™‚æ‰å‰µå»º
                os.makedirs(output_dir, exist_ok=True)

            cap = cv2.VideoCapture(input_path)
            if not cap.isOpened():
                print(f"âŒ ç„¡æ³•é–‹å•Ÿå½±ç‰‡: {input_path}")
                return False

            # è®€å–æ‰€æœ‰å¹€
            original_frames = []
            while True:
                ret, frame = cap.read()
                if not ret:
                    break
                original_frames.append(frame)

            cap.release()

            if len(original_frames) == 0:
                print(f"âŒ å½±ç‰‡ç„¡æœ‰æ•ˆå¹€: {input_path}")
                return False

            # æ­¥é©Ÿ1: å¹€æ•¸æ¨™æº–åŒ–ï¼ˆå¯é¸ï¼‰
            target = target_frames if target_frames is not None else self.target_frames
            if target is not None:
                normalized_frames = self.normalize_frames(original_frames, target)
            else:
                # ä¸é€²è¡Œå¹€æ•¸æ¨™æº–åŒ–
                normalized_frames = original_frames

            # é‡ç½®å›ºå®šè£åˆ‡åƒæ•¸ï¼ˆç‚ºæ¯å€‹æ–°å½±ç‰‡é‡ç½®ï¼Œé¿å…ç·šç¨‹é–“å¹²æ“¾ï¼‰
            self.fixed_crop_params = None

            # æ­¥é©Ÿ2: æ™ºèƒ½è£åˆ‡äººé«”ï¼ˆåœ¨ resize ä¹‹å‰ï¼‰
            processed_frames = []
            crop_success_count = 0
            bg_removal_count = 0

            for i, frame in enumerate(normalized_frames):
                # 2.1 æ™ºèƒ½è£åˆ‡äººé«”ï¼ˆå¦‚æœå•Ÿç”¨ï¼‰
                if self.enable_cropping:
                    frame_cropped, success = self.detect_and_crop_person(frame, is_first_frame=(i == 0))
                    if success:
                        frame = frame_cropped
                        crop_success_count += 1

                # 2.2 èƒŒæ™¯ç§»é™¤ï¼ˆå¦‚æœå•Ÿç”¨ï¼‰
                if self.enable_background_removal:
                    frame = self.apply_background_removal(frame)
                    bg_removal_count += 1

                processed_frames.append(frame)

            # é¡¯ç¤ºè™•ç†çµ±è¨ˆ
            if self.enable_cropping:
                crop_rate = (crop_success_count / len(normalized_frames)) * 100
                if crop_rate < 50:
                    print(f"âš ï¸  è­¦å‘Š: {os.path.basename(input_path)} äººé«”æª¢æ¸¬ç‡è¼ƒä½ ({crop_rate:.1f}%)")


            # æ­¥é©Ÿ3: è§£æåº¦æ¨™æº–åŒ–
            resized_frames = []
            for frame in processed_frames:
                frame_resized = cv2.resize(frame, (self.TARGET_WIDTH, self.TARGET_HEIGHT))
                resized_frames.append(frame_resized)

            # æ­¥é©Ÿ4: æ•¸æ“šå¢å¼·ï¼ˆå¯é¸ï¼‰
            if augmentor is not None:
                # ä½¿ç”¨ DataAugmentor å‰µå»º8å€‹å¢å¼·ç‰ˆæœ¬ï¼ˆOriginal + Mirror å„é…ä»¥å…‰ç…§å¢å¼·ï¼‰
                success_count = augmentor.create_augmented_versions(
                    resized_frames,
                    output_path_base,
                    self._save_video_frames
                )
                return success_count == 8  # æ‰€æœ‰8å€‹ç‰ˆæœ¬éƒ½å¿…é ˆæˆåŠŸ
            else:
                # ä¸ä½¿ç”¨æ•¸æ“šå¢å¼·ï¼Œåªè¼¸å‡ºæ¨™æº–åŒ–ç‰ˆæœ¬
                output_file = output_path_base + self.OUTPUT_EXT
                return self._save_video_frames(resized_frames, output_file)

        except Exception as e:
            print(f"âŒ è™•ç†å½±ç‰‡æ™‚å‡ºéŒ¯: {input_path}, éŒ¯èª¤: {str(e)}")
            import traceback
            traceback.print_exc()
            return False

    def process_directory(self, input_dir, output_dir, max_workers=None, augmentor=None):
        """
        è™•ç†æ•´å€‹ç›®éŒ„ä¸‹çš„æ‰€æœ‰å½±ç‰‡
        
        å„ªåŒ–ï¼š
        1. æ¯å€‹å½±ç‰‡ä½¿ç”¨ç¨ç«‹çš„ VideoProcessorï¼ˆé¿å… fixed_crop_params å…±äº«ï¼‰
        2. å…¨åŸŸå…±äº« MediaPipe Poseï¼ˆæé«˜æ•ˆç‡ï¼‰
        3. å…±äº« AugmentationCacheï¼ˆé¿å…é‡è¤‡è¨ˆç®—ï¼‰
        
        Args:
            input_dir: è¼¸å…¥ç›®éŒ„
            output_dir: è¼¸å‡ºç›®éŒ„
            max_workers: å·¥ä½œç·šç¨‹æ•¸ï¼ˆNone=è‡ªå‹•æª¢æ¸¬ï¼‰
            augmentor: DataAugmentor å¯¦ä¾‹ï¼ˆå¯é¸ï¼Œè¨“ç·´æ™‚ä½¿ç”¨ï¼‰
        """
        if not os.path.exists(input_dir):
            print(f"âŒ è¼¸å…¥ç›®éŒ„ä¸å­˜åœ¨: {input_dir}")
            return
            
        # ç¢ºä¿è¼¸å‡ºç›®éŒ„å­˜åœ¨
        os.makedirs(output_dir, exist_ok=True)
        
        # CPU ç¡¬é«”å„ªåŒ–ï¼šè‡ªå‹•æª¢æ¸¬ CPU æ ¸å¿ƒæ•¸
        cpu_count = psutil.cpu_count(logical=True)
        if max_workers is None:
            # M1 æœ€å„ªé…ç½®ï¼šä½¿ç”¨ P-cores (æ€§èƒ½æ ¸å¿ƒ) æ•¸é‡
            physical_cores = psutil.cpu_count(logical=False)
            # é ç•™ 1 æ ¸çµ¦ç³»çµ±ï¼Œ75% ç”¨æ–¼è¦–é »è™•ç†
            max_workers = max(2, int((physical_cores - 1) * 0.75))
        
        # é™åˆ¶ç·šç¨‹æ•¸é‡æœ€å¤š 3 å€‹ï¼Œé¿å…è¨˜æ†¶é«”éåº¦æ¶ˆè€—
        max_workers = max(1, min(max_workers, 3))
        
        # ç²å–æ‰€æœ‰å½±ç‰‡æ–‡ä»¶
        video_files = []
        for root, _, files in os.walk(input_dir):
            for file in files:
                if file.lower().endswith(('.mp4', '.avi', '.mov', '.wmv')):
                    rel_dir = os.path.relpath(root, input_dir)
                    src_path = os.path.join(root, file)
                    
                    # ä¿æŒç›¸åŒçš„ç›®éŒ„çµæ§‹
                    if rel_dir == '.':
                        dest_dir = output_dir
                    else:
                        dest_dir = os.path.join(output_dir, rel_dir)
                    
                    os.makedirs(dest_dir, exist_ok=True)
                    
                    # çµ±ä¸€è¼¸å‡ºæª”åç‚º.mp4
                    file_base = os.path.splitext(file)[0]
                    dest_file = file_base + self.OUTPUT_EXT
                    dest_path = os.path.join(dest_dir, dest_file)
                    
                    video_files.append((src_path, dest_path))
        
        # å®šç¾©å·¥ä½œå‡½æ•¸ï¼šæ¯å€‹å½±ç‰‡ä¸€å€‹ç¨ç«‹çš„ VideoProcessor
        def process_video_worker(args):
            src_path, dest_path = args
            try:
                # æ¯å€‹ç·šç¨‹å‰µå»ºè‡ªå·±çš„ VideoProcessor å¯¦ä¾‹ï¼ˆç²å–ç·šç¨‹å°ˆç”¨çš„ MediaPipe å¯¦ä¾‹ï¼‰
                processor = VideoProcessor(
                    enable_cropping=self.enable_cropping,
                    enable_background_removal=self.enable_background_removal,
                    target_frames=self.target_frames
                )

                # å¦‚æœæä¾›äº† augmentorï¼Œæ¯å€‹ç·šç¨‹ä½¿ç”¨è‡ªå·±çš„å¿«å–å¯¦ä¾‹ï¼ˆé¿å…ç·šç¨‹é–“å¹²æ“¾ï¼‰
                if augmentor is not None:
                    thread_cache = AugmentationCache()
                    augmentor_with_cache = DataAugmentor(cache=thread_cache)
                    try:
                        return processor.process_video(src_path, dest_path, augmentor=augmentor_with_cache)
                    finally:
                        thread_cache.clear()  # è™•ç†å®Œç•¢å¾Œæ¸…ç†å¿«å–
                else:
                    return processor.process_video(src_path, dest_path, augmentor=None)
            finally:
                # æ¸…ç†ç•¶å‰ç·šç¨‹çš„ MediaPipe å¯¦ä¾‹ï¼ˆé¿å…æ™‚é–“æˆ³è¨˜è¡çªï¼‰
                if self.enable_cropping:
                    MediaPipePosePool.close_thread_detector()
                if self.enable_background_removal:
                    SelfieSegmentationPool.close_thread_segmenter()
        
        # ä½¿ç”¨å¤šç·šç¨‹è™•ç†å½±ç‰‡
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            results = list(tqdm(
                executor.map(process_video_worker, video_files),
                total=len(video_files), 
                desc="ğŸ¬ è™•ç†å½±ç‰‡"
            ))
        
        success_count = sum(results)
        if success_count < len(video_files):
            print(f"âš ï¸  å¤±æ•— {len(video_files) - success_count} å€‹å½±ç‰‡")

    async def process_directory_async(self, input_dir, output_dir, max_workers=None, augmentor=None):
        """
        ç•°æ­¥ç‰ˆæœ¬çš„ç›®éŒ„è™•ç† - ä½¿ç”¨ asyncio + ThreadPoolExecutor æå‡æ•ˆç‡

        å„ªå‹¢ï¼š
        1. éé˜»å¡ I/O æ“ä½œ
        2. æ›´å¥½çš„è³‡æºç®¡ç†
        3. æ›´é«˜çš„ä¸¦ç™¼æ•ˆç‡

        Args:
            input_dir: è¼¸å…¥ç›®éŒ„
            output_dir: è¼¸å‡ºç›®éŒ„
            max_workers: å·¥ä½œç·šç¨‹æ•¸ï¼ˆNone=è‡ªå‹•æª¢æ¸¬ï¼‰
            augmentor: DataAugmentor å¯¦ä¾‹ï¼ˆå¯é¸ï¼Œè¨“ç·´æ™‚ä½¿ç”¨ï¼‰
        """
        if not os.path.exists(input_dir):
            print(f"âŒ è¼¸å…¥ç›®éŒ„ä¸å­˜åœ¨: {input_dir}")
            return

        # ç¢ºä¿è¼¸å‡ºç›®éŒ„å­˜åœ¨
        os.makedirs(output_dir, exist_ok=True)

        # CPU ç¡¬é«”å„ªåŒ–ï¼šè‡ªå‹•æª¢æ¸¬ CPU æ ¸å¿ƒæ•¸
        cpu_count = psutil.cpu_count(logical=True)
        if max_workers is None:
            # M1 æœ€å„ªé…ç½®ï¼šä½¿ç”¨ P-cores (æ€§èƒ½æ ¸å¿ƒ) æ•¸é‡
            physical_cores = psutil.cpu_count(logical=False)
            # ç•°æ­¥ç‰ˆæœ¬å¯ä»¥ä½¿ç”¨æ›´å¤šç·šç¨‹ï¼Œå› ç‚ºè³‡æºç®¡ç†æ›´é«˜æ•ˆ
            max_workers = max(2, int((physical_cores - 1) * 0.9))

        # é™åˆ¶ç·šç¨‹æ•¸é‡é¿å…è¨˜æ†¶é«”éåº¦æ¶ˆè€—
        max_workers = max(1, min(max_workers, 4))

        # ç²å–æ‰€æœ‰å½±ç‰‡æ–‡ä»¶
        video_files = []
        for root, _, files in os.walk(input_dir):
            for file in files:
                if file.lower().endswith(('.mp4', '.avi', '.mov', '.wmv')):
                    rel_dir = os.path.relpath(root, input_dir)
                    src_path = os.path.join(root, file)

                    # ä¿æŒç›¸åŒçš„ç›®éŒ„çµæ§‹
                    if rel_dir == '.':
                        dest_dir = output_dir
                    else:
                        dest_dir = os.path.join(output_dir, rel_dir)

                    os.makedirs(dest_dir, exist_ok=True)

                    # çµ±ä¸€è¼¸å‡ºæª”åç‚º.mp4
                    file_base = os.path.splitext(file)[0]
                    dest_file = file_base + self.OUTPUT_EXT
                    dest_path = os.path.join(dest_dir, dest_file)

                    video_files.append((src_path, dest_path))

        # ç•°æ­¥è™•ç†å‡½æ•¸
        async def process_video_async(src_path, dest_path):
            """ç•°æ­¥è™•ç†å–®ä¸€å½±ç‰‡ï¼ˆç·šç¨‹å®‰å…¨ç‰ˆæœ¬ï¼‰"""
            loop = asyncio.get_event_loop()
            
            def sync_process():
                """åŒæ­¥è™•ç†å‡½æ•¸ï¼ˆåœ¨ç·šç¨‹æ± ä¸­åŸ·è¡Œï¼‰"""
                try:
                    # æ¯å€‹ç·šç¨‹å‰µå»ºè‡ªå·±çš„ VideoProcessor å¯¦ä¾‹
                    processor = VideoProcessor(
                        enable_cropping=self.enable_cropping,
                        enable_background_removal=self.enable_background_removal,
                        target_frames=self.target_frames
                    )

                    # å¦‚æœæä¾›äº† augmentorï¼Œæ¯å€‹ä»»å‹™ä½¿ç”¨è‡ªå·±çš„å¿«å–å¯¦ä¾‹
                    if augmentor is not None:
                        thread_cache = AugmentationCache()
                        augmentor_with_cache = DataAugmentor(cache=thread_cache)
                        try:
                            return processor.process_video(src_path, dest_path, augmentor_with_cache)
                        finally:
                            thread_cache.clear()
                    else:
                        return processor.process_video(src_path, dest_path, None)
                finally:
                    # æ¸…ç†ç•¶å‰ç·šç¨‹çš„ MediaPipe å¯¦ä¾‹
                    if self.enable_cropping:
                        MediaPipePosePool.close_thread_detector()
                    if self.enable_background_removal:
                        SelfieSegmentationPool.close_thread_segmenter()
            
            # åœ¨ç·šç¨‹æ± ä¸­åŸ·è¡ŒåŒæ­¥è™•ç†
            result = await loop.run_in_executor(None, sync_process)
            return result

        # å‰µå»ºç•°æ­¥ä»»å‹™
        tasks = []
        semaphore = asyncio.Semaphore(max_workers)  # é™åˆ¶ä¸¦ç™¼æ•¸é‡

        async def process_with_semaphore(src_path, dest_path):
            async with semaphore:
                return await process_video_async(src_path, dest_path)

        for src_path, dest_path in video_files:
            task = asyncio.create_task(process_with_semaphore(src_path, dest_path))
            tasks.append(task)

        # ä½¿ç”¨ç•°æ­¥é€²åº¦æ¢é¡¯ç¤ºè™•ç†é€²åº¦

        # åŸ·è¡Œæ‰€æœ‰ä»»å‹™ä¸¦é¡¯ç¤ºé€²åº¦
        results = []
        for coro in tqdm(asyncio.as_completed(tasks), total=len(tasks), desc="ğŸ¬ ç•°æ­¥è™•ç†å½±ç‰‡"):
            result = await coro
            results.append(result)

        success_count = sum(results)
        if success_count < len(video_files):
            print(f"âš ï¸  å¤±æ•— {len(video_files) - success_count} å€‹å½±ç‰‡")

        print(f"âœ… ç•°æ­¥è™•ç†å®Œæˆï¼æˆåŠŸ {success_count}/{len(video_files)} å€‹å½±ç‰‡")


def process_for_inference(input_video, output_video, enable_cropping=True, target_frames=None):
    """
    è¾¨è­˜å‰è™•ç† - åƒ…æ¨™æº–åŒ–ï¼ˆä¸å«æ•¸æ“šå¢å¼·ï¼‰
    
    ç”¨æ–¼è¾¨è­˜æ™‚çš„å½±ç‰‡å‰è™•ç†ï¼š
    1. æ™ºèƒ½è£åˆ‡äººé«”ï¼ˆå¯é¸ï¼‰
    2. å¹€æ•¸æ¨™æº–åŒ–ï¼ˆå¯é¸ï¼‰
    3. è§£æåº¦æ¨™æº–åŒ–ï¼ˆ224x224ï¼‰
    
    ä¸åŒ…å«æ•¸æ“šå¢å¼·ï¼åƒ…è¼¸å‡ºå–®å€‹æ¨™æº–åŒ–ç‰ˆæœ¬
    
    Args:
        input_video: è¼¸å…¥å½±ç‰‡è·¯å¾‘
        output_video: è¼¸å‡ºå½±ç‰‡è·¯å¾‘
        enable_cropping: æ˜¯å¦å•Ÿç”¨æ™ºèƒ½è£åˆ‡ï¼ˆé è¨­é–‹å•Ÿï¼‰
        target_frames: ç›®æ¨™å¹€æ•¸ï¼ˆNone=ä¸å¼·åˆ¶æ¨™æº–åŒ–ï¼‰
    
    Returns:
        è™•ç†æ˜¯å¦æˆåŠŸ
    
    ä½¿ç”¨ç¯„ä¾‹ï¼š
        >>> from processor import process_for_inference
        >>> success = process_for_inference('input.mp4', 'output.mp4')
    """
    print("=" * 70)
    print("æ‰‹èªå½±ç‰‡å‰è™•ç†ï¼šæ™ºèƒ½è£åˆ‡ + æ¨™æº–åŒ–ï¼ˆè¾¨è­˜ç”¨ï¼‰")
    print("=" * 70)
    
    # åˆå§‹åŒ–è™•ç†å™¨ï¼ˆä¸ä½¿ç”¨æ•¸æ“šå¢å¼·ï¼‰
    processor = VideoProcessor(enable_cropping=enable_cropping, target_frames=target_frames)
    
    # è™•ç†å½±ç‰‡ï¼ˆaugmentor=Noneï¼Œåªè¼¸å‡ºæ¨™æº–åŒ–ç‰ˆæœ¬ï¼‰
    success = processor.process_video(input_video, output_video, augmentor=None, target_frames=target_frames)
    
    if success:
        print(f"âœ… è™•ç†å®Œæˆ: {output_video}")
    else:
        print(f"âŒ è™•ç†å¤±æ•—: {input_video}")
    
    return success


async def main():
    """
    ä¸»ç¨‹å¼ - ç•°æ­¥è¨“ç·´å‰è™•ç†ï¼ˆåŒ…å«èƒŒæ™¯ç§»é™¤ + æ•¸æ“šå¢å¼·ï¼‰
    
    åŠŸèƒ½ï¼š
    1. æ™ºèƒ½äººé«”è£åˆ‡
    2. èƒŒæ™¯ç§»é™¤
    3. æ•¸æ“šå¢å¼·ï¼ˆ8å€‹ç‰ˆæœ¬ï¼‰
    4. ç•°æ­¥è™•ç†æå‡æ•ˆç‡
    
    å¦‚æœåªéœ€è¦æ¨™æº–åŒ–ï¼ˆè¾¨è­˜æ™‚ä½¿ç”¨ï¼‰ï¼Œè«‹åƒè€ƒ process_for_inference()
    """
    print("=" * 70)
    print("æ‰‹èªå½±ç‰‡å‰è™•ç†ï¼šç•°æ­¥è™•ç† + æ™ºèƒ½è£åˆ‡ + èƒŒæ™¯ç§»é™¤ + æ•¸æ“šå¢å¼·")
    print("=" * 70)

    # æª¢æ¸¬æ˜¯å¦åœ¨ Kaggle ç’°å¢ƒä¸­
    is_kaggle = os.path.exists('/kaggle')

    if is_kaggle:
        # Kaggle ç’°å¢ƒé…ç½®
        input_dir = "/kaggle/input/augment/augmented_videos"
        output_dir = "/kaggle/working/videos_processed"
    else:
        # æœ¬åœ°ç’°å¢ƒé…ç½®
        current_dir = os.path.dirname(os.path.abspath(__file__))
        # ä½¿ç”¨ bai_dataset ä½œç‚ºè¼¸å…¥
        input_dir = os.path.join(os.path.dirname(current_dir), "Final_MVP/up")
        output_dir = os.path.join(os.path.dirname(current_dir), "Final_MVP/videos_processed")

    # åˆå§‹åŒ–è™•ç†å™¨ï¼ˆå•Ÿç”¨æ™ºèƒ½è£åˆ‡ + èƒŒæ™¯ç§»é™¤ï¼‰
    processor = VideoProcessor(
        enable_cropping=True,
        enable_background_removal=True,
        target_frames= None  # è¨­å®šç›®æ¨™å¹€æ•¸ç‚ºNoneï¼Œä¸é€²è¡Œå¹€æ•¸æ¨™æº–åŒ–
    )

    # åˆå§‹åŒ–æ•¸æ“šå¢å¼·å™¨ï¼ˆè¨“ç·´æ™‚ä½¿ç”¨ï¼‰
    augmentor = DataAugmentor()

    # ä½¿ç”¨ç•°æ­¥è™•ç†
    await processor.process_directory_async(input_dir, output_dir, augmentor=augmentor)

    # æ‰“åŒ…è¼¸å‡ºç›®éŒ„ç‚ºzipæª”æ¡ˆ
    if is_kaggle:
        zip_path = "/kaggle/working/videos_processed.zip"
    else:
        zip_path = os.path.join(os.path.dirname(current_dir), "videos_processed.zip")

    try:
        with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:
            for root, dirs, files in os.walk(output_dir):
                for file in files:
                    file_path = os.path.join(root, file)
                    if is_kaggle:
                        arcname = os.path.relpath(file_path, "/kaggle/working")
                    else:
                        arcname = os.path.relpath(file_path, os.path.dirname(current_dir))
                    zipf.write(file_path, arcname)

        print(f"âœ… æ‰“åŒ…å®Œæˆ: {zip_path}")
    except Exception as e:
        print(f"âŒ æ‰“åŒ…å¤±æ•—: {str(e)}")

    print("\n" + "=" * 70)
    print("ç•°æ­¥å‰è™•ç†å®Œæˆï¼")
    print("=" * 70)

    # æ¸…ç†å…¨åŸŸè³‡æº
    MediaPipePosePool.close_all()
    SelfieSegmentationPool.close_all()


if __name__ == "__main__":
    # ç›´æ¥ä½¿ç”¨ç•°æ­¥è™•ç†
    asyncio.run(main())
