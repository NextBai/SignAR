"""
æ‰‹èªå½±ç‰‡å‰è™•ç†è…³æœ¬

æ¨¡çµ„æ¶æ§‹ï¼š
1. VideoProcessorï¼ˆæ ¸å¿ƒè™•ç†å™¨ï¼‰- è¾¨è­˜æ™‚ä¹Ÿæœƒç”¨åˆ°
   - æ™ºèƒ½äººé«”æª¢æ¸¬å’Œè£åˆ‡ï¼ˆMediaPipe Poseï¼‰
   - å½±ç‰‡æ¨™æº–åŒ–ï¼ˆå¹€æ•¸ã€FPSã€è§£æåº¦ï¼‰
   
2. DataAugmentorï¼ˆæ•¸æ“šå¢å¼·å™¨ï¼‰- åƒ…è¨“ç·´å‰è™•ç†ä½¿ç”¨
   - æ¥µè¼•å¾®æ—‹è½‰ã€ç¸®æ”¾ã€äº®åº¦èª¿æ•´
   - å‰µå»ºå¤šå€‹å¢å¼·ç‰ˆæœ¬
"""
import os
import cv2
import numpy as np
from tqdm import tqdm
from concurrent.futures import ThreadPoolExecutor
import zipfile
import psutil
import mediapipe as mp


# ==================== æ•¸æ“šå¢å¼·æ¨¡çµ„ï¼ˆç¨ç«‹ï¼‰ ====================
class DataAugmentor:
    """
    æ•¸æ“šå¢å¼·å™¨ - åƒ…ç”¨æ–¼è¨“ç·´å‰è™•ç†
    
    åŠŸèƒ½ï¼š
    - æ¥µè¼•å¾®æ—‹è½‰ (Â±2Â°)
    - æ¥µè¼•å¾®ç¸®æ”¾ (0.98-1.02x)
    - æ¥µè¼•å¾®äº®åº¦èª¿æ•´ (Â±5%)
    - æ°´å¹³ç¿»è½‰
    
    æ³¨æ„ï¼šè¾¨è­˜æ™‚ä¸éœ€è¦ä½¿ç”¨æ­¤æ¨¡çµ„ï¼
    """
    
    def __init__(self):
        # æ•¸æ“šå¢å¼·åƒæ•¸ - æ¥µç‚ºä¿å®ˆ
        self.augmentation_params = {
            'rotation_range': (-2, 2),      # æ¥µè¼•å¾®æ—‹è½‰è§’åº¦ç¯„åœï¼ˆåº¦ï¼‰
            'scale_range': (0.98, 1.02),    # æ¥µè¼•å¾®ç¸®æ”¾ç¯„åœ
            'brightness_range': (0.95, 1.05), # æ¥µè¼•å¾®äº®åº¦èª¿æ•´ç¯„åœ
        }
    
    def apply_augmentation(self, frame, augmentation_type=None):
        """
        ç¢ºå®šæ€§æ•¸æ“šå¢å¼·
        
        Args:
            frame: è¼¸å…¥å¹€ [H, W, 3]
            augmentation_type: å¢å¼·é¡å‹ ('rotation', 'scale', 'brightness', 'flip', None)
        
        Returns:
            å¢å¼·å¾Œçš„å¹€ [H, W, 3]
        """
        augmented = frame.astype(np.float32)

        if augmentation_type == 'rotation':
            # ç¢ºå®šæ€§æ—‹è½‰ (Â±2Â°)
            angle = np.random.RandomState(42).choice([-2, -1, 1, 2])
            h, w = augmented.shape[:2]
            center = (w // 2, h // 2)
            rotation_matrix = cv2.getRotationMatrix2D(center, angle, 1.0)
            augmented = cv2.warpAffine(augmented, rotation_matrix, (w, h),
                                     borderMode=cv2.BORDER_REFLECT)

        elif augmentation_type == 'scale':
            # ç¢ºå®šæ€§ç¸®æ”¾ (0.98 æˆ– 1.02)
            scale = np.random.RandomState(42).choice([0.98, 1.02])
            h, w = augmented.shape[:2]
            new_w, new_h = int(w * scale), int(h * scale)

            scaled = cv2.resize(augmented, (new_w, new_h))

            # å±…ä¸­æ”¾ç½®ç¸®æ”¾å¾Œçš„åœ–åƒ
            result = np.zeros_like(augmented)
            start_x = (w - new_w) // 2
            start_y = (h - new_h) // 2

            if scale > 1.0:  # æ”¾å¤§ï¼šè£å‰ªä¸­å¿ƒå€åŸŸ
                src_start_x = (new_w - w) // 2
                src_start_y = (new_h - h) // 2
                result[:, :] = scaled[src_start_y:src_start_y+h, src_start_x:src_start_x+w]
            else:  # ç¸®å°ï¼šå±…ä¸­æ”¾ç½®
                end_x = start_x + new_w
                end_y = start_y + new_h
                result[start_y:end_y, start_x:end_x] = scaled

            augmented = result

        elif augmentation_type == 'brightness':
            # ç¢ºå®šæ€§äº®åº¦èª¿æ•´ (Â±5%)
            brightness_factor = np.random.RandomState(42).choice([0.95, 1.05])
            augmented = np.clip(augmented * brightness_factor, 0, 255)

        elif augmentation_type == 'flip':
            # æ°´å¹³åè½‰
            augmented = cv2.flip(augmented, 1)

        return augmented.astype(np.uint8)
    
    def create_augmented_versions(self, frames, output_base_path, save_func):
        """
        ç‚ºçµ¦å®šçš„å¹€åºåˆ—å‰µå»º8å€‹å¢å¼·ç‰ˆæœ¬ä¸¦ä¿å­˜
        
        Args:
            frames: æ¨™æº–åŒ–å¾Œçš„å¹€åˆ—è¡¨
            output_base_path: è¼¸å‡ºåŸºç¤è·¯å¾‘ï¼ˆä¸å«å‰¯æª”åï¼‰
            save_func: ä¿å­˜å½±ç‰‡çš„å‡½æ•¸ï¼Œç°½åç‚º save_func(frames, output_path)
        
        Returns:
            æˆåŠŸä¿å­˜çš„ç‰ˆæœ¬æ•¸é‡
        """
        base_augmentation_types = [None, 'rotation', 'scale', 'brightness']
        base_suffixes = ['', '_rotation', '_scale', '_brightness']
        
        success_count = 0
        output_ext = '.mp4'
        
        for base_aug, base_suffix in zip(base_augmentation_types, base_suffixes):
            # å‰µå»ºåŸºæœ¬å¢å¼·ç‰ˆæœ¬
            if base_aug is None:
                base_frames = frames
            else:
                base_frames = [self.apply_augmentation(frame, base_aug) for frame in frames]
            
            # å‰µå»ºä¸åè½‰ç‰ˆæœ¬
            output_file = output_base_path + base_suffix + output_ext
            if save_func(base_frames, output_file):
                success_count += 1
            
            # å‰µå»ºåè½‰ç‰ˆæœ¬
            flip_frames = [self.apply_augmentation(frame, 'flip') for frame in base_frames]
            flip_suffix = base_suffix + '_flip' if base_suffix else '_flip'
            output_file_flip = output_base_path + flip_suffix + output_ext
            if save_func(flip_frames, output_file_flip):
                success_count += 1
        
        return success_count


# ==================== æ ¸å¿ƒå½±ç‰‡è™•ç†å™¨ ====================

class VideoProcessor:
    """
    æ ¸å¿ƒå½±ç‰‡è™•ç†å™¨ - è¾¨è­˜æ™‚ä¹Ÿæœƒç”¨åˆ°
    
    åŠŸèƒ½ï¼š
    1. æ™ºèƒ½äººé«”æª¢æ¸¬å’Œè£åˆ‡ï¼ˆMediaPipe Poseï¼‰
    2. å½±ç‰‡æ¨™æº–åŒ–ï¼ˆå¹€æ•¸ã€FPSã€è§£æåº¦ï¼‰
    3. å½±ç‰‡ç·¨ç¢¼å„ªåŒ–ï¼ˆh.264ï¼‰
    
    æ³¨æ„ï¼šä¸åŒ…å«æ•¸æ“šå¢å¼·åŠŸèƒ½ï¼æ•¸æ“šå¢å¼·è«‹ä½¿ç”¨ DataAugmentor
    """
    
    # æ¨™æº–åŒ–åƒæ•¸
    TARGET_FRAMES = 80      # ç›®æ¨™å¹€æ•¸
    TARGET_FPS = 30         # ç›®æ¨™å¹€ç‡
    TARGET_WIDTH = 224      # ç›®æ¨™å¯¬åº¦
    TARGET_HEIGHT = 224     # ç›®æ¨™é«˜åº¦
    TARGET_BITRATE = 2000   # ç›®æ¨™æ¯”ç‰¹ç‡ (kbps)
    OUTPUT_EXT = '.mp4'     # çµ±ä¸€è¼¸å‡ºæ ¼å¼
    
    # äººé«”è£åˆ‡åƒæ•¸
    CROP_PADDING = 0.15     # é‚Šç•Œæ¡†æ“´å±•æ¯”ä¾‹ï¼ˆ15%ï¼‰
    MIN_DETECTION_CONFIDENCE = 0.5  # æœ€ä½æª¢æ¸¬ä¿¡å¿ƒåº¦
    
    def __init__(self, enable_cropping=True):
        """
        åˆå§‹åŒ–å½±ç‰‡è™•ç†å™¨
        
        Args:
            enable_cropping: æ˜¯å¦å•Ÿç”¨æ™ºèƒ½äººé«”è£åˆ‡ï¼ˆé è¨­é–‹å•Ÿï¼‰
        """
        # æª¢æŸ¥æ˜¯å¦æœ‰ffmpegï¼ˆç”¨æ–¼å¾Œè™•ç†ç¢ºä¿h264ç·¨ç¢¼ï¼‰
        self.has_ffmpeg = self._check_ffmpeg()
        
        # æ˜¯å¦å•Ÿç”¨æ™ºèƒ½è£åˆ‡
        self.enable_cropping = enable_cropping
        
        # åˆå§‹åŒ– MediaPipe Poseï¼ˆå»¶é²åˆå§‹åŒ–ï¼‰
        self.mp_pose = None
        self.pose_detector = None
        
        if self.enable_cropping:
            print("ğŸ”§ åˆå§‹åŒ– MediaPipe Pose æª¢æ¸¬å™¨...")
            self.mp_pose = mp.solutions.pose
            self.pose_detector = self.mp_pose.Pose(
                static_image_mode=False,
                model_complexity=1,  # 0=Lite, 1=Full, 2=Heavy
                min_detection_confidence=self.MIN_DETECTION_CONFIDENCE,
                min_tracking_confidence=0.5
            )
            print("âœ… MediaPipe Pose å·²åˆå§‹åŒ–")
        
    def _check_ffmpeg(self):
        """æª¢æŸ¥ç³»çµ±æ˜¯å¦æœ‰ffmpeg"""
        import subprocess
        try:
            subprocess.run(['ffmpeg', '-version'], 
                         stdout=subprocess.DEVNULL, 
                         stderr=subprocess.DEVNULL, 
                         check=True)
            return True
        except (subprocess.CalledProcessError, FileNotFoundError):
            return False
    
    def detect_and_crop_person(self, frame):
        """
        æª¢æ¸¬ä¸¦è£åˆ‡ç•«é¢ä¸­ã€Œæœ€å‰é¢ã€çš„äººçš„å®Œæ•´ä¸ŠåŠèº«
        
        ä½¿ç”¨ MediaPipe Pose æª¢æ¸¬äººé«”é—œéµé»ï¼Œè¨ˆç®—åŒ…å«ï¼š
        - é ­éƒ¨ï¼ˆé¼»å­ã€çœ¼ç›ã€è€³æœµï¼‰
        - ä¸ŠåŠèº«ï¼ˆè‚©è†€ã€æ‰‹è‚˜ã€æ‰‹è…•ï¼‰
        - å®Œæ•´æ‰‹è‡‚ï¼ˆç¢ºä¿ä¸è£åˆ‡ï¼‰
        
        Args:
            frame: è¼¸å…¥å¹€ [H, W, 3] BGR
        
        Returns:
            cropped_frame: è£åˆ‡å¾Œçš„å¹€ï¼ˆå¦‚æœæª¢æ¸¬å¤±æ•—è¿”å›åŸå§‹å¹€ï¼‰
            success: æ˜¯å¦æˆåŠŸæª¢æ¸¬ä¸¦è£åˆ‡
        """
        if not self.enable_cropping or self.pose_detector is None:
            return frame, False
        
        h, w = frame.shape[:2]
        
        # è½‰æ›ç‚º RGBï¼ˆMediaPipe éœ€è¦ RGBï¼‰
        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        
        # åŸ·è¡Œå§¿æ…‹æª¢æ¸¬
        results = self.pose_detector.process(frame_rgb)
        
        # å¦‚æœæ²’æœ‰æª¢æ¸¬åˆ°äººé«”ï¼Œè¿”å›åŸå§‹å¹€
        if not results.pose_landmarks:
            return frame, False
        
        # æå–é—œéµé»
        landmarks = results.pose_landmarks.landmark
        
        # å®šç¾©éœ€è¦åŒ…å«çš„é—œéµé»ï¼ˆä¸ŠåŠèº« + æ‰‹è‡‚ï¼‰
        # MediaPipe Pose é—œéµé»ç´¢å¼•ï¼š
        # 0: é¼»å­, 1-2: çœ¼ç›, 3-4: è€³æœµ
        # 11-12: è‚©è†€, 13-14: æ‰‹è‚˜, 15-16: æ‰‹è…•, 17-22: æ‰‹æŒ‡ï¼ˆå¯é¸ï¼‰
        upper_body_indices = [
            0,   # é¼»å­ï¼ˆé ­éƒ¨é ‚éƒ¨åƒè€ƒï¼‰
            1, 2, 3, 4,  # çœ¼ç›ã€è€³æœµ
            11, 12,  # å·¦å³è‚©è†€
            13, 14,  # å·¦å³æ‰‹è‚˜
            15, 16,  # å·¦å³æ‰‹è…•
            17, 18, 19, 20, 21, 22  # æ‰‹æŒ‡ï¼ˆç¢ºä¿å®Œæ•´æ‰‹éƒ¨ï¼‰
        ]
        
        # è¨ˆç®—é‚Šç•Œæ¡†
        x_coords = []
        y_coords = []
        
        for idx in upper_body_indices:
            if idx < len(landmarks):
                landmark = landmarks[idx]
                # æª¢æŸ¥é—œéµé»å¯è¦‹æ€§ï¼ˆvisibility > 0.5 è¡¨ç¤ºå¯è¦‹ï¼‰
                if landmark.visibility > 0.5:
                    x_coords.append(landmark.x * w)
                    y_coords.append(landmark.y * h)
        
        # å¦‚æœæª¢æ¸¬åˆ°çš„é—œéµé»å¤ªå°‘ï¼Œè¿”å›åŸå§‹å¹€
        if len(x_coords) < 4:
            return frame, False
        
        # è¨ˆç®—é‚Šç•Œæ¡†
        x_min = int(min(x_coords))
        x_max = int(max(x_coords))
        y_min = int(min(y_coords))
        y_max = int(max(y_coords))
        
        # æ·»åŠ  paddingï¼ˆç¢ºä¿ä¸è£åˆ‡åˆ°æ‰‹è‡‚ï¼‰
        bbox_width = x_max - x_min
        bbox_height = y_max - y_min
        
        padding_x = int(bbox_width * self.CROP_PADDING)
        padding_y = int(bbox_height * self.CROP_PADDING)
        
        x_min = max(0, x_min - padding_x)
        x_max = min(w, x_max + padding_x)
        y_min = max(0, y_min - padding_y)
        y_max = min(h, y_max + padding_y)
        
        # ç¢ºä¿è£åˆ‡å€åŸŸæ˜¯æ­£æ–¹å½¢ï¼ˆé¿å…è®Šå½¢ï¼‰
        crop_width = x_max - x_min
        crop_height = y_max - y_min
        
        if crop_width > crop_height:
            # å¯¬åº¦å¤§æ–¼é«˜åº¦ï¼Œæ“´å±•é«˜åº¦
            diff = crop_width - crop_height
            y_min = max(0, y_min - diff // 2)
            y_max = min(h, y_max + diff // 2)
        else:
            # é«˜åº¦å¤§æ–¼å¯¬åº¦ï¼Œæ“´å±•å¯¬åº¦
            diff = crop_height - crop_width
            x_min = max(0, x_min - diff // 2)
            x_max = min(w, x_max + diff // 2)
        
        # è£åˆ‡
        cropped = frame[y_min:y_max, x_min:x_max]
        
        # æª¢æŸ¥è£åˆ‡çµæœæ˜¯å¦æœ‰æ•ˆ
        if cropped.size == 0:
            return frame, False
        
        return cropped, True
    
    def __del__(self):
        """æ¸…ç†è³‡æº"""
        if self.pose_detector is not None:
            self.pose_detector.close()
    
    def normalize_frames(self, frames, target_count):
        """
        æ™ºèƒ½å¹€æ•¸æ¨™æº–åŒ–
        - å¦‚æœåŸå§‹å¹€æ•¸ > ç›®æ¨™å¹€æ•¸ï¼šå‡å‹»æ¡æ¨£
        - å¦‚æœåŸå§‹å¹€æ•¸ < ç›®æ¨™å¹€æ•¸ï¼šç·šæ€§æ’å€¼
        
        Args:
            frames: åŸå§‹å¹€åˆ—è¡¨
            target_count: ç›®æ¨™å¹€æ•¸
            
        Returns:
            æ¨™æº–åŒ–å¾Œçš„å¹€åˆ—è¡¨
        """
        original_count = len(frames)
        
        if original_count == target_count:
            return frames
        
        # ç”Ÿæˆæ¡æ¨£ç´¢å¼•
        indices = np.linspace(0, original_count - 1, target_count)
        
        normalized_frames = []
        for idx in indices:
            # å¦‚æœæ˜¯æ•´æ•¸ç´¢å¼•ï¼Œç›´æ¥å–å¹€
            if idx == int(idx):
                normalized_frames.append(frames[int(idx)])
            else:
                # ç·šæ€§æ’å€¼
                lower_idx = int(np.floor(idx))
                upper_idx = int(np.ceil(idx))
                weight = idx - lower_idx
                
                # æ··åˆå…©å¹€
                lower_frame = frames[lower_idx].astype(np.float32)
                upper_frame = frames[upper_idx].astype(np.float32)
                interpolated = (1 - weight) * lower_frame + weight * upper_frame
                normalized_frames.append(interpolated.astype(np.uint8))
        
        return normalized_frames
    
    def process_and_crop_frames(self, frames):
        """
        è™•ç†å¹€åºåˆ—ï¼šæ™ºèƒ½è£åˆ‡ + è§£æåº¦æ¨™æº–åŒ–
        
        æ³¨æ„ï¼šæ­¤å‡½æ•¸ä¸åŒ…å«æ•¸æ“šå¢å¼·ï¼åƒ…ç”¨æ–¼æ ¸å¿ƒè™•ç†
        
        Args:
            frames: è¼¸å…¥å¹€åˆ—è¡¨
        
        Returns:
            è™•ç†å¾Œçš„å¹€åˆ—è¡¨ï¼ˆæ¨™æº–åŒ–åˆ° 224x224ï¼‰
        """
        processed_frames = []
        
        for frame in frames:
            # 1. æ™ºèƒ½è£åˆ‡äººé«”ï¼ˆå¦‚æœå•Ÿç”¨ï¼‰
            if self.enable_cropping:
                frame_cropped, success = self.detect_and_crop_person(frame)
                if success:
                    frame = frame_cropped
            
            # 2. è§£æåº¦æ¨™æº–åŒ–
            frame_resized = cv2.resize(frame, (self.TARGET_WIDTH, self.TARGET_HEIGHT))
            processed_frames.append(frame_resized)
        
        return processed_frames

    def _convert_to_h264(self, input_video, output_video):
        """ä½¿ç”¨ffmpegè½‰æ›ç‚ºh264ç·¨ç¢¼ï¼Œä¸¦è¨­å®šæ¯”ç‰¹ç‡"""
        import subprocess
        
        cmd = [
            'ffmpeg',
            '-i', input_video,
            '-c:v', 'libx264',              # h264ç·¨ç¢¼
            '-b:v', f'{self.TARGET_BITRATE}k',  # æ¯”ç‰¹ç‡
            '-preset', 'medium',            # ç·¨ç¢¼é€Ÿåº¦
            '-movflags', '+faststart',      # å„ªåŒ–ä¸²æµ
            '-y',                           # è¦†è“‹è¼¸å‡º
            output_video
        ]
        
        try:
            subprocess.run(cmd, 
                         stdout=subprocess.DEVNULL, 
                         stderr=subprocess.DEVNULL, 
                         check=True)
            return True
        except subprocess.CalledProcessError:
            return False

    def _save_video_frames(self, frames, output_path):
        """
        å°‡å¹€åºåˆ—ä¿å­˜ç‚ºå½±ç‰‡æ–‡ä»¶

        Args:
            frames: å¹€åˆ—è¡¨
            output_path: è¼¸å‡ºå½±ç‰‡è·¯å¾‘

        Returns:
            ä¿å­˜æ˜¯å¦æˆåŠŸ
        """
        try:
            temp_output = output_path + '.temp.mp4'
            fourcc = cv2.VideoWriter_fourcc(*'mp4v')

            out = cv2.VideoWriter(
                temp_output,
                fourcc,
                self.TARGET_FPS,
                (self.TARGET_WIDTH, self.TARGET_HEIGHT)
            )

            if not out.isOpened():
                print(f"âŒ ç„¡æ³•å‰µå»ºè¼¸å‡ºå½±ç‰‡: {output_path}")
                return False

            for frame in frames:
                out.write(frame)

            out.release()

            # ä½¿ç”¨ffmpegè½‰æ›ç‚ºæ¨™æº–h264ç·¨ç¢¼ï¼ˆå¦‚æœæœ‰ffmpegï¼‰
            if self.has_ffmpeg:
                if self._convert_to_h264(temp_output, output_path):
                    # è½‰æ›æˆåŠŸï¼Œåˆªé™¤è‡¨æ™‚æª”æ¡ˆ
                    os.remove(temp_output)
                else:
                    # è½‰æ›å¤±æ•—ï¼Œä½¿ç”¨è‡¨æ™‚æª”æ¡ˆä½œç‚ºæœ€çµ‚è¼¸å‡º
                    print(f"âš ï¸  ffmpegè½‰æ›å¤±æ•—ï¼Œä½¿ç”¨mp4vç·¨ç¢¼: {os.path.basename(output_path)}")
                    os.rename(temp_output, output_path)
            else:
                # æ²’æœ‰ffmpegï¼Œç›´æ¥ä½¿ç”¨mp4vç·¨ç¢¼
                os.rename(temp_output, output_path)

            # é©—è­‰è¼¸å‡º
            verify_cap = cv2.VideoCapture(output_path)
            verify_frames = int(verify_cap.get(cv2.CAP_PROP_FRAME_COUNT))
            verify_cap.release()

            # æª¢æŸ¥æ˜¯å¦ç¬¦åˆè¦æ ¼ï¼ˆå…è¨±1å¹€çš„èª¤å·®ï¼‰
            if abs(verify_frames - self.TARGET_FRAMES) > 1:
                print(f"âš ï¸  è­¦å‘Š: {os.path.basename(output_path)} å¹€æ•¸ä¸ç¬¦ (æœŸæœ›{self.TARGET_FRAMES}, å¯¦éš›{verify_frames})")

            return True

        except Exception as e:
            print(f"âŒ ä¿å­˜å½±ç‰‡æ™‚å‡ºéŒ¯: {output_path}, éŒ¯èª¤: {str(e)}")
            return False

    def process_video(self, input_path, output_path, augmentor=None):
        """
        è™•ç†å–®ä¸€å½±ç‰‡ï¼šæ¨™æº–åŒ– + å¯é¸çš„æ•¸æ“šå¢å¼·
        
        æ ¸å¿ƒè™•ç†æµç¨‹ï¼š
        1. å¹€æ•¸æ¨™æº–åŒ–ï¼ˆ80å¹€ï¼‰
        2. æ™ºèƒ½è£åˆ‡äººé«”ï¼ˆå¯é¸ï¼‰
        3. è§£æåº¦æ¨™æº–åŒ–ï¼ˆ224x224ï¼‰
        4. æ•¸æ“šå¢å¼·ï¼ˆå¯é¸ï¼Œéœ€è¦æä¾› augmentorï¼‰
        
        å¦‚æœæä¾› augmentorï¼š
        - å‰µå»º8å€‹å¢å¼·ç‰ˆæœ¬ï¼ˆåŸå§‹ã€æ—‹è½‰ã€ç¸®æ”¾ã€äº®åº¦ Ã— ç„¡åè½‰/åè½‰ï¼‰
        
        å¦‚æœä¸æä¾› augmentorï¼š
        - åªè¼¸å‡º1å€‹æ¨™æº–åŒ–ç‰ˆæœ¬
        
        Args:
            input_path: è¼¸å…¥å½±ç‰‡è·¯å¾‘
            output_path: è¼¸å‡ºå½±ç‰‡è·¯å¾‘
            augmentor: DataAugmentor å¯¦ä¾‹ï¼ˆå¯é¸ï¼Œè¨“ç·´æ™‚ä½¿ç”¨ï¼‰
        
        Returns:
            è™•ç†æ˜¯å¦æˆåŠŸ
        """
        try:
            # ç¢ºä¿è¼¸å‡ºå‰¯æª”åç‚º.mp4
            output_path_base = os.path.splitext(output_path)[0]

            # ç¢ºä¿è¼¸å‡ºç›®éŒ„å­˜åœ¨
            output_dir = os.path.dirname(output_path)
            if output_dir:  # åªæœ‰ç•¶æœ‰ç›®éŒ„éƒ¨åˆ†æ™‚æ‰å‰µå»º
                os.makedirs(output_dir, exist_ok=True)

            cap = cv2.VideoCapture(input_path)
            if not cap.isOpened():
                print(f"âŒ ç„¡æ³•é–‹å•Ÿå½±ç‰‡: {input_path}")
                return False

            # è®€å–æ‰€æœ‰å¹€
            original_frames = []
            while True:
                ret, frame = cap.read()
                if not ret:
                    break
                original_frames.append(frame)

            cap.release()

            if len(original_frames) == 0:
                print(f"âŒ å½±ç‰‡ç„¡æœ‰æ•ˆå¹€: {input_path}")
                return False

            # æ­¥é©Ÿ1: å¹€æ•¸æ¨™æº–åŒ–
            normalized_frames = self.normalize_frames(original_frames, self.TARGET_FRAMES)

            # æ­¥é©Ÿ2: æ™ºèƒ½è£åˆ‡äººé«”ï¼ˆåœ¨ resize ä¹‹å‰ï¼‰
            cropped_frames = []
            crop_success_count = 0
            
            for frame in normalized_frames:
                if self.enable_cropping:
                    frame_cropped, success = self.detect_and_crop_person(frame)
                    if success:
                        cropped_frames.append(frame_cropped)
                        crop_success_count += 1
                    else:
                        cropped_frames.append(frame)
                else:
                    cropped_frames.append(frame)
            
            # å¦‚æœå•Ÿç”¨äº†è£åˆ‡ï¼Œé¡¯ç¤ºè£åˆ‡æˆåŠŸç‡
            if self.enable_cropping:
                crop_rate = (crop_success_count / len(normalized_frames)) * 100
                if crop_rate < 50:
                    print(f"âš ï¸  è­¦å‘Š: {os.path.basename(input_path)} äººé«”æª¢æ¸¬ç‡è¼ƒä½ ({crop_rate:.1f}%)")

            # æ­¥é©Ÿ3: è§£æåº¦æ¨™æº–åŒ–
            resized_frames = []
            for frame in cropped_frames:
                frame_resized = cv2.resize(frame, (self.TARGET_WIDTH, self.TARGET_HEIGHT))
                resized_frames.append(frame_resized)

            # æ­¥é©Ÿ4: æ•¸æ“šå¢å¼·ï¼ˆå¯é¸ï¼‰
            if augmentor is not None:
                # ä½¿ç”¨ DataAugmentor å‰µå»º8å€‹å¢å¼·ç‰ˆæœ¬
                success_count = augmentor.create_augmented_versions(
                    resized_frames,
                    output_path_base,
                    self._save_video_frames
                )
                return success_count == 8  # æ‰€æœ‰8å€‹ç‰ˆæœ¬éƒ½å¿…é ˆæˆåŠŸ
            else:
                # ä¸ä½¿ç”¨æ•¸æ“šå¢å¼·ï¼Œåªè¼¸å‡ºæ¨™æº–åŒ–ç‰ˆæœ¬
                output_file = output_path_base + self.OUTPUT_EXT
                return self._save_video_frames(resized_frames, output_file)

        except Exception as e:
            print(f"âŒ è™•ç†å½±ç‰‡æ™‚å‡ºéŒ¯: {input_path}, éŒ¯èª¤: {str(e)}")
            import traceback
            traceback.print_exc()
            return False

    def process_directory(self, input_dir, output_dir, max_workers=None, augmentor=None):
        """
        è™•ç†æ•´å€‹ç›®éŒ„ä¸‹çš„æ‰€æœ‰å½±ç‰‡
        
        Args:
            input_dir: è¼¸å…¥ç›®éŒ„
            output_dir: è¼¸å‡ºç›®éŒ„
            max_workers: å·¥ä½œç·šç¨‹æ•¸ï¼ˆNone=è‡ªå‹•æª¢æ¸¬ï¼‰
            augmentor: DataAugmentor å¯¦ä¾‹ï¼ˆå¯é¸ï¼Œè¨“ç·´æ™‚ä½¿ç”¨ï¼‰
        """
        if not os.path.exists(input_dir):
            print(f"âŒ è¼¸å…¥ç›®éŒ„ä¸å­˜åœ¨: {input_dir}")
            return
            
        # ç¢ºä¿è¼¸å‡ºç›®éŒ„å­˜åœ¨
        os.makedirs(output_dir, exist_ok=True)
        
        # CPU ç¡¬é«”å„ªåŒ–ï¼šè‡ªå‹•æª¢æ¸¬ CPU æ ¸å¿ƒæ•¸
        if max_workers is None:
            cpu_count = psutil.cpu_count(logical=True)
            # ä½¿ç”¨ CPU æ ¸å¿ƒæ•¸çš„ 75%ï¼Œé¿å…ç³»çµ±éè¼‰
            max_workers = max(1, int(cpu_count * 0.75))
            print(f"ğŸ”§ æª¢æ¸¬åˆ° {cpu_count} å€‹é‚è¼¯ CPU æ ¸å¿ƒï¼Œä½¿ç”¨ {max_workers} å€‹å·¥ä½œç·šç¨‹")
        
        print(f"\nğŸ“‹ è™•ç†è¦æ ¼:")
        print(f"   - æ™ºèƒ½è£åˆ‡: {'âœ… å•Ÿç”¨ (MediaPipe Pose)' if self.enable_cropping else 'âŒ åœç”¨'}")
        if self.enable_cropping:
            print(f"      â””â”€ è‡ªå‹•æª¢æ¸¬æœ€å‰é¢çš„äººï¼ˆä¸ŠåŠèº« + å®Œæ•´æ‰‹è‡‚ï¼‰")
            print(f"      â””â”€ Padding: {self.CROP_PADDING * 100:.0f}%")
        print(f"   - å¹€æ•¸: {self.TARGET_FRAMES} å¹€")
        print(f"   - FPS: {self.TARGET_FPS} fps")
        print(f"   - è§£æåº¦: {self.TARGET_WIDTH}x{self.TARGET_HEIGHT}")
        if self.has_ffmpeg:
            print(f"   - ç·¨ç¢¼: h.264 (libx264) âœ“")
        else:
            print(f"   - ç·¨ç¢¼: mp4v (å»ºè­°å®‰è£ffmpegä»¥ä½¿ç”¨h264)")
        
        if augmentor is not None:
            print(f"   - æ•¸æ“šå¢å¼·: âœ… å•Ÿç”¨")
            print(f"      â””â”€ ç¢ºå®šæ€§å¢å¼·ï¼ˆåŸå§‹+æ—‹è½‰+ç¸®æ”¾+äº®åº¦+æ°´å¹³åè½‰ï¼‰")
            print(f"      â””â”€ æ¯å€‹æ¨£æœ¬è¼¸å‡º 8 å€‹ç‰ˆæœ¬")
        else:
            print(f"   - æ•¸æ“šå¢å¼·: âŒ åœç”¨ï¼ˆåƒ…è¼¸å‡ºæ¨™æº–åŒ–ç‰ˆæœ¬ï¼‰")
        print()
        
        # ç²å–æ‰€æœ‰å½±ç‰‡æ–‡ä»¶
        video_files = []
        for root, _, files in os.walk(input_dir):
            for file in files:
                if file.lower().endswith(('.mp4', '.avi', '.mov', '.wmv')):
                    rel_dir = os.path.relpath(root, input_dir)
                    src_path = os.path.join(root, file)
                    
                    # ä¿æŒç›¸åŒçš„ç›®éŒ„çµæ§‹
                    if rel_dir == '.':
                        dest_dir = output_dir
                    else:
                        dest_dir = os.path.join(output_dir, rel_dir)
                    
                    os.makedirs(dest_dir, exist_ok=True)
                    
                    # çµ±ä¸€è¼¸å‡ºæª”åç‚º.mp4
                    file_base = os.path.splitext(file)[0]
                    dest_file = file_base + self.OUTPUT_EXT
                    dest_path = os.path.join(dest_dir, dest_file)
                    
                    video_files.append((src_path, dest_path))
        
        print(f"ğŸ“ ç™¼ç¾ {len(video_files)} å€‹å½±ç‰‡æª”æ¡ˆå¾…è™•ç†\n")
        
        # ä½¿ç”¨å¤šç·šç¨‹è™•ç†å½±ç‰‡
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            results = list(tqdm(
                executor.map(
                    lambda x: self.process_video(x[0], x[1], augmentor), 
                    video_files
                ), 
                total=len(video_files), 
                desc="ğŸ¬ è™•ç†å½±ç‰‡"
            ))
        
        success_count = sum(results)
        print(f"\nâœ… æˆåŠŸè™•ç† {success_count}/{len(video_files)} å€‹å½±ç‰‡")
        
        if success_count < len(video_files):
            print(f"âš ï¸  å¤±æ•— {len(video_files) - success_count} å€‹å½±ç‰‡")

def main():
    """
    ä¸»ç¨‹å¼ - è¨“ç·´å‰è™•ç†ï¼ˆåŒ…å«æ•¸æ“šå¢å¼·ï¼‰
    
    å¦‚æœåªéœ€è¦æ¨™æº–åŒ–ï¼ˆè¾¨è­˜æ™‚ä½¿ç”¨ï¼‰ï¼Œè«‹åƒè€ƒ process_for_inference()
    """
    print("=" * 70)
    print("æ‰‹èªå½±ç‰‡å‰è™•ç†ï¼šæ™ºèƒ½è£åˆ‡ + æ¨™æº–åŒ– + æ•¸æ“šå¢å¼·ï¼ˆè¨“ç·´ç”¨ï¼‰")
    print("=" * 70)
    
    # æª¢æ¸¬æ˜¯å¦åœ¨ Kaggle ç’°å¢ƒä¸­
    is_kaggle = os.path.exists('/kaggle')
    
    if is_kaggle:
        # Kaggle ç’°å¢ƒé…ç½®
        input_dir = "/kaggle/input/augment/augmented_videos"
        output_dir = "/kaggle/working/videos_normalized"
        print("ğŸŒ æª¢æ¸¬åˆ° Kaggle ç’°å¢ƒ")
    else:
        # æœ¬åœ°ç’°å¢ƒé…ç½®
        current_dir = os.path.dirname(os.path.abspath(__file__))
        # ä½¿ç”¨augmented_videosä½œç‚ºè¼¸å…¥
        input_dir = os.path.join(os.path.dirname(current_dir), "/Users/baidongqu/Desktop/MVP/videos")
        output_dir = os.path.join(os.path.dirname(current_dir), "videos_normalized")
        print("ğŸ’» ä½¿ç”¨æœ¬åœ°ç’°å¢ƒ")
    
    print(f"ğŸ“‚ è¼¸å…¥ç›®éŒ„: {input_dir}")
    print(f"ğŸ“‚ è¼¸å‡ºç›®éŒ„: {output_dir}")
    print()
    
    # åˆå§‹åŒ–è™•ç†å™¨ï¼ˆå•Ÿç”¨æ™ºèƒ½è£åˆ‡ï¼‰
    processor = VideoProcessor(enable_cropping=True)
    
    # åˆå§‹åŒ–æ•¸æ“šå¢å¼·å™¨ï¼ˆè¨“ç·´æ™‚ä½¿ç”¨ï¼‰
    augmentor = DataAugmentor()
    print("âœ… æ•¸æ“šå¢å¼·å™¨å·²åˆå§‹åŒ–")
    print()
    
    # è™•ç†å½±ç‰‡ï¼ˆåŒ…å«æ•¸æ“šå¢å¼·ï¼‰
    processor.process_directory(input_dir, output_dir, augmentor=augmentor)
    
    # æ‰“åŒ…è¼¸å‡ºç›®éŒ„ç‚ºzipæª”æ¡ˆ
    if is_kaggle:
        zip_path = "/kaggle/working/videos_normalized.zip"
    else:
        zip_path = os.path.join(os.path.dirname(current_dir), "videos_normalized.zip")
    
    print(f"\nğŸ“¦ æ­£åœ¨æ‰“åŒ…è¼¸å‡ºæª”æ¡ˆåˆ°: {zip_path}")
    
    try:
        with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:
            for root, dirs, files in os.walk(output_dir):
                for file in files:
                    file_path = os.path.join(root, file)
                    if is_kaggle:
                        arcname = os.path.relpath(file_path, "/kaggle/working")
                    else:
                        arcname = os.path.relpath(file_path, os.path.dirname(current_dir))
                    zipf.write(file_path, arcname)
        
        print(f"âœ… æ‰“åŒ…å®Œæˆ: {zip_path}")
    except Exception as e:
        print(f"âŒ æ‰“åŒ…å¤±æ•—: {str(e)}")
    
    print("\n" + "=" * 70)
    print("å‰è™•ç†å®Œæˆï¼")
    print("=" * 70)


def process_for_inference(input_video, output_video, enable_cropping=True):
    """
    è¾¨è­˜å‰è™•ç† - åƒ…æ¨™æº–åŒ–ï¼ˆä¸å«æ•¸æ“šå¢å¼·ï¼‰
    
    ç”¨æ–¼è¾¨è­˜æ™‚çš„å½±ç‰‡å‰è™•ç†ï¼š
    1. æ™ºèƒ½è£åˆ‡äººé«”ï¼ˆå¯é¸ï¼‰
    2. å¹€æ•¸æ¨™æº–åŒ–ï¼ˆ80å¹€ï¼‰
    3. è§£æåº¦æ¨™æº–åŒ–ï¼ˆ224x224ï¼‰
    
    ä¸åŒ…å«æ•¸æ“šå¢å¼·ï¼åƒ…è¼¸å‡ºå–®å€‹æ¨™æº–åŒ–ç‰ˆæœ¬
    
    Args:
        input_video: è¼¸å…¥å½±ç‰‡è·¯å¾‘
        output_video: è¼¸å‡ºå½±ç‰‡è·¯å¾‘
        enable_cropping: æ˜¯å¦å•Ÿç”¨æ™ºèƒ½è£åˆ‡ï¼ˆé è¨­é–‹å•Ÿï¼‰
    
    Returns:
        è™•ç†æ˜¯å¦æˆåŠŸ
    
    ä½¿ç”¨ç¯„ä¾‹ï¼š
        >>> from processor import process_for_inference
        >>> success = process_for_inference('input.mp4', 'output.mp4')
    """
    print("=" * 70)
    print("æ‰‹èªå½±ç‰‡å‰è™•ç†ï¼šæ™ºèƒ½è£åˆ‡ + æ¨™æº–åŒ–ï¼ˆè¾¨è­˜ç”¨ï¼‰")
    print("=" * 70)
    
    # åˆå§‹åŒ–è™•ç†å™¨ï¼ˆä¸ä½¿ç”¨æ•¸æ“šå¢å¼·ï¼‰
    processor = VideoProcessor(enable_cropping=enable_cropping)
    
    # è™•ç†å½±ç‰‡ï¼ˆaugmentor=Noneï¼Œåªè¼¸å‡ºæ¨™æº–åŒ–ç‰ˆæœ¬ï¼‰
    success = processor.process_video(input_video, output_video, augmentor=None)
    
    if success:
        print(f"âœ… è™•ç†å®Œæˆ: {output_video}")
    else:
        print(f"âŒ è™•ç†å¤±æ•—: {input_video}")
    
    return success


if __name__ == "__main__":
    main()
