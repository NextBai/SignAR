#!/usr/bin/env python3
"""
æ‰‹èªè­˜åˆ¥æ¨è«–è…³æœ¬ - M1 MPS å„ªåŒ–ç‰ˆæœ¬

âš ï¸  æ¨¡å‹ç‰ˆæœ¬ï¼šv3.0 - Deep Improvementsï¼ˆæ·±åº¦æ”¹é€²ç‰ˆï¼‰

ğŸ¯ v3.0 é—œéµæ”¹é€²ï¼š
1. å¤šå°ºåº¦æ± åŒ–ï¼šGlobalMaxPool + GlobalAvgPool èåˆ
2. MaxNorm ç´„æŸï¼šé˜²æ­¢æ¬Šé‡éå¤§å°è‡´æ¥µç«¯ logits
3. Focal Lossï¼šé—œæ³¨é›£åˆ†é¡æ¨£æœ¬ï¼Œæ¸›å°‘ç°¡å–®æ¨£æœ¬å½±éŸ¿
4. Temperature Scalingï¼šæ ¡æº–è¼¸å‡ºä¿¡å¿ƒåº¦ï¼ˆT=1.5ï¼‰
5. Mixup æ•¸æ“šå¢å¼·ï¼šbatch ç´šåˆ¥æ¨£æœ¬æ··åˆï¼ˆåƒ…è¨“ç·´æ™‚ï¼‰
6. Label Smoothingï¼šè»ŸåŒ–æ¨™ç±¤åˆ†ä½ˆï¼ˆÎµ=0.1ï¼‰

ğŸ“Š é æœŸæ”¹å–„ï¼š
- ä¿¡å¿ƒåº¦ï¼šå¾ 98%+ â†’ 70-85%ï¼ˆæ›´åˆç†ï¼‰
- æº–ç¢ºç‡ï¼šå¾ 100% â†’ 92-96%ï¼ˆå¥åº·ï¼‰
- Top-1 åˆ° Top-5 åˆ†ä½ˆæ›´å¹³æ»‘

ğŸ”§ æ¨è«–ç‰¹æ€§ï¼š
- è‡ªå‹•è™•ç† BatchNormalization/LayerNormalizationï¼ˆæ¨è«–æ¨¡å¼ï¼‰
- Temperature Scaling å±¤è‡ªå‹•ç”Ÿæ•ˆ
- MaxNorm ç´„æŸåœ¨æ¨è«–æ™‚ä¸å½±éŸ¿æ€§èƒ½
- ç„¡éœ€ä»»ä½•ä»£ç¢¼ä¿®æ”¹ï¼Œç›´æ¥ä½¿ç”¨è¨“ç·´å¥½çš„æ¨¡å‹

æ•´åˆæµç¨‹ï¼š
1. è¦–é »æ¨™æº–åŒ– (processor.py)
2. RGB ç‰¹å¾µæå– (MobileNetV3)
3. éª¨æ¶ç‰¹å¾µæå– (MediaPipe Holistic - 159ç¶­)
4. Bi-GRU æ¨è«–ï¼ˆå¸¶æº«åº¦æ ¡æº–ï¼‰

æ”¯æŒï¼š
- M1 MPS åŠ é€Ÿï¼ˆç‰¹å¾µæå–ï¼‰
- CPU æ¨è«–ï¼ˆBiGRU åœ¨ CPU ä¸Šæ›´å¿«ï¼‰
- æ‰¹æ¬¡æ¨è«–
- æ¨¡å‹ç²¾åº¦è½‰æ›ï¼ˆbfloat16 â†’ float32ï¼‰
"""

import os
import sys
import cv2
import numpy as np
import torch
from pathlib import Path
from tqdm import tqdm
import json

# è¨­ç½® Keras backend ç‚º TensorFlowï¼ˆM1 MPS å…¼å®¹ï¼‰
os.environ['KERAS_BACKEND'] = 'tensorflow'
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'

import tensorflow as tf
import keras

# å°å…¥ç‰¹å¾µæå–å™¨
sys.path.append(str(Path(__file__).parent / "feature_extraction"))


# ==================== è‡ªå®šç¾©å±¤å®šç¾©ï¼ˆè¼‰å…¥æ¨¡å‹éœ€è¦ï¼‰====================
@keras.saving.register_keras_serializable()
class FocalLoss(keras.losses.Loss):
    """Focal Loss - ç”¨æ–¼è¼‰å…¥è¨“ç·´å¥½çš„æ¨¡å‹"""
    def __init__(self, num_classes=15, gamma=2.0, alpha=None, label_smoothing=0.1, name='focal_loss', **kwargs):
        super().__init__(name=name, **kwargs)
        self.num_classes = num_classes
        self.gamma = gamma
        self.alpha = alpha
        self.label_smoothing = label_smoothing
    
    def call(self, y_true, y_pred):
        from keras import ops
        if len(ops.shape(y_true)) == 1:
            y_true_one_hot = ops.one_hot(ops.cast(y_true, 'int32'), self.num_classes)
            if self.label_smoothing > 0:
                y_true_one_hot = y_true_one_hot * (1.0 - self.label_smoothing) + self.label_smoothing / self.num_classes
        else:
            y_true_one_hot = y_true
        epsilon = 1e-7
        y_pred = ops.clip(y_pred, epsilon, 1.0 - epsilon)
        cross_entropy = -y_true_one_hot * ops.log(y_pred)
        p_t = ops.sum(y_true_one_hot * y_pred, axis=-1, keepdims=True)
        focal_weight = ops.power(1.0 - p_t, self.gamma)
        focal_cross_entropy = focal_weight * cross_entropy
        if self.alpha is not None:
            alpha_weight = y_true_one_hot * self.alpha
            focal_cross_entropy = alpha_weight * focal_cross_entropy
        return ops.mean(ops.sum(focal_cross_entropy, axis=-1))
    
    def get_config(self):
        config = super().get_config()
        config.update({'num_classes': self.num_classes, 'gamma': self.gamma, 'alpha': self.alpha, 'label_smoothing': self.label_smoothing})
        return config


@keras.saving.register_keras_serializable()
class MixupAccuracy(keras.metrics.Metric):
    """Mixup Accuracy"""
    def __init__(self, name='mixup_accuracy', **kwargs):
        super().__init__(name=name, **kwargs)
        self.total = self.add_weight(name='total', initializer='zeros')
        self.count = self.add_weight(name='count', initializer='zeros')
    def update_state(self, y_true, y_pred, sample_weight=None):
        from keras import ops
        if len(ops.shape(y_true)) == 1:
            y_true_labels = ops.cast(y_true, 'int32')
        else:
            y_true_labels = ops.argmax(y_true, axis=-1)
        y_pred_labels = ops.argmax(y_pred, axis=-1)
        matches = ops.cast(ops.equal(y_true_labels, y_pred_labels), 'float32')
        self.total.assign_add(ops.sum(matches))
        self.count.assign_add(ops.cast(ops.size(matches), 'float32'))
    def result(self):
        return self.total / self.count
    def reset_state(self):
        self.total.assign(0.0)
        self.count.assign(0.0)


@keras.saving.register_keras_serializable()
class MixupTop3Accuracy(keras.metrics.Metric):
    """Mixup Top-3 Accuracy"""
    def __init__(self, name='mixup_top3_accuracy', **kwargs):
        super().__init__(name=name, **kwargs)
        self.total = self.add_weight(name='total', initializer='zeros')
        self.count = self.add_weight(name='count', initializer='zeros')
    def update_state(self, y_true, y_pred, sample_weight=None):
        from keras import ops
        if len(ops.shape(y_true)) == 1:
            y_true_labels = ops.cast(y_true, 'int32')
        else:
            y_true_labels = ops.argmax(y_true, axis=-1)
        top3_pred = ops.top_k(y_pred, k=3)[1]
        y_true_expanded = ops.expand_dims(y_true_labels, axis=-1)
        matches = ops.any(ops.equal(top3_pred, y_true_expanded), axis=-1)
        matches = ops.cast(matches, 'float32')
        self.total.assign_add(ops.sum(matches))
        self.count.assign_add(ops.cast(ops.size(matches), 'float32'))
    def result(self):
        return self.total / self.count
    def reset_state(self):
        self.total.assign(0.0)
        self.count.assign(0.0)


class SignLanguageInference:
    """æ‰‹èªè­˜åˆ¥æ¨è«–å™¨"""
    
    def __init__(self, model_path, label_map_path, device='mps'):
        """
        åˆå§‹åŒ–æ¨è«–å™¨
        
        Args:
            model_path: Keras æ¨¡å‹è·¯å¾‘ (.keras)
            label_map_path: æ¨™ç±¤æ˜ å°„ JSON è·¯å¾‘
            device: è¨­å‚™é¡å‹ ('mps', 'gpu', 'cpu')
        """
        self.device = device
        self.model_path = Path(model_path)
        self.label_map_path = Path(label_map_path)
        
        # è¨­ç½® TensorFlow GPU/MPS
        self._setup_tf_device()
        
        # è¼‰å…¥æ¨¡å‹å’Œæ¨™ç±¤
        self.model = self._load_model()
        self.label_map = self._load_label_map()
        self.idx_to_word = {v: k for k, v in self.label_map.items()}
        
        # åˆå§‹åŒ–ç‰¹å¾µæå–å™¨
        self._init_extractors()
        
        print(f"âœ… æ¨è«–å™¨åˆå§‹åŒ–å®Œæˆï¼æ”¯æŒ {len(self.label_map)} å€‹æ‰‹èªå–®è©")
    
    def _setup_tf_device(self):
        """è¨­ç½® TensorFlow è¨­å‚™"""
        if self.device == 'mps':
            # M1 MPS è¨­ç½®
            try:
                # TensorFlow Metal æ’ä»¶æœƒè‡ªå‹•ä½¿ç”¨ MPS
                physical_devices = tf.config.list_physical_devices('GPU')
                if physical_devices:
                    print(f"ğŸš€ ä½¿ç”¨ M1 MPS åŠ é€Ÿï¼ˆ{len(physical_devices)} å€‹ GPUï¼‰")
                else:
                    print("âš ï¸  æœªæª¢æ¸¬åˆ° MPSï¼Œä½¿ç”¨ CPU")
                    self.device = 'cpu'
            except Exception as e:
                print(f"âš ï¸  MPS è¨­ç½®å¤±æ•—: {e}ï¼Œä½¿ç”¨ CPU")
                self.device = 'cpu'
        elif self.device == 'gpu':
            gpus = tf.config.list_physical_devices('GPU')
            if gpus:
                try:
                    for gpu in gpus:
                        tf.config.experimental.set_memory_growth(gpu, True)
                    print(f"ğŸš€ ä½¿ç”¨ CUDA GPUï¼ˆ{len(gpus)} å€‹ï¼‰")
                except RuntimeError as e:
                    print(f"âš ï¸  GPU è¨­ç½®å¤±æ•—: {e}")
        else:
            print("ğŸ’» ä½¿ç”¨ CPU")
    
    def _load_model(self):
        """è¼‰å…¥ Keras æ¨¡å‹ä¸¦è½‰æ›ç²¾åº¦"""
        print(f"ğŸ“¥ è¼‰å…¥æ¨¡å‹: {self.model_path}")
        
        try:
            # å•Ÿç”¨ unsafe deserializationï¼ˆè™•ç† Lambda å±¤ï¼‰
            keras.config.enable_unsafe_deserialization()
            
            # è¼‰å…¥æ¨¡å‹ï¼ˆKeras 3 æœƒè‡ªå‹•è™•ç† backend å·®ç•°ï¼‰
            custom_objects = {
                'FocalLoss': FocalLoss,
                'MixupAccuracy': MixupAccuracy,
                'MixupTop3Accuracy': MixupTop3Accuracy
            }
            model = keras.models.load_model(self.model_path, custom_objects=custom_objects)
            
            print(f"âœ… æ¨¡å‹è¼‰å…¥æˆåŠŸ")
            print(f"   è¼¸å…¥å½¢ç‹€: {model.input_shape}")
            print(f"   è¼¸å‡ºå½¢ç‹€: {model.output_shape}")
            
            # æª¢æŸ¥æ··åˆç²¾åº¦ï¼Œå¦‚æœæ˜¯ bfloat16 å‰‡è½‰æ›ç‚º float32
            if hasattr(model, 'dtype_policy'):
                print(f"   åŸå§‹ç²¾åº¦: {model.dtype_policy}")
            
            # åœ¨ M1 MPS ä¸Šå¼·åˆ¶ä½¿ç”¨ float32
            if self.device == 'mps':
                print("ğŸ”§ è½‰æ›æ¨¡å‹ç²¾åº¦ç‚º float32ï¼ˆM1 MPS å…¼å®¹ï¼‰")
                # Keras 3 æ¨¡å‹æœƒè‡ªå‹•è™•ç†ç²¾åº¦
                keras.mixed_precision.set_global_policy('float32')
            
            return model
            
        except Exception as e:
            print(f"âŒ æ¨¡å‹è¼‰å…¥å¤±æ•—: {e}")
            raise
    
    def _load_label_map(self):
        """è¼‰å…¥æ¨™ç±¤æ˜ å°„"""
        print(f"ğŸ“‹ è¼‰å…¥æ¨™ç±¤æ˜ å°„: {self.label_map_path}")
        
        with open(self.label_map_path, 'r', encoding='utf-8') as f:
            label_map = json.load(f)
        
        print(f"âœ… è¼‰å…¥ {len(label_map)} å€‹æ¨™ç±¤")
        return label_map
    
    def _init_extractors(self):
        """åˆå§‹åŒ–ç‰¹å¾µæå–å™¨"""
        print("\nğŸ”§ åˆå§‹åŒ–ç‰¹å¾µæå–å™¨...")
        
        # RGB ç‰¹å¾µæå–å™¨ï¼ˆPyTorch - MPS åŠ é€Ÿï¼‰
        from rgb_feature_extraction import RGBFeatureExtractor
        
        if self.device == 'mps':
            torch_device = torch.device('mps')
            device_type = 'gpu'
        elif self.device == 'gpu':
            torch_device = torch.device('cuda')
            device_type = 'gpu'
        else:
            torch_device = torch.device('cpu')
            device_type = 'cpu'
        
        self.rgb_extractor = RGBFeatureExtractor(torch_device, device_type)
        
        # éª¨æ¶ç‰¹å¾µæå–å™¨ï¼ˆMediaPipe Holistic - 159ç¶­ï¼Œè‡ªå‹•ä½¿ç”¨ Metal åŠ é€Ÿï¼‰
        from skeleton_feature_extraction import EnhancedSkeletonExtractor

        # MediaPipe Holistic è‡ªå‹•å„ªåŒ–
        self.skeleton_extractor = EnhancedSkeletonExtractor(num_threads=4)
        
        print("âœ… ç‰¹å¾µæå–å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def extract_features(self, video_path, max_length=300):
        """
        æå–è¦–é »ç‰¹å¾µï¼ˆRGB + Skeletonï¼‰
        
        Args:
            video_path: è¦–é »è·¯å¾‘
            max_length: æœ€å¤§åºåˆ—é•·åº¦
        
        Returns:
            features: (max_length, 1011) çš„ç‰¹å¾µå‘é‡
        """
        video_path = Path(video_path)
        
        # 1. æå– RGB ç‰¹å¾µ (T, 960)
        rgb_features = self.rgb_extractor.extract_features(video_path)
        if rgb_features is None:
            raise ValueError(f"ç„¡æ³•æå– RGB ç‰¹å¾µ: {video_path}")

        # 2. æå–éª¨æ¶ç‰¹å¾µ (T, 159) - MediaPipe Holistic
        skeleton_features = self.skeleton_extractor.extract_features(video_path)
        if skeleton_features is None:
            raise ValueError(f"ç„¡æ³•æå–éª¨æ¶ç‰¹å¾µ: {video_path}")

        # 3. å°é½Šé•·åº¦
        min_len = min(len(rgb_features), len(skeleton_features))
        rgb_features = rgb_features[:min_len]
        skeleton_features = skeleton_features[:min_len]

        # 4. Concat èåˆ (T, 1119)
        concat_features = np.concatenate([rgb_features, skeleton_features], axis=1)

        # 5. Padding æˆ–æˆªæ–·åˆ°å›ºå®šé•·åº¦
        if len(concat_features) < max_length:
            padding = np.zeros((max_length - len(concat_features), 1119), dtype=np.float32)
            concat_features = np.concatenate([concat_features, padding], axis=0)
        else:
            concat_features = concat_features[:max_length]
        
        return concat_features
    
    def predict(self, video_path, top_k=3):
        """
        é æ¸¬å–®å€‹è¦–é »
        
        Args:
            video_path: è¦–é »è·¯å¾‘
            top_k: è¿”å›å‰ k å€‹é æ¸¬çµæœ
        
        Returns:
            predictions: [(word, confidence), ...] æŒ‰ä¿¡å¿ƒåº¦æ’åº
        """
        # æå–ç‰¹å¾µ
        features = self.extract_features(video_path)
        
        # æ“´å±•ç‚º batch (1, max_length, 1011)
        features_batch = np.expand_dims(features, axis=0)
        
        # æ¨è«–
        with tf.device('/GPU:0' if self.device in ['mps', 'gpu'] else '/CPU:0'):
            predictions = self.model.predict(features_batch, verbose=0)
        
        # ç²å– top-k çµæœ
        top_indices = np.argsort(predictions[0])[::-1][:top_k]
        
        results = []
        for idx in top_indices:
            word = self.idx_to_word[idx]
            confidence = float(predictions[0][idx])
            results.append((word, confidence))
        
        return results
    
    def predict_batch(self, video_paths, batch_size=8, top_k=3):
        """
        æ‰¹æ¬¡é æ¸¬å¤šå€‹è¦–é »
        
        Args:
            video_paths: è¦–é »è·¯å¾‘åˆ—è¡¨
            batch_size: æ‰¹æ¬¡å¤§å°
            top_k: è¿”å›å‰ k å€‹é æ¸¬çµæœ
        
        Returns:
            all_results: [[(word, confidence), ...], ...]
        """
        all_results = []
        
        for i in tqdm(range(0, len(video_paths), batch_size), desc="æ‰¹æ¬¡æ¨è«–"):
            batch_paths = video_paths[i:i + batch_size]
            batch_features = []
            
            # æå–ç‰¹å¾µ
            for video_path in batch_paths:
                try:
                    features = self.extract_features(video_path)
                    batch_features.append(features)
                except Exception as e:
                    print(f"âš ï¸  è·³é {video_path}: {e}")
                    batch_features.append(np.zeros((300, 1119), dtype=np.float32))
            
            # æ‰¹æ¬¡æ¨è«–
            batch_features = np.array(batch_features)  # (batch, 300, 1119)
            
            with tf.device('/GPU:0' if self.device in ['mps', 'gpu'] else '/CPU:0'):
                predictions = self.model.predict(batch_features, verbose=0)
            
            # è§£æçµæœ
            for pred in predictions:
                top_indices = np.argsort(pred)[::-1][:top_k]
                results = [(self.idx_to_word[idx], float(pred[idx])) for idx in top_indices]
                all_results.append(results)
        
        return all_results


def demo_single_video():
    """å–®å€‹è¦–é »æ¨è«–ç¤ºç¯„"""
    print("=" * 70)
    print("ğŸ¯ æ‰‹èªè­˜åˆ¥æ¨è«– - å–®å€‹è¦–é »ç¤ºç¯„")
    print("=" * 70)
    
    # é…ç½®è·¯å¾‘
    model_dir = Path("/Users/baidongqu/Desktop/MVP/model_output")
    model_path = model_dir / "best_model_mps.keras"
    label_map_path = model_dir / "label_map.json"
    
    if not model_path.exists():
        print(f"âŒ æ¨¡å‹ä¸å­˜åœ¨: {model_path}")
        return
    
    # åˆå§‹åŒ–æ¨è«–å™¨
    inferencer = SignLanguageInference(
        model_path=model_path,
        label_map_path=label_map_path,
        device='mps'  # M1 Mac ä½¿ç”¨ 'mps'ï¼ŒCUDA GPU ä½¿ç”¨ 'gpu'ï¼Œå¦å‰‡ 'cpu'
    )
    
    # æ¸¬è©¦è¦–é »
    test_video = Path("/Users/baidongqu/Desktop/MVP/1.MP4")
    
    if not test_video.exists():
        print(f"âŒ æ¸¬è©¦è¦–é »ä¸å­˜åœ¨: {test_video}")
        return
    
    print(f"\nğŸ¬ æ¸¬è©¦è¦–é »: {test_video.name}")
    print("ğŸ”„ é–‹å§‹æ¨è«–...")
    
    # æ¨è«–
    results = inferencer.predict(test_video, top_k=5)
    
    # è¼¸å‡ºçµæœ
    print("\n" + "=" * 70)
    print("ğŸ‰ æ¨è«–çµæœï¼š")
    print("=" * 70)
    for i, (word, confidence) in enumerate(results, 1):
        bar = "â–ˆ" * int(confidence * 50)
        print(f"{i}. {word:15s} {confidence*100:6.2f}% {bar}")
    print("=" * 70)


def demo_batch_inference():
    """æ‰¹æ¬¡æ¨è«–ç¤ºç¯„"""
    print("=" * 70)
    print("ğŸ¯ æ‰‹èªè­˜åˆ¥æ¨è«– - æ‰¹æ¬¡è™•ç†ç¤ºç¯„")
    print("=" * 70)
    
    # é…ç½®è·¯å¾‘
    model_dir = Path("/Users/baidongqu/Desktop/MVP/model_output")
    model_path = model_dir / "best_model_mps.keras"
    label_map_path = model_dir / "label_map.json"
    
    # åˆå§‹åŒ–æ¨è«–å™¨
    inferencer = SignLanguageInference(
        model_path=model_path,
        label_map_path=label_map_path,
        device='mps'
    )
    
    # ç²å–æ¸¬è©¦è¦–é »ï¼ˆæ¯å€‹é¡åˆ¥é¸ 3 å€‹ï¼‰
    videos_dir = Path("/Users/baidongqu/Desktop/MVP/videos")
    test_videos = []
    ground_truth = []
    
    for word_dir in sorted(videos_dir.iterdir()):
        if word_dir.is_dir():
            videos = list(word_dir.glob("*.mp4"))[:3]  # æ¯å€‹é¡åˆ¥ 3 å€‹è¦–é »
            test_videos.extend(videos)
            ground_truth.extend([word_dir.name] * len(videos))
    
    print(f"\nğŸ“Š æ¸¬è©¦é›†: {len(test_videos)} å€‹è¦–é »")
    
    # æ‰¹æ¬¡æ¨è«–
    results = inferencer.predict_batch(test_videos, batch_size=4, top_k=3)
    
    # çµ±è¨ˆæº–ç¢ºç‡
    correct = 0
    top3_correct = 0
    
    print("\n" + "=" * 70)
    print("ğŸ“‹ è©³ç´°çµæœï¼š")
    print("=" * 70)
    
    for i, (video_path, gt, preds) in enumerate(zip(test_videos, ground_truth, results)):
        top1_word, top1_conf = preds[0]
        
        if top1_word == gt:
            correct += 1
            status = "âœ…"
        else:
            status = "âŒ"
        
        if any(word == gt for word, _ in preds):
            top3_correct += 1
        
        print(f"{status} {video_path.name:25s} | GT: {gt:10s} | Pred: {top1_word:10s} ({top1_conf*100:.1f}%)")
    
    # è¼¸å‡ºçµ±è¨ˆ
    print("\n" + "=" * 70)
    print("ğŸ“Š çµ±è¨ˆçµæœï¼š")
    print("=" * 70)
    print(f"Top-1 æº–ç¢ºç‡: {correct}/{len(test_videos)} = {correct/len(test_videos)*100:.2f}%")
    print(f"Top-3 æº–ç¢ºç‡: {top3_correct}/{len(test_videos)} = {top3_correct/len(test_videos)*100:.2f}%")
    print("=" * 70)


def main():
    """ä¸»å‡½æ•¸"""
    import argparse
    
    parser = argparse.ArgumentParser(description='æ‰‹èªè­˜åˆ¥æ¨è«–')
    parser.add_argument('--mode', type=str, default='single', choices=['single', 'batch'],
                       help='æ¨è«–æ¨¡å¼: single (å–®å€‹è¦–é ») æˆ– batch (æ‰¹æ¬¡è™•ç†)')
    parser.add_argument('--video', type=str, help='è¦–é »è·¯å¾‘ï¼ˆsingle æ¨¡å¼ï¼‰')
    parser.add_argument('--model', type=str, default='model_output/best_model.keras',
                       help='æ¨¡å‹è·¯å¾‘')
    parser.add_argument('--device', type=str, default='mps', choices=['mps', 'gpu', 'cpu'],
                       help='è¨­å‚™é¡å‹')
    
    args = parser.parse_args()
    
    if args.mode == 'single':
        if args.video:
            # è‡ªå®šç¾©è¦–é »
            model_dir = Path(args.model).parent
            inferencer = SignLanguageInference(
                model_path=args.model,
                label_map_path=model_dir / "label_map.json",
                device=args.device
            )
            results = inferencer.predict(args.video, top_k=5)
            
            print("\nğŸ‰ æ¨è«–çµæœï¼š")
            for i, (word, confidence) in enumerate(results, 1):
                print(f"{i}. {word}: {confidence*100:.2f}%")
        else:
            # ç¤ºç¯„æ¨¡å¼
            demo_single_video()
    else:
        demo_batch_inference()


if __name__ == "__main__":
    main()

