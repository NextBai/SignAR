#!/usr/bin/env python3
"""
Bi-GRU æ‰‹èªè­˜åˆ¥è¨“ç·´è…³æœ¬ - Kaggle TPU å„ªåŒ–ç‰ˆæœ¬
å°ˆç‚º Kaggle TPU v3-8 å„ªåŒ–
- æ”¯æŒ TPU v3-8 (Kaggle é»˜èªæä¾›)
- ç°¡åŒ–æ•¸æ“šå¢å¼·é‚è¼¯
- å„ªåŒ–æ•¸æ“šç®¡ç·šä»¥å……åˆ†åˆ©ç”¨ TPU æ€§èƒ½
"""

import os
import warnings

# ==================== Kaggle TPU ç’°å¢ƒè¨­ç½®ï¼ˆå¿…é ˆåœ¨å°å…¥ä»»ä½•æ·±åº¦å­¸ç¿’åº«ä¹‹å‰ï¼‰====================
print("ğŸ”§ é…ç½® Kaggle TPU Runtime...")

# è¨­ç½® PJRT è¨­å‚™ç‚º TPUï¼ˆTPU v5e-8 å¿…éœ€ï¼‰
os.environ['PJRT_DEVICE'] = 'TPU'

# è¨­ç½® Keras ä½¿ç”¨ JAX backendï¼ˆèˆ‡ PJRT TPU å®Œç¾å…¼å®¹ï¼‰
os.environ['KERAS_BACKEND'] = 'jax'

# å…¶ä»–ç’°å¢ƒè¨­ç½®
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
os.environ['XLA_PYTHON_CLIENT_PREALLOCATE'] = 'false'  # JAX å…§å­˜å„ªåŒ–

import numpy as np
import jax
import tensorflow as tf  # åƒ…ç”¨æ–¼ tf.data pipeline
import keras
from pathlib import Path
from datetime import datetime
from tqdm import tqdm
import json
import hashlib
import random


# ==================== TPU åˆå§‹åŒ–ï¼ˆKaggle TPU v5e-8 PJRT + JAX Backendï¼‰====================
def init_tpu_strategy():
    """åˆå§‹åŒ– TPU ç­–ç•¥ï¼ˆä½¿ç”¨ Keras JAX backend + PJRT Runtimeï¼‰"""
    print("\nğŸš€ åˆå§‹åŒ– Kaggle TPU v5e-8 (Keras JAX Backend + PJRT)...")
    
    try:
        # æª¢æŸ¥ JAX å’Œ Keras backend
        print(f"ğŸ” Keras Backend: {keras.backend.backend()}")
        
        # æª¢æŸ¥ JAX TPU é€£æ¥
        jax_devices = jax.devices()
        print(f"âœ… JAX æª¢æ¸¬åˆ° {len(jax_devices)} å€‹ TPU è¨­å‚™")
        print(f"   è¨­å‚™é¡å‹: {jax_devices[0].platform}")
        
        # ä½¿ç”¨ Keras çš„åˆ†ä½ˆå¼ APIï¼ˆèˆ‡ JAX backend å®Œç¾å…¼å®¹ï¼‰
        print("ğŸ“¡ æ­£åœ¨å‰µå»º Keras åˆ†ä½ˆå¼ç­–ç•¥...")
        
        # Keras 3 + JAX backend æœƒè‡ªå‹•ä½¿ç”¨æ‰€æœ‰å¯ç”¨çš„ JAX è¨­å‚™
        # ä¸éœ€è¦æ‰‹å‹•å‰µå»º strategyï¼ŒKeras æœƒè‡ªå‹•åˆ†ä½ˆè¨“ç·´
        num_devices = len(jax_devices)
        print(f"âœ… Keras å°‡ä½¿ç”¨ {num_devices} å€‹ TPU è¨­å‚™é€²è¡Œåˆ†ä½ˆå¼è¨“ç·´")
        
        # å•Ÿç”¨æ··åˆç²¾åº¦è¨“ç·´ï¼ˆTPU v5e-8 åŠ é€Ÿé—œéµï¼‰
        keras.mixed_precision.set_global_policy('mixed_bfloat16')
        print("âœ… æ··åˆç²¾åº¦è¨“ç·´å·²å•Ÿç”¨: mixed_bfloat16")
        
        # è¿”å›è¨­å‚™æ•¸é‡ï¼ˆç”¨æ–¼è¨ˆç®—å…¨å±€æ‰¹æ¬¡å¤§å°ï¼‰
        class DummyStrategy:
            def __init__(self, num_replicas):
                self.num_replicas_in_sync = num_replicas
        
        return DummyStrategy(num_devices)
        
    except Exception as e:
        print(f"\nâŒ TPU åˆå§‹åŒ–å¤±æ•—: {e}")
        print("\nğŸ’¡ æ•…éšœæ’é™¤å»ºè­°ï¼š")
        print("1. ç¢ºèª Kaggle Notebook å·²å•Ÿç”¨ TPU åŠ é€Ÿå™¨ï¼ˆSettings â†’ Accelerator â†’ TPU v5 litepodï¼‰")
        print("2. é‡æ–°å•Ÿå‹• Kernel ä¸¦å†æ¬¡åŸ·è¡Œ")
        print("3. ç¢ºèªä½¿ç”¨çš„æ˜¯ Kaggle ç’°å¢ƒè€Œéæœ¬åœ°ç’°å¢ƒ")
        print("4. æª¢æŸ¥ KERAS_BACKEND=jax ç’°å¢ƒè®Šæ•¸æ˜¯å¦è¨­ç½®")
        
        # æ‰“å°è©³ç´°èª¿è©¦ä¿¡æ¯
        print("\nğŸ” ç’°å¢ƒè¨ºæ–·ä¿¡æ¯ï¼š")
        print(f"   Keras ç‰ˆæœ¬: {keras.__version__}")
        print(f"   Keras Backend: {keras.backend.backend()}")
        print(f"   JAX ç‰ˆæœ¬: {jax.__version__}")
        print(f"   PJRT_DEVICE: {os.environ.get('PJRT_DEVICE', 'Not Set')}")
        print(f"   KERAS_BACKEND: {os.environ.get('KERAS_BACKEND', 'Not Set')}")
        print(f"   JAX è¨­å‚™: {jax.devices()}")
        
        raise RuntimeError(f"Kaggle TPU v5e-8 åˆå§‹åŒ–å¤±æ•—: {e}")


# ==================== æ•¸æ“šé›†é¡ ====================
class SignLanguageDataset:
    """æ‰‹èªæ•¸æ“šé›†åŠ è¼‰å™¨ - TPU å„ªåŒ–ç‰ˆæœ¬ï¼ˆå«å»é‡å’Œæ•¸æ“šå¢å¼·ï¼‰"""
    
    def __init__(self, rgb_dir, skeleton_dir, max_length=300, use_augmentation=True, use_mixup=False, mixup_alpha=0.2):
        """
        åˆå§‹åŒ–æ•¸æ“šé›†
        
        Args:
            rgb_dir: RGB ç‰¹å¾µç›®éŒ„ (960ç¶­)
            skeleton_dir: Skeleton ç‰¹å¾µç›®éŒ„ (159ç¶­ - MediaPipe Holistic)
            max_length: æœ€å¤§åºåˆ—é•·åº¦
            use_augmentation: æ˜¯å¦ä½¿ç”¨æ•¸æ“šå¢å¼·å¹³è¡¡é¡åˆ¥
            use_mixup: æ˜¯å¦å•Ÿç”¨ Mixup å¢å¼·ï¼ˆå°æŠ—ç‰¹å¾µæ··ç–Šï¼‰
            mixup_alpha: Mixup çš„ Beta åˆ†ä½ˆåƒæ•¸
        """
        self.rgb_dir = Path(rgb_dir)
        self.skeleton_dir = Path(skeleton_dir)
        self.max_length = max_length
        self.use_augmentation = use_augmentation
        self.use_mixup = use_mixup
        self.mixup_alpha = mixup_alpha
        
        self.samples = []
        self.label_map = {}
        self._build_dataset()
        
        # å¦‚æœå•Ÿç”¨å¢å¼·ï¼Œé€²è¡Œé¡åˆ¥å¹³è¡¡
        if use_augmentation:
            self._balance_classes_with_augmentation()
    
    def _build_dataset(self):
        """æ§‹å»ºæ•¸æ“šé›†ç´¢å¼• - ä½¿ç”¨ Hash å»é‡"""
        print("ğŸ“š è¼‰å…¥æ•¸æ“šé›†ï¼ˆå»é‡æ¨¡å¼ï¼‰...")
        
        # ç²å–æ‰€æœ‰å–®è©ç›®éŒ„ä¸¦å‰µå»ºæ¨™ç±¤æ˜ å°„
        word_dirs = sorted([d for d in self.rgb_dir.iterdir() if d.is_dir()])
        self.label_map = {word.name: idx for idx, word in enumerate(word_dirs)}
        self.num_classes = len(self.label_map)
        print(f"ğŸ“ é¡åˆ¥æ•¸é‡: {self.num_classes}")
        
        # æ”¶é›†æ¨£æœ¬ä¸¦å»é‡
        seen_hashes = set()
        duplicate_count = 0
        
        for word_dir in tqdm(word_dirs, desc="æƒææ•¸æ“š"):
            word_name = word_dir.name
            label = self.label_map[word_name]
            
            rgb_files = list((self.rgb_dir / word_name).glob("*.npy"))
            
            for rgb_file in rgb_files:
                skeleton_file = self.skeleton_dir / word_name / rgb_file.name
                
                if skeleton_file.exists():
                    try:
                        # è¨ˆç®— RGB æ•¸æ“šçš„ hash ä¾†å»é‡
                        rgb_data = np.load(rgb_file)
                        data_hash = hashlib.md5(rgb_data.tobytes()).hexdigest()
                        
                        if data_hash not in seen_hashes:
                            seen_hashes.add(data_hash)
                            self.samples.append({
                                'rgb_path': str(rgb_file),
                                'skeleton_path': str(skeleton_file),
                                'label': label,
                                'word': word_name
                            })
                        else:
                            duplicate_count += 1
                    except Exception as e:
                        print(f"âš ï¸  è¼‰å…¥å¤±æ•— {rgb_file.name}: {e}")
                        continue
        
        print(f"âœ… ç¸½å”¯ä¸€æ¨£æœ¬æ•¸: {len(self.samples)}")
        if duplicate_count > 0:
            print(f"ğŸ—‘ï¸  ç§»é™¤é‡è¤‡æ¨£æœ¬: {duplicate_count} å€‹")
        
        # æ‰“å°é¡åˆ¥åˆ†å¸ƒ
        class_counts = {}
        for sample in self.samples:
            word = sample['word']
            class_counts[word] = class_counts.get(word, 0) + 1
        
        print("\nğŸ“Š å»é‡å¾Œé¡åˆ¥åˆ†å¸ƒ:")
        for word, count in sorted(class_counts.items()):
            print(f"  {word}: {count} æ¨£æœ¬")
    
    def _balance_classes_with_augmentation(self):
        """ä½¿ç”¨è¼•é‡ç´šæ•¸æ“šå¢å¼·å¹³è¡¡é¡åˆ¥"""
        print("\nâš–ï¸  å•Ÿç”¨é¡åˆ¥å¹³è¡¡å¢å¼·...")
        
        # çµ±è¨ˆç•¶å‰é¡åˆ¥åˆ†å¸ƒ
        class_counts = {}
        class_samples = {}
        for sample in self.samples:
            word = sample['word']
            if word not in class_samples:
                class_samples[word] = []
            class_samples[word].append(sample)
            class_counts[word] = class_counts.get(word, 0) + 1
        
        # æ‰¾åˆ°ç›®æ¨™æ¨£æœ¬æ•¸ï¼ˆæœ€å¤šçš„é¡åˆ¥ï¼‰
        target_count = max(class_counts.values())
        print(f"ğŸ¯ ç›®æ¨™æ¨£æœ¬æ•¸: {target_count} (æ¯å€‹é¡åˆ¥)")
        
        # å°æ¯å€‹é¡åˆ¥é€²è¡Œå¢å¼·
        balanced_samples = list(self.samples)  # ä¿ç•™åŸå§‹æ¨£æœ¬
        
        for word, samples in class_samples.items():
            current_count = len(samples)
            needed_count = target_count - current_count
            
            if needed_count <= 0:
                print(f"âœ… {word}: {current_count} æ¨£æœ¬ (ç„¡éœ€å¢å¼·)")
                continue
            
            print(f"ğŸ”§ {word}: {current_count} â†’ {target_count} æ¨£æœ¬ (éœ€è¦ {needed_count} å€‹å¢å¼·)")
            
            # ç°¡å–®ç­–ç•¥ï¼šå¾ç¾æœ‰æ¨£æœ¬ä¸­éš¨æ©Ÿé¸æ“‡ä¸¦æ¨™è¨˜ç‚ºéœ€è¦å¢å¼·
            for i in range(needed_count):
                original_sample = random.choice(samples)
                # å‰µå»ºå¢å¼·æ¨£æœ¬çš„å¼•ç”¨ï¼ˆå¯¦éš›å¢å¼·åœ¨è¼‰å…¥æ™‚å‹•æ…‹é€²è¡Œï¼‰
                augmented_sample = {
                    'rgb_path': original_sample['rgb_path'],
                    'skeleton_path': original_sample['skeleton_path'],
                    'label': original_sample['label'],
                    'word': original_sample['word'],
                    'augment': True  # æ¨™è¨˜ç‚ºéœ€è¦å¢å¼·
                }
                balanced_samples.append(augmented_sample)
        
        self.samples = balanced_samples
        print(f"âœ… å¹³è¡¡å®Œæˆï¼ç¸½æ¨£æœ¬æ•¸: {len(self.samples)}")
        
        # é‡æ–°çµ±è¨ˆæœ€çµ‚åˆ†å¸ƒ
        final_counts = {}
        for sample in self.samples:
            word = sample['word']
            final_counts[word] = final_counts.get(word, 0) + 1
        
        print("\nğŸ“Š å¹³è¡¡å¾Œé¡åˆ¥åˆ†å¸ƒ:")
        for word, count in sorted(final_counts.items()):
            print(f"  {word}: {count} æ¨£æœ¬")
    
    def _apply_mixup(self, feat1, label1, feat2, label2, alpha=0.2):
        """
        æ‡‰ç”¨ Mixup æ•¸æ“šå¢å¼·ï¼ˆå°æŠ—ç‰¹å¾µæ··ç–Šï¼‰
        
        Mixup é€šéæ··åˆå…©å€‹æ¨£æœ¬ï¼Œå¼·åˆ¶æ¨¡å‹å­¸ç¿’æ›´å¹³æ»‘çš„æ±ºç­–é‚Šç•Œï¼Œ
        é˜²æ­¢ embedding space ä¸­çš„ç‰¹å¾µéåº¦é›†ä¸­ã€‚
        
        Args:
            feat1, feat2: å…©å€‹ç‰¹å¾µæ¨£æœ¬
            label1, label2: å°æ‡‰æ¨™ç±¤
            alpha: Beta åˆ†ä½ˆåƒæ•¸ï¼ˆè¶Šå°æ··åˆè¶Šæº«å’Œï¼‰
        
        Returns:
            mixed_feat: æ··åˆå¾Œçš„ç‰¹å¾µ
            mixed_label: æ··åˆå¾Œçš„æ¨™ç±¤ï¼ˆè»Ÿæ¨™ç±¤ï¼‰
        """
        # å¾ Beta åˆ†ä½ˆæ¡æ¨£æ··åˆæ¯”ä¾‹
        lam = np.random.beta(alpha, alpha) if alpha > 0 else 1.0
        
        # æ··åˆç‰¹å¾µï¼ˆç·šæ€§æ’å€¼ï¼‰
        mixed_feat = lam * feat1 + (1 - lam) * feat2
        
        # æ··åˆæ¨™ç±¤ï¼ˆå‰µå»ºè»Ÿæ¨™ç±¤ï¼‰
        # æ³¨æ„ï¼šé€™è£¡è¿”å›å–®å€‹æ¨™ç±¤ï¼Œä½†å¸¶æœ‰æ··åˆä¿¡æ¯ï¼ˆç”¨æ–¼å¾ŒçºŒè™•ç†ï¼‰
        # å¯¦éš›æ‡‰ç”¨ä¸­éœ€è¦åœ¨ loss ä¸­ä½¿ç”¨è»Ÿæ¨™ç±¤
        return mixed_feat.astype(np.float32), label1, lam
    
    def _apply_light_augmentation(self, rgb_feat, skeleton_feat):
        """
        æ‡‰ç”¨è¼•é‡ç´šæ™‚åºå¢å¼·ï¼ˆæ‰€æœ‰è¨“ç·´æ¨£æœ¬ä½¿ç”¨ï¼‰
        ç­–ç•¥ï¼šéš¨æ©Ÿæ‡‰ç”¨ 1-2 ç¨®è¼•åº¦å¢å¼·ï¼Œä¿æŒæ•¸æ“šçœŸå¯¦æ€§
        """
        # éš¨æ©Ÿé¸æ“‡ 1-2 ç¨®å¢å¼·æ–¹æ³•
        aug_methods = random.sample([
            'gaussian_noise',
            'time_masking', 
            'feature_scaling',
            'temporal_shift'
        ], k=random.randint(1, 2))
        
        for method in aug_methods:
            if method == 'gaussian_noise':
                # è¼•åº¦é«˜æ–¯å™ªè²
                noise_std = random.uniform(0.005, 0.015)  # é™ä½å™ªè²å¼·åº¦
                rgb_feat = rgb_feat + np.random.normal(0, noise_std, rgb_feat.shape).astype(np.float32)
                skeleton_feat = skeleton_feat + np.random.normal(0, noise_std * 0.5, skeleton_feat.shape).astype(np.float32)
                
            elif method == 'time_masking':
                # æ™‚é–“é®ç½©ï¼ˆè¼•åº¦ï¼‰
                seq_len = rgb_feat.shape[0]
                mask_ratio = random.uniform(0.05, 0.1)  # é™ä½é®ç½©æ¯”ä¾‹
                mask_len = int(seq_len * mask_ratio)
                if mask_len > 0:
                    start_idx = random.randint(0, seq_len - mask_len)
                    rgb_feat[start_idx:start_idx + mask_len, :] *= random.uniform(0.3, 0.7)  # å¼±åŒ–
                    skeleton_feat[start_idx:start_idx + mask_len, :] *= random.uniform(0.3, 0.7)
                    
            elif method == 'feature_scaling':
                # ç‰¹å¾µç¸®æ”¾ï¼ˆè¼•åº¦ï¼‰
                scale = random.uniform(0.95, 1.05)  # é™ä½ç¸®æ”¾ç¯„åœ
                rgb_feat = rgb_feat * scale
                
            elif method == 'temporal_shift':
                # æ™‚é–“åç§»ï¼ˆè¼•å¾®ï¼‰
                shift = random.randint(-2, 2)  # æ¸›å°‘åç§»ç¯„åœ
                if shift != 0:
                    rgb_feat = np.roll(rgb_feat, shift, axis=0)
                    skeleton_feat = np.roll(skeleton_feat, shift, axis=0)
        
        return rgb_feat, skeleton_feat
    
    def _apply_strong_augmentation(self, rgb_feat, skeleton_feat):
        """
        æ‡‰ç”¨å¼·åŠ›çµ„åˆå¢å¼·ï¼ˆé¡åˆ¥å¹³è¡¡å°ˆç”¨ï¼‰
        ç­–ç•¥ï¼šçµ„åˆå¤šç¨®å¢å¼·ï¼Œå‰µå»ºæ›´å¤šæ¨£åŒ–çš„è¨“ç·´æ¨£æœ¬
        """
        # å¿…å®šæ‡‰ç”¨å¤šç¨®å¢å¼·ï¼ˆ2-3ç¨®ï¼‰
        aug_methods = random.sample([
            'gaussian_noise',
            'time_masking', 
            'feature_scaling',
            'temporal_shift',
            'speed_perturbation',  # æ–°å¢ï¼šé€Ÿåº¦æ“¾å‹•
            'feature_dropout'       # æ–°å¢ï¼šç‰¹å¾µdropout
        ], k=random.randint(2, 3))
        
        for method in aug_methods:
            if method == 'gaussian_noise':
                # è¼ƒå¼·çš„é«˜æ–¯å™ªè²
                noise_std = random.uniform(0.02, 0.04)
                rgb_feat = rgb_feat + np.random.normal(0, noise_std, rgb_feat.shape).astype(np.float32)
                skeleton_feat = skeleton_feat + np.random.normal(0, noise_std * 0.5, skeleton_feat.shape).astype(np.float32)
                
            elif method == 'time_masking':
                # è¼ƒå¼·çš„æ™‚é–“é®ç½©
                seq_len = rgb_feat.shape[0]
                mask_ratio = random.uniform(0.15, 0.25)
                mask_len = int(seq_len * mask_ratio)
                if mask_len > 0:
                    start_idx = random.randint(0, seq_len - mask_len)
                    rgb_feat[start_idx:start_idx + mask_len, :] *= random.uniform(0.1, 0.3)
                    skeleton_feat[start_idx:start_idx + mask_len, :] *= random.uniform(0.1, 0.3)
                    
            elif method == 'feature_scaling':
                # è¼ƒå¼·çš„ç‰¹å¾µç¸®æ”¾
                scale = random.uniform(0.85, 1.15)
                rgb_feat = rgb_feat * scale
                
            elif method == 'temporal_shift':
                # è¼ƒå¤§çš„æ™‚é–“åç§»
                shift = random.randint(-5, 5)
                if shift != 0:
                    rgb_feat = np.roll(rgb_feat, shift, axis=0)
                    skeleton_feat = np.roll(skeleton_feat, shift, axis=0)
                    
            elif method == 'speed_perturbation':
                # é€Ÿåº¦æ“¾å‹•ï¼ˆæ™‚é–“æ‹‰ä¼¸/å£“ç¸®ï¼‰
                speed_factor = random.uniform(0.9, 1.1)
                new_len = int(len(rgb_feat) * speed_factor)
                if new_len > 0:
                    indices = np.linspace(0, len(rgb_feat) - 1, new_len).astype(int)
                    rgb_feat = rgb_feat[indices]
                    skeleton_feat = skeleton_feat[indices]
                    
            elif method == 'feature_dropout':
                # ç‰¹å¾µé€šé“éš¨æ©Ÿdropout
                dropout_ratio = random.uniform(0.05, 0.15)
                mask = np.random.binomial(1, 1 - dropout_ratio, rgb_feat.shape[1])
                rgb_feat = rgb_feat * mask
        
        return rgb_feat, skeleton_feat
    
    def _load_and_process(self, rgb_path, skeleton_path, label, should_augment, is_training, mixup_data=None):
        """
        è¼‰å…¥ä¸¦è™•ç†å–®å€‹æ¨£æœ¬ï¼ˆæ”¯æŒå…©å±¤å¢å¼·ç­–ç•¥ + Mixupï¼‰
        
        Args:
            should_augment: æ˜¯å¦ç‚ºé¡åˆ¥å¹³è¡¡çš„å¢å¼·æ¨£æœ¬ï¼ˆä½¿ç”¨å¼·å¢å¼·ï¼‰
            is_training: æ˜¯å¦ç‚ºè¨“ç·´æ¨¡å¼ï¼ˆè¨“ç·´æ™‚æ‰€æœ‰æ¨£æœ¬éƒ½æ‡‰ç”¨è¼•å¢å¼·ï¼‰
            mixup_data: Mixup æ•¸æ“š (rgb_path2, skeleton_path2, label2, lambda) æˆ– None
        """
        try:
            # è¼‰å…¥ç‰¹å¾µ
            rgb_feat = np.load(rgb_path.numpy().decode()).astype(np.float32)
            skeleton_feat = np.load(skeleton_path.numpy().decode()).astype(np.float32)
            
            # é©—è­‰ç¶­åº¦
            if rgb_feat.shape[1] != 960 or skeleton_feat.shape[1] != 159:
                raise ValueError(f"ç‰¹å¾µç¶­åº¦éŒ¯èª¤: RGB {rgb_feat.shape}, Skeleton {skeleton_feat.shape}")
            
            # å°é½Šé•·åº¦
            min_len = min(len(rgb_feat), len(skeleton_feat))
            rgb_feat = rgb_feat[:min_len]
            skeleton_feat = skeleton_feat[:min_len]
            
            # å…©å±¤å¢å¼·ç­–ç•¥ï¼š
            # 1. è¨“ç·´æ™‚ï¼Œæ‰€æœ‰åŸå§‹æ¨£æœ¬æ‡‰ç”¨è¼•é‡å¢å¼·ï¼ˆ80%æ¦‚ç‡ï¼‰
            # 2. é¡åˆ¥å¹³è¡¡çš„å¢å¼·æ¨£æœ¬æ‡‰ç”¨å¼·åŠ›å¢å¼·ï¼ˆ100%ï¼‰
            
            if should_augment.numpy():
                # é¡åˆ¥å¹³è¡¡æ¨£æœ¬ï¼šå¼·åŠ›å¢å¼·
                rgb_feat, skeleton_feat = self._apply_strong_augmentation(rgb_feat, skeleton_feat)
            elif is_training.numpy() and random.random() < 0.8:
                # åŸå§‹è¨“ç·´æ¨£æœ¬ï¼š80%æ¦‚ç‡æ‡‰ç”¨è¼•é‡å¢å¼·
                rgb_feat, skeleton_feat = self._apply_light_augmentation(rgb_feat, skeleton_feat)
            
            # Concat èåˆ (960 + 159 = 1119)
            concat_feat = np.concatenate([rgb_feat, skeleton_feat], axis=1)
            
            # Padding æˆ–æˆªæ–·åˆ°å›ºå®šé•·åº¦
            if len(concat_feat) < self.max_length:
                padding = np.zeros((self.max_length - len(concat_feat), 1119), dtype=np.float32)
                concat_feat = np.concatenate([concat_feat, padding], axis=0)
            else:
                concat_feat = concat_feat[:self.max_length]
            
            # Mixupï¼ˆå¦‚æœæä¾›äº†æ··åˆæ•¸æ“šï¼‰
            if mixup_data is not None:
                try:
                    # è§£æ mixup_dataï¼ˆåƒ…åœ¨è¨“ç·´æ™‚ä½¿ç”¨ï¼‰
                    rgb_path2, skeleton_path2, label2, lam = mixup_data
                    
                    # è¼‰å…¥ç¬¬äºŒå€‹æ¨£æœ¬
                    rgb_feat2 = np.load(rgb_path2.decode()).astype(np.float32)
                    skeleton_feat2 = np.load(skeleton_path2.decode()).astype(np.float32)
                    
                    min_len2 = min(len(rgb_feat2), len(skeleton_feat2))
                    rgb_feat2 = rgb_feat2[:min_len2]
                    skeleton_feat2 = skeleton_feat2[:min_len2]
                    
                    concat_feat2 = np.concatenate([rgb_feat2, skeleton_feat2], axis=1)
                    
                    # Padding
                    if len(concat_feat2) < self.max_length:
                        padding2 = np.zeros((self.max_length - len(concat_feat2), 1119), dtype=np.float32)
                        concat_feat2 = np.concatenate([concat_feat2, padding2], axis=0)
                    else:
                        concat_feat2 = concat_feat2[:self.max_length]
                    
                    # æ··åˆç‰¹å¾µ
                    concat_feat = lam * concat_feat + (1 - lam) * concat_feat2
                    
                    # æ³¨æ„ï¼šMixup çš„è»Ÿæ¨™ç±¤è™•ç†éœ€è¦åœ¨ loss ä¸­å¯¦ç¾
                    # é€™è£¡è¿”å›åŸå§‹æ¨™ç±¤ï¼Œä½†ç‰¹å¾µå·²ç¶“æ··åˆ
                    
                except Exception as e:
                    # Mixup å¤±æ•—æ™‚ä½¿ç”¨åŸå§‹ç‰¹å¾µ
                    pass
            
            return concat_feat, np.int32(label)
        
        except Exception as e:
            print(f"âš ï¸  è¼‰å…¥å¤±æ•—: {e}")
            return np.zeros((self.max_length, 1119), dtype=np.float32), np.int32(0)
    
    def create_tf_dataset(self, batch_size, val_split=0.2, shuffle=True):
        """
        å‰µå»º TensorFlow Datasetï¼ˆTPU å„ªåŒ–ï¼‰
        
        Args:
            batch_size: å…¨å±€æ‰¹æ¬¡å¤§å°ï¼ˆå·²ç¶“è€ƒæ…®äº† TPU æ ¸å¿ƒæ•¸ï¼‰
            val_split: é©—è­‰é›†æ¯”ä¾‹
            shuffle: æ˜¯å¦æ‰“äº‚
        
        Returns:
            train_dataset, val_dataset
        """
        # åˆ†å±¤åˆ†å‰²ï¼ˆæŒ‰é¡åˆ¥ï¼‰
        train_samples, val_samples = [], []
        
        for word_name in self.label_map.keys():
            word_samples = [s for s in self.samples if s['word'] == word_name]
            
            if shuffle:
                np.random.shuffle(word_samples)
            
            val_size = max(1, int(len(word_samples) * val_split))
            train_samples.extend(word_samples[:-val_size])
            val_samples.extend(word_samples[-val_size:])
        
        if shuffle:
            np.random.shuffle(train_samples)
            np.random.shuffle(val_samples)
        
        print(f"âœ‚ï¸  è¨“ç·´é›†: {len(train_samples)} æ¨£æœ¬")
        print(f"âœ‚ï¸  é©—è­‰é›†: {len(val_samples)} æ¨£æœ¬")
        
        # å‰µå»ºæ•¸æ“šé›†
        train_dataset = self._create_dataset(train_samples, batch_size, shuffle=True, augment=True, is_training=True)
        val_dataset = self._create_dataset(val_samples, batch_size, shuffle=False, augment=False, is_training=False)
        
        return train_dataset, val_dataset
    
    def _create_dataset(self, samples, batch_size, shuffle, augment, is_training=False):
        """
        å‰µå»ºå–®å€‹ TF Datasetï¼ˆTPU å„ªåŒ–ç‰ˆæœ¬ + å…§å­˜ç·©å­˜ + å…©å±¤å¢å¼· + Mixupï¼‰
        
        Mixup åœ¨ batch ç´šåˆ¥å¯¦ç¾ï¼š
        1. æ­£å¸¸è¼‰å…¥ batch
        2. åœ¨ batch å…§éƒ¨é€²è¡Œéš¨æ©Ÿ Mixup
        3. ç”Ÿæˆè»Ÿæ¨™ç±¤
        """
        # æå–è·¯å¾‘ã€æ¨™ç±¤å’Œå¢å¼·æ¨™è¨˜
        rgb_paths = [s['rgb_path'] for s in samples]
        skeleton_paths = [s['skeleton_path'] for s in samples]
        labels = [s['label'] for s in samples]
        # æª¢æŸ¥æ˜¯å¦ç‚ºé¡åˆ¥å¹³è¡¡çš„å¢å¼·æ¨£æœ¬ï¼ˆå¼·å¢å¼·ï¼‰
        should_augment = [augment and s.get('augment', False) for s in samples]
        # æ¨™è¨˜æ˜¯å¦ç‚ºè¨“ç·´æ¨¡å¼ï¼ˆåŸå§‹æ¨£æœ¬ç”¨è¼•å¢å¼·ï¼‰
        is_training_flags = [is_training] * len(samples)
        
        # å‰µå»º Dataset
        dataset = tf.data.Dataset.from_tensor_slices((
            rgb_paths, skeleton_paths, labels, should_augment, is_training_flags
        ))
        
        # ä¸¦è¡Œè¼‰å…¥æ•¸æ“š
        def load_wrapper(rgb_path, skeleton_path, label, should_aug, is_train):
            # ä¸ä½¿ç”¨æ¨£æœ¬ç´š Mixupï¼ˆæ”¹ç”¨ batch ç´šï¼‰
            features, label_out = tf.py_function(
                func=lambda rp, sp, l, sa, it: self._load_and_process(rp, sp, l, sa, it, mixup_data=None),
                inp=[rgb_path, skeleton_path, label, should_aug, is_train],
                Tout=(tf.float32, tf.int32)
            )
            features.set_shape([self.max_length, 1119])
            label_out.set_shape([])
            return features, label_out
        
        dataset = dataset.map(
            load_wrapper,
            num_parallel_calls=tf.data.AUTOTUNE,
            deterministic=False
        )
        
        # TPU å„ªåŒ–ï¼šç·©å­˜åˆ°å…§å­˜ï¼ˆè¨“ç·´é›†å’Œé©—è­‰é›†éƒ½ç·©å­˜ï¼‰
        # æ³¨æ„ï¼šç”±æ–¼å¢å¼·æ˜¯å‹•æ…‹çš„ï¼Œè¨“ç·´é›†ä¸ç·©å­˜ï¼Œé©—è­‰é›†å¯ä»¥ç·©å­˜
        if not is_training:
            dataset = dataset.cache()
            print(f"   âœ… é©—è­‰é›†æ•¸æ“šå·²ç·©å­˜åˆ°å…§å­˜ ({len(samples)} æ¨£æœ¬)")
        else:
            print(f"   â„¹ï¸  è¨“ç·´é›†ä½¿ç”¨å‹•æ…‹å¢å¼·ï¼Œä¸ç·©å­˜ ({len(samples)} æ¨£æœ¬)")
        
        # è¨“ç·´é›†æ‰é€²è¡Œ shuffleï¼ˆåœ¨ cache ä¹‹å¾Œï¼‰
        if shuffle:
            dataset = dataset.shuffle(buffer_size=2000, reshuffle_each_iteration=True)
        
        # TPU å„ªåŒ–ï¼šdrop_remainder=True ç¢ºä¿æ‰¹æ¬¡å¤§å°å›ºå®š
        dataset = dataset.batch(batch_size, drop_remainder=True)
        
        # â­ Mixup å¢å¼·ï¼ˆbatch ç´šåˆ¥ï¼Œåƒ…è¨“ç·´é›†ï¼‰
        if is_training and self.use_mixup:
            print(f"   ğŸ¨ å•Ÿç”¨ Batch-level Mixup (alpha={self.mixup_alpha})")
            
            def apply_mixup_batch(features, labels):
                """åœ¨ batch å…§éƒ¨æ‡‰ç”¨ Mixup"""
                import keras.ops as ops
                
                batch_size = ops.shape(features)[0]
                
                # ç”Ÿæˆéš¨æ©Ÿæ’åˆ—ç´¢å¼•ï¼ˆç”¨æ–¼é…å°æ¨£æœ¬ï¼‰
                indices = tf.random.shuffle(tf.range(batch_size))
                
                # å¾ Beta åˆ†ä½ˆæ¡æ¨£ lambda
                lam = tf.random.uniform([], minval=0.0, maxval=1.0)
                # ä½¿ç”¨ç°¡åŒ–çš„æ··åˆæ¯”ä¾‹ï¼ˆé¿å… Beta åˆ†ä½ˆçš„è¤‡é›œæ€§ï¼‰
                if self.mixup_alpha > 0:
                    lam = tf.maximum(lam, 1.0 - lam)  # å‚¾å‘æ–¼ 0.5 é™„è¿‘
                
                # æ··åˆç‰¹å¾µ
                mixed_features = lam * features + (1.0 - lam) * tf.gather(features, indices)
                
                # æ··åˆæ¨™ç±¤ï¼ˆç”Ÿæˆè»Ÿæ¨™ç±¤ï¼‰
                labels_a = tf.one_hot(labels, depth=self.num_classes)
                labels_b = tf.one_hot(tf.gather(labels, indices), depth=self.num_classes)
                mixed_labels = lam * labels_a + (1.0 - lam) * labels_b
                
                return mixed_features, mixed_labels
            
            dataset = dataset.map(
                apply_mixup_batch,
                num_parallel_calls=tf.data.AUTOTUNE
            )
        
        # TPU å„ªåŒ–ï¼šprefetch é è¼‰ä¸‹ä¸€æ‰¹æ•¸æ“š
        dataset = dataset.prefetch(tf.data.AUTOTUNE)
        
        return dataset


# ==================== è‡ªå®šç¾© Metricsï¼ˆæ”¯æŒè»Ÿæ¨™ç±¤ï¼‰====================
class MixupAccuracy(keras.metrics.Metric):
    """æ”¯æŒ Mixup è»Ÿæ¨™ç±¤çš„æº–ç¢ºç‡è¨ˆç®—"""
    
    def __init__(self, name='mixup_accuracy', **kwargs):
        super().__init__(name=name, **kwargs)
        self.total = self.add_weight(name='total', initializer='zeros')
        self.count = self.add_weight(name='count', initializer='zeros')
    
    def update_state(self, y_true, y_pred, sample_weight=None):
        import keras.ops as ops
        
        # è™•ç†ç¡¬æ¨™ç±¤æˆ–è»Ÿæ¨™ç±¤
        if len(ops.shape(y_true)) == 1:
            # ç¡¬æ¨™ç±¤
            y_true_labels = y_true
        else:
            # è»Ÿæ¨™ç±¤ï¼ˆMixupï¼‰ï¼šå–æœ€å¤§æ¦‚ç‡çš„é¡åˆ¥
            y_true_labels = ops.argmax(y_true, axis=-1)
        
        # é æ¸¬æ¨™ç±¤
        y_pred_labels = ops.argmax(y_pred, axis=-1)
        
        # è¨ˆç®—æº–ç¢ºç‡
        matches = ops.cast(ops.equal(y_true_labels, y_pred_labels), 'float32')
        
        self.total.assign_add(ops.sum(matches))
        self.count.assign_add(ops.cast(ops.size(matches), 'float32'))
    
    def result(self):
        return self.total / self.count
    
    def reset_state(self):
        self.total.assign(0.0)
        self.count.assign(0.0)


class MixupTop3Accuracy(keras.metrics.Metric):
    """æ”¯æŒ Mixup è»Ÿæ¨™ç±¤çš„ Top-3 æº–ç¢ºç‡è¨ˆç®—"""
    
    def __init__(self, name='mixup_top3_accuracy', **kwargs):
        super().__init__(name=name, **kwargs)
        self.total = self.add_weight(name='total', initializer='zeros')
        self.count = self.add_weight(name='count', initializer='zeros')
    
    def update_state(self, y_true, y_pred, sample_weight=None):
        import keras.ops as ops
        
        # è™•ç†ç¡¬æ¨™ç±¤æˆ–è»Ÿæ¨™ç±¤
        if len(ops.shape(y_true)) == 1:
            # ç¡¬æ¨™ç±¤
            y_true_labels = y_true
        else:
            # è»Ÿæ¨™ç±¤ï¼ˆMixupï¼‰ï¼šå–æœ€å¤§æ¦‚ç‡çš„é¡åˆ¥
            y_true_labels = ops.argmax(y_true, axis=-1)
        
        # ç²å– Top-3 é æ¸¬
        top3_pred = ops.top_k(y_pred, k=3)[1]  # è¿”å›ç´¢å¼•
        
        # æª¢æŸ¥çœŸå¯¦æ¨™ç±¤æ˜¯å¦åœ¨ Top-3 ä¸­
        y_true_expanded = ops.expand_dims(y_true_labels, axis=-1)
        matches = ops.any(ops.equal(top3_pred, y_true_expanded), axis=-1)
        matches = ops.cast(matches, 'float32')
        
        self.total.assign_add(ops.sum(matches))
        self.count.assign_add(ops.cast(ops.size(matches), 'float32'))
    
    def result(self):
        return self.total / self.count
    
    def reset_state(self):
        self.total.assign(0.0)
        self.count.assign(0.0)


# ==================== è‡ªå®šç¾©æå¤±å‡½æ•¸ ====================
class FocalLoss(keras.losses.Loss):
    """
    Focal Loss - è§£æ±ºé¡åˆ¥ä¸å¹³è¡¡å’Œéåº¦è‡ªä¿¡å•é¡Œï¼ˆJAX å…¼å®¹ç‰ˆæœ¬ï¼‰
    
    Focal Loss é€šéé™ä½æ˜“åˆ†é¡æ¨£æœ¬çš„æ¬Šé‡ï¼Œå¼·åˆ¶æ¨¡å‹é—œæ³¨é›£åˆ†é¡æ¨£æœ¬ï¼Œ
    æœ‰æ•ˆå°æŠ—éåº¦è‡ªä¿¡å’Œç‰¹å¾µæ··ç–Šã€‚
    
    è«–æ–‡ï¼šFocal Loss for Dense Object Detection (Lin et al., 2017)
    
    å…¬å¼ï¼šFL(p_t) = -Î±_t * (1 - p_t)^Î³ * log(p_t)
    
    Args:
        gamma: èª¿ç¯€å› å­ï¼Œæ§åˆ¶æ˜“åˆ†é¡æ¨£æœ¬çš„æ¬Šé‡è¡°æ¸›é€Ÿåº¦
               gamma=0 é€€åŒ–ç‚ºæ¨™æº–äº¤å‰ç†µ
               gamma=2 æ˜¯è«–æ–‡æ¨è–¦å€¼
        alpha: é¡åˆ¥æ¬Šé‡ï¼ˆå¯é¸ï¼‰
        label_smoothing: æ¨™ç±¤å¹³æ»‘å› å­
        num_classes: é¡åˆ¥æ•¸é‡ï¼ˆå¿…é ˆåœ¨åˆå§‹åŒ–æ™‚æä¾›ï¼Œé¿å… JAX tracer éŒ¯èª¤ï¼‰
    """
    
    def __init__(self, num_classes, gamma=2.0, alpha=None, label_smoothing=0.1, name='focal_loss'):
        super().__init__(name=name)
        self.num_classes = num_classes
        self.gamma = gamma
        self.alpha = alpha
        self.label_smoothing = label_smoothing
    
    def call(self, y_true, y_pred):
        """
        è¨ˆç®— Focal Lossï¼ˆæ”¯æŒç¡¬æ¨™ç±¤å’Œè»Ÿæ¨™ç±¤ï¼‰
        
        Args:
            y_true: çœŸå¯¦æ¨™ç±¤ (batch_size,) æˆ– one-hot/è»Ÿæ¨™ç±¤ (batch_size, num_classes)
            y_pred: é æ¸¬æ¦‚ç‡ (batch_size, num_classes)
        """
        import keras.ops as ops
        
        # æª¢æŸ¥ y_true æ˜¯å¦ç‚ºè»Ÿæ¨™ç±¤ï¼ˆMixupï¼‰æˆ–ç¡¬æ¨™ç±¤
        if len(ops.shape(y_true)) == 1:
            # ç¡¬æ¨™ç±¤ï¼šè½‰æ›ç‚º one-hot
            y_true_one_hot = ops.one_hot(ops.cast(y_true, 'int32'), self.num_classes)
            
            # Label smoothing
            if self.label_smoothing > 0:
                y_true_one_hot = y_true_one_hot * (1.0 - self.label_smoothing) + \
                                self.label_smoothing / float(self.num_classes)
        else:
            # è»Ÿæ¨™ç±¤ï¼ˆMixupï¼‰ï¼šç›´æ¥ä½¿ç”¨
            y_true_one_hot = y_true
        
        # é¿å… log(0)
        epsilon = 1e-7
        y_pred = ops.clip(y_pred, epsilon, 1.0 - epsilon)
        
        # è¨ˆç®—äº¤å‰ç†µ
        cross_entropy = -y_true_one_hot * ops.log(y_pred)
        
        # è¨ˆç®— Focal æ¬Šé‡ï¼š(1 - p_t)^gamma
        # p_t æ˜¯æ­£ç¢ºé¡åˆ¥çš„é æ¸¬æ¦‚ç‡
        p_t = ops.sum(y_true_one_hot * y_pred, axis=-1, keepdims=True)
        focal_weight = ops.power(1.0 - p_t, self.gamma)
        
        # æ‡‰ç”¨ Focal æ¬Šé‡
        focal_cross_entropy = focal_weight * cross_entropy
        
        # å¯é¸çš„é¡åˆ¥æ¬Šé‡
        if self.alpha is not None:
            alpha_weight = y_true_one_hot * self.alpha
            focal_cross_entropy = alpha_weight * focal_cross_entropy
        
        # è¿”å›å¹³å‡æå¤±
        return ops.mean(ops.sum(focal_cross_entropy, axis=-1))


# ==================== æ¨¡å‹æ§‹å»º ====================
def build_bigru_model(input_shape, num_classes, gru_units=256, dropout=0.4, use_temperature_scaling=True):
    """
    æ§‹å»º Bi-GRU æ¨¡å‹ï¼ˆKeras 3.0 - è§£æ±ºç‰¹å¾µæ··ç–Šèˆ‡éåº¦è‡ªä¿¡ï¼‰
    
    é‡å°æ€§æ”¹é€²ï¼š
    1. ã€ç‰¹å¾µæ··ç–Šã€‘æ·»åŠ ä¸­å¿ƒæå¤±æ­£å‰‡åŒ–çš„æº–å‚™ï¼ˆembedding å±¤ï¼‰
    2. ã€Distribution Shiftã€‘å¤šå°ºåº¦ç‰¹å¾µèåˆï¼Œå¢å¼·é­¯æ£’æ€§
    3. ã€Calibrationã€‘æº«åº¦ç¸®æ”¾é è™•ç† + æ›´å¼·çš„ dropout
    4. ã€Logit Varianceã€‘é™åˆ¶è¼¸å‡ºå±¤æ¬Šé‡ç¯„æ•¸ï¼Œé˜²æ­¢æ¥µç«¯ logits
    
    Args:
        input_shape: (max_length, feature_dim) - (300, 1119)
        num_classes: é¡åˆ¥æ•¸é‡
        gru_units: GRU éš±è—å–®å…ƒæ•¸
        dropout: Dropout æ¯”ä¾‹
        use_temperature_scaling: æ˜¯å¦åœ¨è¼¸å‡ºå‰æ‡‰ç”¨æº«åº¦ç¸®æ”¾å±¤
    """
    print("ğŸ—ï¸  æ§‹å»º Bi-GRU æ¨¡å‹ï¼ˆè§£æ±ºç‰¹å¾µæ··ç–Š & éåº¦è‡ªä¿¡ï¼‰...")
    
    sequence_input = keras.Input(shape=input_shape, name='sequence_input')
    
    # Masking å±¤
    x = keras.layers.Masking(mask_value=0.0)(sequence_input)
    
    # è¼¸å…¥å±¤æ­¸ä¸€åŒ–ï¼ˆå°æŠ— Distribution Shiftï¼‰
    x = keras.layers.LayerNormalization(name='input_norm')(x)
    
    # ç¬¬ä¸€å±¤ Bi-GRU
    gru1_output = keras.layers.Bidirectional(
        keras.layers.GRU(
            units=gru_units,
            return_sequences=True,
            dropout=dropout,
            recurrent_dropout=0.3,
            kernel_regularizer=keras.regularizers.l2(2e-5),
            recurrent_regularizer=keras.regularizers.l2(2e-5),
            name='gru_1'
        ),
        name='bidirectional_gru_1'
    )(x)
    
    # æ·»åŠ  LayerNormalization
    gru1_output = keras.layers.LayerNormalization(name='layer_norm_1')(gru1_output)
    
    # ç¬¬äºŒå±¤ Bi-GRUï¼ˆè¿”å›åºåˆ—å’Œæœ€çµ‚ç‹€æ…‹ï¼Œç”¨æ–¼å¤šå°ºåº¦èåˆï¼‰
    gru2_output = keras.layers.Bidirectional(
        keras.layers.GRU(
            units=gru_units // 2,
            return_sequences=True,  # æ”¹ç‚º Trueï¼Œæå–å¤šå°ºåº¦ç‰¹å¾µ
            dropout=dropout,
            recurrent_dropout=0.3,
            kernel_regularizer=keras.regularizers.l2(2e-5),
            recurrent_regularizer=keras.regularizers.l2(2e-5),
            name='gru_2'
        ),
        name='bidirectional_gru_2'
    )(gru1_output)
    
    # ã€å°æŠ—ç‰¹å¾µæ··ç–Šã€‘å¤šå°ºåº¦æ™‚åºæ± åŒ–
    # ä¸åŒæ™‚é–“çª—å£çš„ç‰¹å¾µï¼Œé˜²æ­¢å–®ä¸€è¦–è§’çš„æ··ç–Š
    gru2_max = keras.layers.GlobalMaxPooling1D(name='global_max_pool')(gru2_output)
    gru2_avg = keras.layers.GlobalAveragePooling1D(name='global_avg_pool')(gru2_output)
    
    # æ‹¼æ¥å¤šå°ºåº¦ç‰¹å¾µ
    x = keras.layers.Concatenate(name='multi_scale_concat')([gru2_max, gru2_avg])
    
    # ã€Embedding å±¤ã€‘ç”¨æ–¼å¾ŒçºŒå¯è¦–åŒ–å’Œä¸­å¿ƒæå¤±ï¼ˆå¯é¸ï¼‰
    # é€™å±¤ç‰¹å¾µæ‡‰è©²æœ‰æ˜ç¢ºçš„é¡é–“åˆ†é›¢
    embedding = keras.layers.Dense(
        256,
        kernel_regularizer=keras.regularizers.l2(3e-5),  # æ›´å¼·çš„æ­£å‰‡åŒ–
        kernel_constraint=keras.constraints.MaxNorm(3.0),  # é™åˆ¶æ¬Šé‡ç¯„æ•¸
        name='embedding'
    )(x)
    embedding = keras.layers.BatchNormalization(name='embedding_bn')(embedding)
    embedding = keras.layers.Activation('relu', name='embedding_activation')(embedding)
    embedding = keras.layers.Dropout(dropout, name='embedding_dropout')(embedding)
    
    # ã€åˆ†é¡é ­ã€‘è¼ƒå°çš„ç“¶é ¸å±¤ï¼Œé˜²æ­¢éåº¦æ“¬åˆ
    x = keras.layers.Dense(
        128,
        kernel_regularizer=keras.regularizers.l2(3e-5),
        kernel_constraint=keras.constraints.MaxNorm(2.0),  # é™åˆ¶æ¬Šé‡ï¼Œé˜²æ­¢æ¥µç«¯ logits
        name='classifier_hidden'
    )(embedding)
    x = keras.layers.BatchNormalization(name='classifier_bn')(x)
    x = keras.layers.Activation('relu', name='classifier_activation')(x)
    x = keras.layers.Dropout(dropout * 0.6, name='classifier_dropout')(x)
    
    # ã€è¼¸å‡ºå±¤ã€‘é™åˆ¶æ¬Šé‡ç¯„æ•¸ï¼Œé˜²æ­¢ logit variance éå¤§
    logits = keras.layers.Dense(
        num_classes,
        kernel_regularizer=keras.regularizers.l2(3e-5),
        kernel_constraint=keras.constraints.MaxNorm(1.5),  # â­ é—œéµï¼šé™åˆ¶ logits å¹…åº¦
        use_bias=True,
        dtype='float32',
        name='logits'
    )(x)
    
    # ã€Calibrationã€‘å¯é¸çš„æº«åº¦ç¸®æ”¾å±¤ï¼ˆé€šé bias å¯¦ç¾æº«åº¦æ•ˆæœï¼‰
    # æ³¨æ„ï¼šçœŸæ­£çš„æº«åº¦ç¸®æ”¾éœ€è¦åœ¨è¨“ç·´å¾Œèª¿æ•´ï¼Œé€™è£¡å…ˆé ç•™æ¶æ§‹
    if use_temperature_scaling:
        # æ·»åŠ ä¸€å€‹å¯å­¸ç¿’çš„ç¸®æ”¾å› å­ï¼ˆåˆå§‹åŒ–ç‚ºæ¥è¿‘ 2.0ï¼Œé™ä½ä¿¡å¿ƒåº¦ï¼‰
        import keras.backend as K
        
        # ä½¿ç”¨ Lambda å±¤å¯¦ç¾æº«åº¦ç¸®æ”¾çš„æº–å‚™ï¼ˆåˆæœŸé™¤ä»¥è¼ƒå¤§çš„æº«åº¦ï¼‰
        # è¨“ç·´æ™‚æœƒè‡ªå‹•èª¿æ•´
        logits = keras.layers.Lambda(
            lambda x: x / 1.5,  # åˆå§‹æº«åº¦ 1.5ï¼Œé™ä½éåº¦è‡ªä¿¡
            name='temperature_scaling'
        )(logits)
    
    # Softmax æ¿€æ´»
    outputs = keras.layers.Activation('softmax', dtype='float32', name='output')(logits)
    
    model = keras.Model(inputs=sequence_input, outputs=outputs, name='BiGRU_AntiCollapse_Calibrated')
    
    return model


# ==================== è¨“ç·´å‡½æ•¸ ====================
def train_model(
    rgb_dir,
    skeleton_dir,
    output_dir,
    strategy,
    max_length=300,
    batch_size_per_replica=64,  # æ¯å€‹ TPU æ ¸å¿ƒçš„æ‰¹æ¬¡å¤§å°ï¼ˆé™ä½ä»¥ç¯€çœå…§å­˜ï¼‰
    epochs=50,
    learning_rate=5e-4,
    val_split=0.2
):
    """
    è¨“ç·´æ¨¡å‹ï¼ˆTPU å„ªåŒ–ï¼‰
    
    Args:
        rgb_dir: RGB ç‰¹å¾µç›®éŒ„
        skeleton_dir: Skeleton ç‰¹å¾µç›®éŒ„
        output_dir: è¼¸å‡ºç›®éŒ„
        strategy: TPUStrategy
        batch_size_per_replica: æ¯å€‹ TPU æ ¸å¿ƒçš„æ‰¹æ¬¡å¤§å°
        epochs: è¨“ç·´è¼ªæ•¸
        learning_rate: å­¸ç¿’ç‡
        val_split: é©—è­‰é›†æ¯”ä¾‹
    """
    output_path = Path(output_dir)
    output_path.mkdir(exist_ok=True, parents=True)
    
    # è¨ˆç®—å…¨å±€æ‰¹æ¬¡å¤§å°
    global_batch_size = batch_size_per_replica * strategy.num_replicas_in_sync
    print(f"\nğŸ“¦ å…¨å±€æ‰¹æ¬¡å¤§å°: {global_batch_size} ({batch_size_per_replica} Ã— {strategy.num_replicas_in_sync} æ ¸å¿ƒ)")
    
    # è¼‰å…¥æ•¸æ“šé›†
    print("\n" + "=" * 60)
    dataset = SignLanguageDataset(
        rgb_dir, 
        skeleton_dir, 
        max_length=max_length,
        use_augmentation=True,  # å•Ÿç”¨å»é‡å¾Œçš„æ•¸æ“šå¢å¼·
        use_mixup=True,         # â­ å•Ÿç”¨ Mixupï¼ˆå°æŠ—ç‰¹å¾µæ··ç–Šï¼‰
        mixup_alpha=0.2
    )
    
    # å‰µå»º TF Dataset
    print("\n" + "=" * 60)
    train_dataset, val_dataset = dataset.create_tf_dataset(
        batch_size=global_batch_size,
        val_split=val_split,
        shuffle=True
    )
    
    # é©—è­‰æ•¸æ“šæ ¼å¼
    print("\nğŸ” é©—è­‰æ•¸æ“šé›†æ ¼å¼...")
    for features, labels in train_dataset.take(1):
        print(f"âœ… Features shape: {features.shape}")
        print(f"âœ… Labels shape: {labels.shape}")
        print(f"âœ… Features range: [{features.numpy().min():.3f}, {features.numpy().max():.3f}]")
        break
    
    # æ§‹å»ºå’Œç·¨è­¯æ¨¡å‹ï¼ˆKeras JAX backend è‡ªå‹•è™•ç†åˆ†ä½ˆå¼ï¼‰
    print("\n" + "=" * 60)
    model = build_bigru_model(
        input_shape=(max_length, 1119),
        num_classes=dataset.num_classes,
        gru_units=256,
        dropout=0.4  # å¢åŠ  dropout
    )
    
    # ç·¨è­¯æ¨¡å‹ï¼ˆä½¿ç”¨ Focal Loss + Label Smoothing + Mixup æ”¯æŒï¼‰
    print("\nâš™ï¸  ç·¨è­¯æ¨¡å‹...")
    print("   ğŸ¯ å•Ÿç”¨ Focal Loss (gamma=2.0) å°æŠ—éåº¦è‡ªä¿¡èˆ‡ç‰¹å¾µæ··ç–Š")
    print("   ğŸ¯ å•Ÿç”¨ Label Smoothing (0.1) é™ä½æ¨¡å‹éåº¦è‡ªä¿¡")
    print("   ğŸ¯ å•Ÿç”¨æº«åº¦ç¸®æ”¾ (T=1.5) æ ¡æº–ä¿¡å¿ƒåº¦åˆ†å¸ƒ")
    if dataset.use_mixup:
        print("   ğŸ¨ å•Ÿç”¨ Mixup (alpha=0.2) å°æŠ—ç‰¹å¾µæ··ç–Š")
    
    # ä½¿ç”¨ Focal Loss æ›¿ä»£æ¨™æº–äº¤å‰ç†µ
    focal_loss = FocalLoss(
        num_classes=dataset.num_classes,  # JAX è¦æ±‚éœæ…‹å½¢ç‹€
        gamma=2.0,           # èšç„¦é›£åˆ†é¡æ¨£æœ¬
        alpha=None,          # ä¸ä½¿ç”¨é¡åˆ¥æ¬Šé‡ï¼ˆå·²ç¶“ç”¨æ•¸æ“šå¢å¼·å¹³è¡¡ï¼‰
        label_smoothing=0.1  # æ¨™ç±¤å¹³æ»‘
    )
    
    # é¸æ“‡åˆé©çš„ metricsï¼ˆæ ¹æ“šæ˜¯å¦ä½¿ç”¨ Mixupï¼‰
    if dataset.use_mixup:
        metrics_list = [
            MixupAccuracy(name='accuracy'),
            MixupTop3Accuracy(name='top3_accuracy')
        ]
    else:
        metrics_list = [
            keras.metrics.SparseCategoricalAccuracy(name='accuracy'),
            keras.metrics.SparseTopKCategoricalAccuracy(k=3, name='top3_accuracy')
        ]
    
    model.compile(
        optimizer=keras.optimizers.Adam(
            learning_rate=learning_rate,
            clipnorm=1.0  # æ¢¯åº¦è£å‰ªï¼Œé˜²æ­¢è¨“ç·´ä¸ç©©å®š
        ),
        loss=focal_loss,
        metrics=metrics_list
    )
    
    # è¨­ç½® Callbacks
    callbacks = [
        keras.callbacks.ModelCheckpoint(
            filepath=str(output_path / 'best_model.keras'),
            monitor='val_accuracy',
            mode='max',
            save_best_only=True,
            verbose=1
        ),
        keras.callbacks.EarlyStopping(
            monitor='val_loss',
            patience=15,
            min_delta=1e-4,
            verbose=1,
            restore_best_weights=True
        ),
        keras.callbacks.ReduceLROnPlateau(
            monitor='val_loss',
            factor=0.5,
            patience=8,
            min_lr=1e-6,
            verbose=1
        ),
        keras.callbacks.CSVLogger(
            filename=str(output_path / 'training_log.csv'),
            separator=',',
            append=False
        ),
        keras.callbacks.TensorBoard(
            log_dir=str(output_path / 'tensorboard_logs'),
            histogram_freq=1
        )
    ]
    
    # é–‹å§‹è¨“ç·´
    print("\n" + "=" * 60)
    print("ğŸš€ é–‹å§‹è¨“ç·´...")
    print(f"ğŸ“Š é…ç½®:")
    print(f"  - å…¨å±€æ‰¹æ¬¡å¤§å°: {global_batch_size}")
    print(f"  - è¨“ç·´è¼ªæ•¸: {epochs}")
    print(f"  - å­¸ç¿’ç‡: {learning_rate}")
    print(f"  - TPU æ ¸å¿ƒ: {strategy.num_replicas_in_sync}")
    print("=" * 60)
    
    start_time = datetime.now()
    
    try:
        history = model.fit(
            train_dataset,
            validation_data=val_dataset,
            epochs=epochs,
            callbacks=callbacks,
            verbose=1
        )
    except Exception as e:
        print(f"\nâŒ è¨“ç·´éŒ¯èª¤: {e}")
        import traceback
        traceback.print_exc()
        return
    
    training_time = datetime.now() - start_time
    
    # ä¿å­˜æ¨¡å‹å’Œçµæœ
    print("\n" + "=" * 60)
    print("ğŸ’¾ ä¿å­˜æ¨¡å‹...")
    
    model.save(str(output_path / 'final_model.keras'))
    
    with open(output_path / 'label_map.json', 'w', encoding='utf-8') as f:
        json.dump(dataset.label_map, f, ensure_ascii=False, indent=2)
    
    # ç”Ÿæˆè¨“ç·´å ±å‘Š
    report = {
        'training_time': str(training_time),
        'total_samples': len(dataset.samples),
        'num_classes': dataset.num_classes,
        'global_batch_size': global_batch_size,
        'tpu_cores': strategy.num_replicas_in_sync,
        'epochs': epochs,
        'final_metrics': {
            'train_loss': float(history.history['loss'][-1]),
            'train_accuracy': float(history.history['accuracy'][-1]),
            'val_loss': float(history.history['val_loss'][-1]),
            'val_accuracy': float(history.history['val_accuracy'][-1])
        },
        'best_metrics': {
            'best_val_accuracy': float(max(history.history['val_accuracy'])),
            'best_epoch': int(np.argmax(history.history['val_accuracy']) + 1)
        }
    }
    
    with open(output_path / 'training_report.json', 'w', encoding='utf-8') as f:
        json.dump(report, f, ensure_ascii=False, indent=2)
    
    # ç”Ÿæˆè¨“ç·´æ›²ç·šåœ–
    print("\nğŸ“Š ç”Ÿæˆè¨“ç·´æ›²ç·šåœ–...")
    try:
        import matplotlib
        matplotlib.use('Agg')  # éäº¤äº’å¼å¾Œç«¯
        import matplotlib.pyplot as plt
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        
        # 1. Loss æ›²ç·š
        axes[0, 0].plot(history.history['loss'], label='Training Loss', linewidth=2)
        axes[0, 0].plot(history.history['val_loss'], label='Validation Loss', linewidth=2)
        axes[0, 0].set_xlabel('Epoch', fontsize=12)
        axes[0, 0].set_ylabel('Loss', fontsize=12)
        axes[0, 0].set_title('Model Loss (Focal Loss)', fontsize=14, fontweight='bold')
        axes[0, 0].legend(fontsize=10)
        axes[0, 0].grid(True, alpha=0.3)
        
        # 2. Accuracy æ›²ç·š
        axes[0, 1].plot(history.history['accuracy'], label='Training Accuracy', linewidth=2)
        axes[0, 1].plot(history.history['val_accuracy'], label='Validation Accuracy', linewidth=2)
        axes[0, 1].set_xlabel('Epoch', fontsize=12)
        axes[0, 1].set_ylabel('Accuracy', fontsize=12)
        axes[0, 1].set_title('Model Accuracy', fontsize=14, fontweight='bold')
        axes[0, 1].legend(fontsize=10)
        axes[0, 1].grid(True, alpha=0.3)
        axes[0, 1].set_ylim([0.5, 1.0])
        
        # 3. Top-3 Accuracy æ›²ç·š
        if 'top3_accuracy' in history.history:
            axes[1, 0].plot(history.history['top3_accuracy'], label='Training Top-3', linewidth=2)
            axes[1, 0].plot(history.history['val_top3_accuracy'], label='Validation Top-3', linewidth=2)
            axes[1, 0].set_xlabel('Epoch', fontsize=12)
            axes[1, 0].set_ylabel('Top-3 Accuracy', fontsize=12)
            axes[1, 0].set_title('Top-3 Accuracy', fontsize=14, fontweight='bold')
            axes[1, 0].legend(fontsize=10)
            axes[1, 0].grid(True, alpha=0.3)
            axes[1, 0].set_ylim([0.8, 1.0])
        
        # 4. è¨“ç·´/é©—è­‰å·®è·
        accuracy_gap = np.array(history.history['accuracy']) - np.array(history.history['val_accuracy'])
        axes[1, 1].plot(accuracy_gap, linewidth=2, color='red')
        axes[1, 1].axhline(y=0, color='black', linestyle='--', alpha=0.3)
        axes[1, 1].set_xlabel('Epoch', fontsize=12)
        axes[1, 1].set_ylabel('Accuracy Gap', fontsize=12)
        axes[1, 1].set_title('Train-Val Accuracy Gap (Overfitting Indicator)', fontsize=14, fontweight='bold')
        axes[1, 1].grid(True, alpha=0.3)
        axes[1, 1].fill_between(range(len(accuracy_gap)), accuracy_gap, 0, 
                                where=(accuracy_gap > 0), alpha=0.3, color='red', label='Overfitting')
        axes[1, 1].fill_between(range(len(accuracy_gap)), accuracy_gap, 0, 
                                where=(accuracy_gap <= 0), alpha=0.3, color='green', label='Good Generalization')
        axes[1, 1].legend(fontsize=10)
        
        plt.tight_layout()
        plt.savefig(str(output_path / 'training_curves.png'), dpi=150, bbox_inches='tight')
        plt.close()
        
        print(f"âœ… è¨“ç·´æ›²ç·šåœ–å·²ä¿å­˜: {output_path / 'training_curves.png'}")
        
    except Exception as e:
        print(f"âš ï¸  ç”Ÿæˆè¨“ç·´æ›²ç·šåœ–å¤±æ•—: {e}")
    
    # ç”Ÿæˆæ··æ·†çŸ©é™£
    print("\nğŸ“Š ç”Ÿæˆæ··æ·†çŸ©é™£...")
    try:
        from sklearn.metrics import confusion_matrix, classification_report
        import matplotlib.pyplot as plt
        import seaborn as sns
        
        # æ”¶é›†é©—è­‰é›†çš„æ‰€æœ‰é æ¸¬
        print("   æ­£åœ¨é æ¸¬é©—è­‰é›†...")
        all_labels = []
        all_predictions = []
        
        for features, labels in val_dataset:
            predictions = model.predict(features, verbose=0)
            all_labels.extend(labels.numpy())
            all_predictions.extend(np.argmax(predictions, axis=1))
        
        all_labels = np.array(all_labels)
        all_predictions = np.array(all_predictions)
        
        # è¨ˆç®—æ··æ·†çŸ©é™£
        cm = confusion_matrix(all_labels, all_predictions)
        
        # æ­¸ä¸€åŒ–æ··æ·†çŸ©é™£ï¼ˆç™¾åˆ†æ¯”ï¼‰
        cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
        
        # ç¹ªè£½æ··æ·†çŸ©é™£
        fig, axes = plt.subplots(1, 2, figsize=(24, 10))
        
        # 1. åŸå§‹è¨ˆæ•¸
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                   xticklabels=sorted(dataset.label_map.keys()),
                   yticklabels=sorted(dataset.label_map.keys()),
                   ax=axes[0], cbar_kws={'label': 'Count'})
        axes[0].set_title('Confusion Matrix (Counts)', fontsize=16, fontweight='bold')
        axes[0].set_xlabel('Predicted Label', fontsize=12)
        axes[0].set_ylabel('True Label', fontsize=12)
        axes[0].tick_params(axis='both', labelsize=10)
        
        # 2. æ­¸ä¸€åŒ–ç™¾åˆ†æ¯”
        sns.heatmap(cm_normalized, annot=True, fmt='.2%', cmap='YlOrRd',
                   xticklabels=sorted(dataset.label_map.keys()),
                   yticklabels=sorted(dataset.label_map.keys()),
                   ax=axes[1], cbar_kws={'label': 'Percentage'})
        axes[1].set_title('Confusion Matrix (Normalized)', fontsize=16, fontweight='bold')
        axes[1].set_xlabel('Predicted Label', fontsize=12)
        axes[1].set_ylabel('True Label', fontsize=12)
        axes[1].tick_params(axis='both', labelsize=10)
        
        plt.tight_layout()
        plt.savefig(str(output_path / 'confusion_matrix.png'), dpi=150, bbox_inches='tight')
        plt.close()
        
        print(f"âœ… æ··æ·†çŸ©é™£å·²ä¿å­˜: {output_path / 'confusion_matrix.png'}")
        
        # ç”Ÿæˆåˆ†é¡å ±å‘Š
        idx_to_word = {v: k for k, v in dataset.label_map.items()}
        target_names = [idx_to_word[i] for i in range(dataset.num_classes)]
        
        report_text = classification_report(all_labels, all_predictions, 
                                           target_names=target_names, 
                                           digits=4)
        
        with open(output_path / 'classification_report.txt', 'w', encoding='utf-8') as f:
            f.write("=" * 70 + "\n")
            f.write("æ‰‹èªè­˜åˆ¥æ¨¡å‹ - åˆ†é¡å ±å‘Š\n")
            f.write("=" * 70 + "\n\n")
            f.write(report_text)
            f.write("\n\n" + "=" * 70 + "\n")
            f.write("æ··æ·†çŸ©é™£åˆ†æ:\n")
            f.write("=" * 70 + "\n")
            
            # åˆ†ææ··æ·†çš„é¡åˆ¥å°
            confusion_pairs = []
            for i in range(len(cm)):
                for j in range(len(cm)):
                    if i != j and cm[i, j] > 0:
                        confusion_pairs.append((
                            target_names[i], 
                            target_names[j], 
                            cm[i, j],
                            cm_normalized[i, j]
                        ))
            
            confusion_pairs.sort(key=lambda x: x[2], reverse=True)
            
            f.write("\næœ€å¸¸è¦‹çš„æ··æ·† (Top 10):\n")
            f.write("-" * 70 + "\n")
            for true_label, pred_label, count, ratio in confusion_pairs[:10]:
                f.write(f"{true_label:12s} â†’ {pred_label:12s}: {count:3d} æ¬¡ ({ratio:6.2%})\n")
        
        print(f"âœ… åˆ†é¡å ±å‘Šå·²ä¿å­˜: {output_path / 'classification_report.txt'}")
        
        # æ‰“å°åˆ°æ§åˆ¶å°
        print("\n" + "=" * 70)
        print("ğŸ“‹ åˆ†é¡å ±å‘Šæ‘˜è¦:")
        print("=" * 70)
        print(report_text)
        
    except Exception as e:
        print(f"âš ï¸  ç”Ÿæˆæ··æ·†çŸ©é™£å¤±æ•—: {e}")
        import traceback
        traceback.print_exc()
    
    # è¼¸å‡ºçµæœ
    print("\n" + "=" * 60)
    print("ğŸ‰ è¨“ç·´å®Œæˆï¼")
    print(f"â±ï¸  ç¸½è€—æ™‚: {training_time}")
    print(f"ğŸ“Š æœ€ä½³é©—è­‰æº–ç¢ºç‡: {report['best_metrics']['best_val_accuracy']:.4f} (Epoch {report['best_metrics']['best_epoch']})")
    print(f"ğŸ“Š æœ€çµ‚è¨“ç·´æº–ç¢ºç‡: {report['final_metrics']['train_accuracy']:.4f}")
    print(f"ğŸ“Š æœ€çµ‚é©—è­‰æº–ç¢ºç‡: {report['final_metrics']['val_accuracy']:.4f}")
    print("=" * 60)


# ==================== ä¸»å‡½æ•¸ ====================
def main():
    """ä¸»å‡½æ•¸"""
    print("ğŸš€ Bi-GRU æ‰‹èªè­˜åˆ¥è¨“ç·´ - Kaggle TPU v5e-8 ç‰ˆæœ¬")
    print("=" * 60)
    
    # åˆå§‹åŒ– TPU
    strategy = init_tpu_strategy()
    
    # Kaggle ç’°å¢ƒè·¯å¾‘
    rgb_dir = Path("/kaggle/input/mvp-rgb/rgb_features")
    skeleton_dir = Path("/kaggle/input/mvp-skeleton/skeleton_features")
    output_dir = Path("/kaggle/working/model_output")
    
    # é©—è­‰è·¯å¾‘
    if not rgb_dir.exists():
        raise FileNotFoundError(f"RGB ç‰¹å¾µç›®éŒ„ä¸å­˜åœ¨: {rgb_dir}")
    if not skeleton_dir.exists():
        raise FileNotFoundError(f"Skeleton ç‰¹å¾µç›®éŒ„ä¸å­˜åœ¨: {skeleton_dir}")
    
    print(f"ğŸ“‚ RGB ç‰¹å¾µ: {rgb_dir}")
    print(f"ğŸ“‚ Skeleton ç‰¹å¾µ: {skeleton_dir}")
    print(f"ğŸ“‚ è¼¸å‡ºç›®éŒ„: {output_dir}")
    
    # é–‹å§‹è¨“ç·´
    train_model(
        rgb_dir=rgb_dir,
        skeleton_dir=skeleton_dir,
        output_dir=output_dir,
        strategy=strategy,
        max_length=300,
        batch_size_per_replica=64,
        epochs=50,
        learning_rate=5e-4,  # ä¿æŒåŸå­¸ç¿’ç‡
        val_split=0.2
    )


if __name__ == "__main__":
    main()
