#!/usr/bin/env python3
"""
éª¨æ¶æµç‰¹å¾µæå– - MediaPipe Holisticï¼ˆä¸ŠåŠèº« + æ‰‹æŒ‡ç´°ç¯€ï¼‰
å°ˆç‚º M1 MPS Metal æˆ– CPU å„ªåŒ–çš„æ‰‹èªè¦–é »éª¨æ¶ç‰¹å¾µæå–
- ä¸ŠåŠèº«å§¿æ…‹ï¼ˆ11 å€‹é—œéµé»ï¼‰
- é›™æ‰‹ç´°ç¯€ï¼ˆ21 é» Ã— 2 = 42 å€‹é—œéµé»ï¼‰
- ç¸½è¨ˆ 53 å€‹é—œéµé»ï¼Œ159 ç¶­ç‰¹å¾µ
- å¤šé€²ç¨‹ + å¤šç·šç¨‹ CPU å„ªåŒ–
"""

import os
import cv2
import numpy as np
from pathlib import Path
from tqdm import tqdm
import zipfile
from datetime import datetime
import json
import multiprocessing as mp
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor
import threading
import psutil

try:
    import mediapipe as mp
except ImportError as e:
    raise ImportError("âŒ è«‹å®‰è£ mediapipe: pip install mediapipe") from e


class EnhancedSkeletonExtractor:
    """å¢å¼·ç‰ˆéª¨æ¶æå–å™¨ - MediaPipe Holisticï¼ˆä¸ŠåŠèº« + æ‰‹æŒ‡ç´°ç¯€ï¼‰"""
    
    # MediaPipe Pose ä¸ŠåŠèº«é—œéµé»ç´¢å¼•
    UPPER_BODY_INDICES = [
        0,   # nose
        2,   # left_eye_inner
        5,   # right_eye_inner
        11,  # left_shoulder
        12,  # right_shoulder
        13,  # left_elbow
        14,  # right_elbow
        15,  # left_wrist
        16,  # right_wrist
        23,  # left_hip (è»€å¹¹åƒè€ƒ)
        24   # right_hip (è»€å¹¹åƒè€ƒ)
    ]
    
    def __init__(self, num_threads=None):
        """
        åˆå§‹åŒ–æå–å™¨ - MediaPipe è‡ªå‹•ä½¿ç”¨ M1 Metal åŠ é€Ÿ

        Args:
            num_threads: ç·šç¨‹æ•¸ï¼ŒNone è¡¨ç¤ºè‡ªå‹•æª¢æ¸¬
        """
        # æª¢æ¸¬ç³»çµ±å’Œå¯ç”¨è³‡æº
        self._detect_system_resources(num_threads)

        # åˆå§‹åŒ– MediaPipe Holistic (å¼·åˆ¶ä½¿ç”¨ CPU)
        print("ğŸ“¥ è¼‰å…¥ MediaPipe Holistic æ¨¡å‹...")
        print("   æ¨¡å¼: CPU (å¼·åˆ¶)")
        print(f"   ç·šç¨‹æ•¸: {self.num_threads}")

        # å¼·åˆ¶ä½¿ç”¨ CPU (ç¦ç”¨æ‰€æœ‰ GPU/OpenGL åŠ é€Ÿ)
        # é€™äº›ç’°å¢ƒè®Šæ•¸å¿…é ˆåœ¨å°å…¥ mediapipe ä¹‹å‰è¨­ç½®ï¼Œä½†ç‚ºäº†å®¹å™¨ç’°å¢ƒé¡å¤–ç¢ºä¿
        os.environ['CUDA_VISIBLE_DEVICES'] = ''  # ç¦ç”¨ CUDA GPU
        os.environ['MEDIAPIPE_GPU_DISABLED'] = '1'  # ç¦ç”¨ MediaPipe GPU
        os.environ['MEDIAPIPE_DISABLE_GPU'] = '1'  # æ›¿ä»£ç’°å¢ƒè®Šæ•¸
        
        # ç¦ç”¨ OpenGL/EGL (é—œéµï¼Zeabur å®¹å™¨ç„¡ GPU æ”¯æ´)
        os.environ['GLOG_minloglevel'] = '2'  # æ¸›å°‘éŒ¯èª¤æ—¥èªŒ
        os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # æ¸›å°‘ TensorFlow æ—¥èªŒ
        
        self.mp_holistic = mp.solutions.holistic
        self.holistic = self.mp_holistic.Holistic(
            static_image_mode=False,
            model_complexity=1,  # ä¿æŒ Full ç‰ˆæœ¬ä»¥ç¶­æŒç‰¹å¾µå“è³ª
            smooth_landmarks=True,
            enable_segmentation=False,
            smooth_segmentation=False,
            min_detection_confidence=0.5,  # ä¿æŒé«˜å“è³ªæª¢æ¸¬
            min_tracking_confidence=0.5
        )

        print("âœ… æ¨¡å‹è¼‰å…¥å®Œæˆ")

    def _detect_system_resources(self, num_threads):
        """æª¢æ¸¬ç³»çµ±è³‡æºå’Œå„ªåŒ–è¨­ç½®"""
        try:
            import torch
            self.mps_available = torch.backends.mps.is_available()
        except:
            self.mps_available = False

        # æª¢æ¸¬ CPU æ ¸å¿ƒæ•¸
        self.cpu_count = psutil.cpu_count(logical=True)
        self.physical_cpu_count = psutil.cpu_count(logical=False)

        # è¨­ç½®ç·šç¨‹æ•¸ (å¼·åˆ¶CPUæ¨¡å¼)
        if num_threads is None:
            # CPUæ¨¡å¼ï¼šä½¿ç”¨æ›´å¤šç·šç¨‹ä½†ç•™æ ¸å¿ƒçµ¦ç³»çµ±
            self.num_threads = max(1, self.physical_cpu_count - 1)
        else:
            self.num_threads = num_threads

        print(f"ğŸ” ç³»çµ±è³‡æºæª¢æ¸¬:")
        print(f"   - CPU æ ¸å¿ƒ: {self.cpu_count} (ç‰©ç†: {self.physical_cpu_count})")
        print(f"   - æ¨¡å¼: CPU (å¼·åˆ¶)")
        print(f"   - å„ªåŒ–ç·šç¨‹æ•¸: {self.num_threads}")

        # è¨­ç½® OpenMP ç·šç¨‹æ•¸ï¼ˆå½±éŸ¿ MediaPipeï¼‰
        os.environ['OMP_NUM_THREADS'] = str(self.num_threads)
        os.environ['MKL_NUM_THREADS'] = str(self.num_threads)

        # é—œéµé»åç¨±ï¼ˆç”¨æ–¼èª¿è©¦ï¼‰
        self.upper_body_names = [
            'nose', 'left_eye_inner', 'right_eye_inner',
            'left_shoulder', 'right_shoulder',
            'left_elbow', 'right_elbow',
            'left_wrist', 'right_wrist',
            'left_hip', 'right_hip'
        ]

        print(f"ğŸ“Š ç‰¹å¾µé…ç½®:")
        print(f"   - ä¸ŠåŠèº«é—œéµé»: {len(self.UPPER_BODY_INDICES)} å€‹")
        print(f"   - å·¦æ‰‹é—œéµé»: 21 å€‹")
        print(f"   - å³æ‰‹é—œéµé»: 21 å€‹")
        print(f"   - ç¸½é—œéµé»: {len(self.UPPER_BODY_INDICES) + 42} å€‹")
        print(f"   - ç¸½ç‰¹å¾µç¶­åº¦: {(len(self.UPPER_BODY_INDICES) + 42) * 3} ç¶­")
    
    def normalize_skeleton_features(self, upper_body, left_hand, right_hand, frame_width, frame_height):
        """
        æ­£è¦åŒ–éª¨æ¶ç‰¹å¾µ
        
        Args:
            upper_body: (T, 11, 3) - ä¸ŠåŠèº«é—œéµé»
            left_hand: (T, 21, 3) - å·¦æ‰‹é—œéµé»
            right_hand: (T, 21, 3) - å³æ‰‹é—œéµé»
            frame_width: å¹€å¯¬åº¦
            frame_height: å¹€é«˜åº¦
        
        Returns:
            normalized: (T, 159) - æ­£è¦åŒ–å¾Œçš„ç‰¹å¾µ
        """
        T = len(upper_body)
        
        # çµ„åˆæ‰€æœ‰é—œéµé» (T, 53, 3)
        all_keypoints = np.concatenate([upper_body, left_hand, right_hand], axis=1)
        all_keypoints = all_keypoints.astype(np.float32)
        
        normalized = all_keypoints.copy()
        
        # Step 1: åº§æ¨™æ­£è¦åŒ– [0, 1]
        # MediaPipe å·²ç¶“è¼¸å‡ºæ­£è¦åŒ–åº§æ¨™ï¼Œä½†æˆ‘å€‘å†ç¢ºä¿ä¸€æ¬¡
        normalized[:, :, 0] = np.clip(normalized[:, :, 0], 0, 1)  # x
        normalized[:, :, 1] = np.clip(normalized[:, :, 1], 0, 1)  # y
        normalized[:, :, 2] = np.clip(normalized[:, :, 2], -1, 1)  # z (æ·±åº¦)
        
        # Step 2: ç½®ä¸­è™•ç† - ä»¥è‚©è†€ä¸­å¿ƒç‚ºåŸé»
        for t in range(T):
            # ä¸ŠåŠèº«ç´¢å¼•ï¼šleft_shoulder=3, right_shoulder=4 (åœ¨çµ„åˆå¾Œçš„æ•¸çµ„ä¸­)
            left_shoulder = normalized[t, 3]
            right_shoulder = normalized[t, 4]
            
            # æª¢æŸ¥ç½®ä¿¡åº¦ï¼ˆMediaPipe çš„ visibilityï¼‰
            if left_shoulder[2] > 0.3 and right_shoulder[2] > 0.3:
                center_x = (left_shoulder[0] + right_shoulder[0]) / 2
                center_y = (left_shoulder[1] + right_shoulder[1]) / 2
                
                # æ‰€æœ‰é»ç›¸å°æ–¼è‚©è†€ä¸­å¿ƒ
                normalized[t, :, 0] -= center_x
                normalized[t, :, 1] -= center_y
        
        # Step 3: å°ºåº¦ä¸è®Šæ€§ - æ ¹æ“šè‚©å¯¬æ­£è¦åŒ–
        for t in range(T):
            left_shoulder = normalized[t, 3]
            right_shoulder = normalized[t, 4]
            
            if left_shoulder[2] > 0.3 and right_shoulder[2] > 0.3:
                shoulder_width = np.linalg.norm(
                    left_shoulder[:2] - right_shoulder[:2]
                )
                
                if shoulder_width > 0.01:
                    # æ‰€æœ‰ x,y åº§æ¨™é™¤ä»¥è‚©å¯¬
                    normalized[t, :, :2] /= shoulder_width
        
        # Step 4: è™•ç†ç¼ºå¤±é—œéµé»
        # MediaPipe çš„ visibility < 0.3 è¦–ç‚ºä¸å¯è¦‹
        low_confidence_mask = normalized[:, :, 2] < 0.3
        normalized[low_confidence_mask, :2] = 0
        
        # Step 5: Flatten - (T, 159)
        return normalized.reshape(T, -1)
    
    def extract_features_from_frames(self, frames, frame_width=640, frame_height=480):
        """
        ç›´æ¥å¾ frames æå–éª¨æ¶ç‰¹å¾µï¼ˆå„ªåŒ–ç‰ˆï¼Œç„¡éœ€è®€å–è¦–é »æ–‡ä»¶ï¼‰
        
        Args:
            frames: list of numpy arrays (H, W, 3) - RGB frames
            frame_width: å¹€å¯¬åº¦ï¼ˆç”¨æ–¼æ­£è¦åŒ–ï¼‰
            frame_height: å¹€é«˜åº¦ï¼ˆç”¨æ–¼æ­£è¦åŒ–ï¼‰
        
        Returns:
            features: (T, 159) numpy array
        """
        try:
            if frames is None or len(frames) == 0:
                print(f"âš ï¸  æ²’æœ‰æä¾› frames")
                return None
            
            # å­˜å„²é—œéµé»
            upper_body_keypoints = []
            left_hand_keypoints = []
            right_hand_keypoints = []
            
            # é€å¹€è™•ç†
            for frame in frames:
                # ç¢ºä¿æ˜¯ RGB æ ¼å¼
                if frame.shape[2] == 3:
                    # å¦‚æœæ˜¯ BGRï¼Œè½‰æ›ç‚º RGB
                    # å‡è¨­å·²ç¶“æ˜¯ RGBï¼ˆå¾éŒ„å½±ç›´æ¥ä¾†ï¼‰
                    frame_rgb = frame
                else:
                    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                
                # MediaPipe è™•ç†
                results = self.holistic.process(frame_rgb)
                
                # æå–ä¸ŠåŠèº«é—œéµé»
                if results.pose_landmarks:
                    pose = results.pose_landmarks.landmark
                    upper_body = np.array([
                        [pose[i].x, pose[i].y, pose[i].visibility]
                        for i in self.UPPER_BODY_INDICES
                    ], dtype=np.float32)
                else:
                    upper_body = np.zeros((11, 3), dtype=np.float32)
                
                upper_body_keypoints.append(upper_body)
                
                # æå–å·¦æ‰‹é—œéµé»
                if results.left_hand_landmarks:
                    left_hand = np.array([
                        [lm.x, lm.y, lm.z]
                        for lm in results.left_hand_landmarks.landmark
                    ], dtype=np.float32)
                else:
                    left_hand = np.zeros((21, 3), dtype=np.float32)
                
                left_hand_keypoints.append(left_hand)
                
                # æå–å³æ‰‹é—œéµé»
                if results.right_hand_landmarks:
                    right_hand = np.array([
                        [lm.x, lm.y, lm.z]
                        for lm in results.right_hand_landmarks.landmark
                    ], dtype=np.float32)
                else:
                    right_hand = np.zeros((21, 3), dtype=np.float32)
                
                right_hand_keypoints.append(right_hand)
            
            if len(upper_body_keypoints) == 0:
                print(f"âš ï¸  æœªæå–åˆ°ä»»ä½•å¹€")
                return None
            
            # è½‰æ›ç‚º numpy array
            upper_body_keypoints = np.array(upper_body_keypoints, dtype=np.float32)
            left_hand_keypoints = np.array(left_hand_keypoints, dtype=np.float32)
            right_hand_keypoints = np.array(right_hand_keypoints, dtype=np.float32)
            
            # æ­£è¦åŒ–
            features = self.normalize_skeleton_features(
                upper_body_keypoints,
                left_hand_keypoints,
                right_hand_keypoints,
                frame_width,
                frame_height
            )
            
            return features  # (T, 159)
        
        except Exception as e:
            import traceback
            print(f"âŒ å¾ frames æå–ç‰¹å¾µå¤±æ•—: {e}")
            print(f"è©³ç´°éŒ¯èª¤: {traceback.format_exc()}")
            return None
    
    def extract_features(self, video_path):
        """æå–å–®å€‹è¦–é »çš„å¢å¼·éª¨æ¶ç‰¹å¾µ"""
        try:
            # é–‹å•Ÿè¦–é »
            cap = cv2.VideoCapture(str(video_path))
            if not cap.isOpened():
                print(f"âš ï¸  ç„¡æ³•é–‹å•Ÿè¦–é »: {video_path.name}")
                return None
            
            # ç²å–è¦–é »è³‡è¨Š
            total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
            width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
            height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
            
            if total_frames == 0:
                cap.release()
                print(f"âš ï¸  è¦–é »ç„¡å¹€: {video_path.name}")
                return None
            
            # è®€å–æ‰€æœ‰å¹€
            frames = []
            while True:
                ret, frame = cap.read()
                if not ret:
                    break
                # è½‰æ›ç‚º RGB
                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                frames.append(frame_rgb)
            
            cap.release()
            
            # ä½¿ç”¨æ–°æ–¹æ³•æå–ç‰¹å¾µ
            return self.extract_features_from_frames(frames, width, height)
        
        except Exception as e:
            import traceback
            print(f"âŒ è™•ç†å¤±æ•— {video_path.name}: {e}")
            print(f"è©³ç´°éŒ¯èª¤: {traceback.format_exc()}")
            return None
    
    def process_directory(self, input_dir, output_dir):
        """è™•ç†æ•´å€‹ç›®éŒ„ - å¤šé€²ç¨‹CPUå„ªåŒ–"""
        input_path = Path(input_dir)
        output_path = Path(output_dir)
        output_path.mkdir(exist_ok=True, parents=True)

        # ç²å–æ‰€æœ‰å–®è©ç›®éŒ„
        word_dirs = [d for d in input_path.iterdir() if d.is_dir()]

        stats = {
            "total_videos": 0,
            "successful": 0,
            "failed": 0,
            "word_details": [],
            "feature_dim": 159,
            "keypoint_breakdown": {
                "upper_body": 11,
                "left_hand": 21,
                "right_hand": 21,
                "total": 53
            },
            "cpu_optimization": {
                "num_threads": self.num_threads,
                "mode": "CPU (å¼·åˆ¶)",
                "cpu_cores": self.cpu_count
            }
        }

        print(f"ğŸ“ ç™¼ç¾ {len(word_dirs)} å€‹å–®è©ç›®éŒ„")
        print(f"ğŸš€ ä½¿ç”¨å¤šé€²ç¨‹å„ªåŒ–: {self.num_threads} ç·šç¨‹")

        # è™•ç†æ¯å€‹å–®è©ç›®éŒ„
        for word_dir in tqdm(word_dirs, desc="è™•ç†å–®è©"):
            word_name = word_dir.name
            word_output_dir = output_path / word_name
            word_output_dir.mkdir(exist_ok=True, parents=True)

            # ç²å–æ‰€æœ‰è¦–é »æ–‡ä»¶
            video_files = list(word_dir.glob("*.mp4"))
            word_stats = {"word": word_name, "total": len(video_files), "success": 0, "failed": 0}

            # ä½¿ç”¨ç·šç¨‹æ± è™•ç†å–®è©å…§çš„è¦–é »æ–‡ä»¶
            def process_video_batch(video_batch):
                """è™•ç†ä¸€æ‰¹è¦–é »æ–‡ä»¶"""
                local_success = 0
                local_failed = 0

                for video_file in video_batch:
                    try:
                        # æå–ç‰¹å¾µ
                        features = self.extract_features(video_file)

                        if features is not None:
                            # ä¿å­˜ .npy
                            output_file = word_output_dir / f"{video_file.stem}.npy"
                            np.save(output_file, features)
                            local_success += 1
                        else:
                            local_failed += 1
                    except Exception as e:
                        print(f"âš ï¸  è™•ç†å¤±æ•— {video_file.name}: {e}")
                        local_failed += 1

                return local_success, local_failed

            # å°‡è¦–é »æ–‡ä»¶åˆ†æˆæ‰¹æ¬¡é€²è¡Œè™•ç†
            batch_size = max(1, len(video_files) // self.num_threads)
            video_batches = [video_files[i:i + batch_size] for i in range(0, len(video_files), batch_size)]

            # ä½¿ç”¨ç·šç¨‹æ± è™•ç†æ‰¹æ¬¡
            with ThreadPoolExecutor(max_workers=self.num_threads) as executor:
                futures = [executor.submit(process_video_batch, batch) for batch in video_batches]

                # æ”¶é›†çµæœ
                for future in futures:
                    success, failed = future.result()
                    word_stats["success"] += success
                    word_stats["failed"] += failed

            stats["total_videos"] += len(video_files)
            stats["successful"] += word_stats["success"]
            stats["failed"] += word_stats["failed"]
            stats["word_details"].append(word_stats)

        return stats
    
    def __del__(self):
        """æ¸…ç†è³‡æº"""
        if hasattr(self, 'holistic'):
            self.holistic.close()


def create_zip(output_dir, zip_name):
    """æ‰“åŒ…è¼¸å‡ºç›®éŒ„ç‚º ZIP"""
    output_path = Path(output_dir)
    zip_path = output_path.parent / zip_name
    
    print(f"ğŸ“¦ æ‰“åŒ…è¼¸å‡ºæª”æ¡ˆåˆ° {zip_path}...")
    
    with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:
        for file_path in output_path.rglob('*.npy'):
            arcname = file_path.relative_to(output_path.parent)
            zipf.write(file_path, arcname)
    
    zip_size = zip_path.stat().st_size / (1024 * 1024)  # MB
    print(f"âœ… æ‰“åŒ…å®Œæˆï¼ZIP æª”æ¡ˆå¤§å°: {zip_size:.2f} MB")
    return zip_path


def main():
    """ä¸»å‡½æ•¸"""
    print("ğŸš€ éª¨æ¶æµç‰¹å¾µæå– - MediaPipe Holisticï¼ˆä¸ŠåŠèº« + æ‰‹æŒ‡ç´°ç¯€ï¼‰")
    print("=" * 60)

    # è·¯å¾‘é…ç½®
    kaggle_input = Path("/kaggle/input/original-seg/videos_segmented")
    kaggle_working = Path("/kaggle/working")

    if kaggle_input.exists():
        print("ğŸ  æª¢æ¸¬åˆ° Kaggle ç’°å¢ƒ")
        input_dir = kaggle_input
        output_dir = kaggle_working / "skeleton_features"
        num_threads = None  # è‡ªå‹•æª¢æ¸¬
    else:
        print("ğŸ’» æœ¬åœ°ç’°å¢ƒæ¨¡å¼")
        input_dir = Path("/Users/baidongqu/Desktop/MVP/videos")
        output_dir = Path("/Users/baidongqu/Desktop/MVP/skeleton_features")
        num_threads = None  # è‡ªå‹•æª¢æ¸¬CPUæ ¸å¿ƒæ•¸

    if not input_dir.exists():
        print(f"âŒ è¼¸å…¥ç›®éŒ„ä¸å­˜åœ¨: {input_dir}")
        return

    # åˆå§‹åŒ–æå–å™¨ - MediaPipe è‡ªå‹•å„ªåŒ–
    start_time = datetime.now()
    extractor = EnhancedSkeletonExtractor(num_threads=num_threads)
    
    # è™•ç†ç›®éŒ„
    stats = extractor.process_directory(input_dir, output_dir)
    
    # æ‰“åŒ… ZIP
    if stats["successful"] > 0:
        zip_path = create_zip(output_dir, "skeleton_features.zip")
        stats["zip_path"] = str(zip_path)
    
    # ä¿å­˜å ±å‘Š
    stats["processing_time"] = str(datetime.now() - start_time)
    report_path = output_dir / "extraction_report.json"
    with open(report_path, 'w', encoding='utf-8') as f:
        json.dump(stats, f, ensure_ascii=False, indent=2)
    
    # è¼¸å‡ºçµ±è¨ˆ
    print("\n" + "=" * 60)
    print("ğŸ‰ éª¨æ¶ç‰¹å¾µæå–å®Œæˆï¼")
    print(f"ğŸ“Š ç¸½è¦–é »æ•¸: {stats['total_videos']}")
    print(f"âœ… æˆåŠŸ: {stats['successful']}")
    print(f"âŒ å¤±æ•—: {stats['failed']}")
    print(f"â±ï¸  ç¸½è€—æ™‚: {stats['processing_time']}")
    print(f"ğŸ“ ç‰¹å¾µç¶­åº¦: {stats['feature_dim']}")
    print(f"   - ä¸ŠåŠèº«: {stats['keypoint_breakdown']['upper_body']} é»")
    print(f"   - å·¦æ‰‹: {stats['keypoint_breakdown']['left_hand']} é»")
    print(f"   - å³æ‰‹: {stats['keypoint_breakdown']['right_hand']} é»")
    print(f"ğŸ“‹ å ±å‘Š: {report_path}")
    if "zip_path" in stats:
        print(f"ğŸ“¦ ZIP: {stats['zip_path']}")
    print("=" * 60)


if __name__ == "__main__":
    main()
